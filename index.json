[{"content":"Installing Kafka and Kafka Connect on separate servers allows for better resource management, especially in production environments where Kafka brokers and Connectors may need dedicated hardware. This guide will walk you through the steps to set up Kafka and Kafka Connect on separate Linux servers, using Ubuntu 24.04.\nPrerequisites Two Linux machines (Ubuntu 24.04) Inbound ports opened on both servers: Kafka broker ports: 9092, 9093 Kafka Connect REST port: 8083 Java 11 (OpenJDK) installed on both servers Step 1: Set Up Kafka on the First Server Set the hostname for the Kafka server:\nsudo hostnamectl set-hostname kafka-server sudo nano /etc/hosts Replace 127.0.1.1 with your new hostname.\nInstall necessary updates: Update and upgrade your server with:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade Reboot the server to apply changes:\nsudo reboot Install OpenJDK: Kafka requires Java to run. Install OpenJDK 11:\nsudo apt install openjdk-11-jdk -y Verify the installation:\njava -version Download and Extract Kafka: Download Kafka from the official site:\nwget https://dlcdn.apache.org/kafka/3.8.0/kafka_2.13-3.8.0.tgz tar -xzf kafka_2.13-3.8.0.tgz sudo mv kafka_2.13-3.8.0 /opt/kafka Create a Kafka User and Group: For better management, create a dedicated service account:\nsudo groupadd kafka sudo useradd -r -g kafka -d /opt/kafka -s /bin/false kafka sudo chown -R kafka:kafka /opt/kafka Configure Kafka: Kafka stores logs in /tmp by default. To make management easier, move the logs to /var/log/kafka:\nsudo mkdir -p /var/log/kafka sudo chown -R kafka:kafka /var/log/kafka sudo chmod -R 755 /var/log/kafka Edit the Kafka Configuration: Update the server.properties file to listen on all interfaces:\nsudo nano /opt/kafka/config/kraft/server.properties Add the following lines:\nlisteners=PLAINTEXT://0.0.0.0:9092 advertised.listeners=PLAINTEXT://\u0026lt;Kafka_Server_IP\u0026gt;:9092 log.dirs=/var/log/kafka Format the Log Directory: Change directory to Kafka folder and format the storage with a unique cluster ID:\nKAFKA_CLUSTER_ID=\u0026#34;$(bin/kafka-storage.sh random-uuid)\u0026#34; sudo -u kafka bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties Set Up Kafka as a Systemd Service: Create a service file:\nsudo nano /etc/systemd/system/kafka.service Add the following content:\n[Unit] Description=Apache Kafka Server (KRaft Mode) After=network.target [Service] User=kafka Group=kafka ExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/kraft/server.properties ExecStop=/opt/kafka/bin/kafka-server-stop.sh Restart=on-failure RestartSec=10 [Install] WantedBy=multi-user.target Enable and start the Kafka service:\nsudo systemctl daemon-reload sudo systemctl start kafka sudo systemctl enable kafka sudo systemctl status kafka Step 2: Set Up Kafka Connect on the Second Server Install necessary updates: Update and upgrade your server with:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade Reboot the server to apply changes:\nsudo reboot Install Java on Kafka Connect Server: Just like with the Kafka server, install Java:\nsudo apt install openjdk-11-jdk -y java -version Download and Extract Kafka Connect: Kafka Connect is part of the Kafka package, so download Kafka here too:\nwget https://dlcdn.apache.org/kafka/3.8.0/kafka_2.13-3.8.0.tgz tar -xzf kafka_2.13-3.8.0.tgz sudo mv kafka_2.13-3.8.0 /opt/kafka Configure Kafka Connect: Edit the connect-distributed.properties file to configure Kafka Connect:\nsudo nano /opt/kafka/config/connect-distributed.properties Update the plugin path:\nplugin.path=/opt/kafka/connectors bootstrap.servers=\u0026lt;Kafka_Server_IP\u0026gt;:9092 Create a Connectors Directory: Create a directory for Kafka Connect plugins:\nsudo mkdir /opt/kafka/connectors sudo chown kafka:kafka /opt/kafka/connectors Set Up Kafka Connect as a Systemd Service: Create a service file:\nsudo nano /etc/systemd/system/kafka-connect.service Add the following content:\n[Unit] Description=Kafka Connect Distributed Mode Service After=network.target [Service] User=kafka Group=kafka ExecStart=/opt/kafka/bin/connect-distributed.sh /opt/kafka/config/connect-distributed.properties Restart=on-failure RestartSec=10 Environment=\u0026#34;KAFKA_HEAP_OPTS=-Xmx1G -Xms1G\u0026#34; [Install] WantedBy=multi-user.target Enable and start the Kafka Connect service:\nsudo systemctl daemon-reload sudo systemctl start kafka-connect sudo systemctl enable kafka-connect sudo systemctl status kafka-connect Step 3: Verify the Installation On the Kafka server, check the Kafka logs:\nsudo journalctl -u kafka On the Kafka Connect server, check the Kafka Connect logs:\nsudo journalctl -u kafka-connect Step 4: Install and Configure Debezium (Optional) If you want to set up Debezium for change data capture (CDC), follow these steps:\nDownload Debezium SQL Server Connector:\nwget https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/2.7.1.Final/debezium-connector-sqlserver-2.7.1.Final-plugin.tar.gz sudo tar -xzf debezium-connector-sqlserver-2.7.1.Final-plugin.tar.gz -C /opt/kafka/connectors/ sudo chown -R kafka:kafka /opt/kafka/connectors/debezium-connector-sqlserver Restart Kafka Connect:\nsudo systemctl restart kafka-connect Verify the Plugin:\ncurl -s localhost:8083/connector-plugins | jq Commands for TroubleShooting Kafka # List all the topics $KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list # Creating new topic $KAFKA_HOME/bin/kafka-topics.sh --create --topic first-topic --bootstrap-server localhost:9092 # Get information about the topic $KAFKA_HOME/bin/kafka-topics.sh --describe --topic first-topic --bootstrap-server localhost:9092 # Read the messages from the topics $KAFKA_HOME/bin/kafka-console-consumer.sh --topic first-topic --bootstrap-server localhost:9092 # Read the message before the consumer $KAFKA_HOME/bin/kafka-console-consumer.sh --topic first-topic --from-beginning --bootstrap-server localhost:9092 # Delete topic bin/kafka-topics.sh --delete --topic first-topic --bootstrap-server localhost:9092 # Example codes /opt/kafka_2.13-3.8.0/bin/kafka-console-consumer.sh --bootstrap-server \u0026lt;SQL SERVER\u0026gt;:9092 --topic crewing.vessel.fake.crew --from-beginning | jq \u0026#39;.payload | { operation: .op, before: .before.crew_id , after: .after.crew_id ,first_name: .after.first_name, last_name: .after.last_name, change_lsn: .source.change_lsn, commit_lsn: .source.commit_lsn, transaction: .transaction.id }\u0026#39; Kafka Connect # List all connectors curl -X GET http://localhost:8083/connectors # Get status of the connectors curl -X GET http://localhost:8083/connectors/my-connector/status # Delete a connector curl -X DELETE http://localhost:8083/connectors/my-connector # Update a connector curl -X PUT -H \u0026#34;Content-Type: application/json\u0026#34; -d @updated-connector-config.json http://localhost:8083/connectors/connector_name/config # Verifying the update of the connector curl -X GET http://localhost:8083/connectors/connector_name/config Conclusion You have successfully installed Kafka and Kafka Connect on separate servers, ensuring that both services are set up for distributed, scalable processing. This setup is optimal for large-scale deployments and is ready for further configuration, such as integrating Debezium or other Kafka Connect plugins.\n","permalink":"https://bookofdaniel.in/posts/installing-kafka-and-kafka-connect-on-seperate-servers/","summary":"\u003cp\u003eInstalling Kafka and Kafka Connect on separate servers allows for better resource management, especially in production environments where Kafka brokers and Connectors may need dedicated hardware. This guide will walk you through the steps to set up Kafka and Kafka Connect on separate Linux servers, using Ubuntu 24.04.\u003c/p\u003e\n\u003ch3 id=\"prerequisites\"\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTwo Linux machines\u003c/strong\u003e (Ubuntu 24.04)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInbound ports opened\u003c/strong\u003e on both servers:\n\u003cul\u003e\n\u003cli\u003eKafka broker ports: 9092, 9093\u003c/li\u003e\n\u003cli\u003eKafka Connect REST port: 8083\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eJava 11 (OpenJDK)\u003c/strong\u003e installed on both servers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-1-set-up-kafka-on-the-first-server\"\u003eStep 1: Set Up Kafka on the First Server\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSet the hostname\u003c/strong\u003e for the Kafka server:\u003c/p\u003e","title":"Installing Kafka and Kafka Connect on Seperate Servers"},{"content":"Welcome to the hard way of installing Kubernetes in Azure Virtual Machines. The instructions will be moreover same for On-prem. For many developers, using managed Kubernetes services like GKE, EKS, or AKS can be convenient, but they often abstract away the intricate details of how a cluster operates under the hood.\nRequirements NODES IP HOSTNAME MACHINE TYPE OPERATING SYSTEM master 172.16.39.14 k8s-master.local Standard B2ms Ubuntu 24.04 k8s-worker1 172.16.39.23 k8s-worker1.local Standard B4ms Ubuntu 24.04 Provisioning the Servers in Azure To provision two Linux virtual machines (VMs) in Azure with the specified details, you can use the Azure CLI (az) to achieve this. Here‚Äôs how you can provision both machines using az vm create commands.\n# 1. Set common variables RESOURCE_GROUP=\u0026#34;k8s-cluster\u0026#34; LOCATION=\u0026#34;eastus\u0026#34; MASTER_VM_NAME=\u0026#34;k8s-master\u0026#34; WORKER_VM_NAME=\u0026#34;k8s-worker1\u0026#34; MASTER_IP=\u0026#34;172.16.39.14\u0026#34; WORKER_IP=\u0026#34;172.16.39.23\u0026#34; VNET_NAME=\u0026#34;k8s-vnet\u0026#34; SUBNET_NAME=\u0026#34;k8s-subnet\u0026#34; MASTER_HOSTNAME=\u0026#34;k8s-master.local\u0026#34; WORKER_HOSTNAME=\u0026#34;k8s-worker1.local\u0026#34; # 2. Resource group creation az group create --name $RESOURCE_GROUP --location $LOCATION # 3. Create a virtual network (VNet) az network vnet create \\ --resource-group $RESOURCE_GROUP \\ --name $VNET_NAME \\ --address-prefix 172.16.0.0/16 \\ --subnet-name $SUBNET_NAME \\ --subnet-prefix 172.16.39.0/24 # 4. Create static IP addresses # Create public IPs az network public-ip create --resource-group $RESOURCE_GROUP --name masterPublicIP --allocation-method Static --sku Standard az network public-ip create --resource-group $RESOURCE_GROUP --name worker1PublicIP --allocation-method Static --sku Standard # Create NIC for the master node az network nic create \\ --resource-group $RESOURCE_GROUP \\ --name masterNIC \\ --vnet-name $VNET_NAME \\ --subnet $SUBNET_NAME \\ --private-ip-address $MASTER_IP \\ --public-ip-address masterPublicIP # Create NIC for the worker node az network nic create \\ --resource-group $RESOURCE_GROUP \\ --name worker1NIC \\ --vnet-name $VNET_NAME \\ --subnet $SUBNET_NAME \\ --private-ip-address $WORKER_IP \\ --public-ip-address worker1PublicIP # 5. Provision the master node az vm create \\ --resource-group $RESOURCE_GROUP \\ --name $MASTER_VM_NAME \\ --size Standard_B2ms \\ --nics masterNIC \\ --image Canonical:0001-com-ubuntu-server-jammy:24_04-lts:latest \\ --admin-username azureuser \\ --generate-ssh-keys \\ --custom-data cloud-init.yaml \\ --host-name $MASTER_HOSTNAME # 6. Provision the worker node az vm create \\ --resource-group $RESOURCE_GROUP \\ --name $WORKER_VM_NAME \\ --size Standard_B4ms \\ --nics worker1NIC \\ --image Canonical:0001-com-ubuntu-server-jammy:24_04-lts:latest \\ --admin-username azureuser \\ --generate-ssh-keys \\ --custom-data cloud-init.yaml \\ --host-name $WORKER_HOSTNAME # 7. Verification az vm list --resource-group $RESOURCE_GROUP -o table This will show you the VMs with their details. You should see k8s-master and k8s-worker1 with the correct IP addresses and machine types.\nSystem Preparation for Kubernetes Installation Before diving into the installation of Kubernetes, it\u0026rsquo;s essential to prepare your system for optimal performance and stability. This section outlines the necessary steps to get both the master and worker nodes ready for Kubernetes.\n‚ÑπÔ∏è Execute the following commands on both master and worker nodes.\n1. Update the OS First, ensure your system is up-to-date by running the following commands to update and upgrade all installed packages:\nsudo apt update sudo apt upgrade -y After the upgrade completes, reboot the system to apply all changes:\nsudo reboot 2. Set Hostname Assign meaningful hostnames to both your master and worker nodes. This makes it easier to identify and manage the nodes in your cluster.\nMaster Node: sudo hostnamectl set-hostname \u0026#34;k8s-master.local\u0026#34; Worker Node: sudo hostnamectl set-hostname \u0026#34;k8s-worker1.local\u0026#34; Next, update the /etc/hosts file on both nodes to map the hostnames to their corresponding IP addresses. Add the following lines to the file:\n172.16.39.14 k8s-master.local 172.16.39.23 k8s-worker1.local 3. Disable Swap Kubernetes requires swap to be disabled to function properly. This is crucial because the Kubelet, the primary Kubernetes agent running on each node, does not handle memory swapping well. Enabling swap can lead to performance degradation and unpredictable behavior in your cluster.\nDisable swap immediately with the following commands:\nsudo swapoff -a sudo sed -i \u0026#39;/ swap / s/^/#/\u0026#39; /etc/fstab sudo mount -a By commenting out the swap entry in /etc/fstab, this ensures swap remains disabled after a reboot. Verify swap is disabled with:\nfree -h The output should indicate that swap is set to 0:\ntotal used free shared buff/cache available Mem: 7.8Gi 1.4Gi 4.9Gi 5.0Mi 1.7Gi 6.3Gi Swap: 0B 0B 0B 4. Update Kernel and Configure Modules For Kubernetes to run efficiently, specific kernel modules and network parameters need to be configured.\nFirst, create a configuration file for kernel modules:\nsudo tee /etc/modules-load.d/containerd.conf \u0026lt;\u0026lt;EOF overlay br_netfilter EOF overlay: Used for overlay filesystems, which are essential for container storage. br_netfilter: Enables Kubernetes to manage network traffic between containers. Next, load the required kernel modules:\nsudo modprobe overlay sudo modprobe br_netfilter Set the necessary kernel parameters for Kubernetes by creating a configuration file:\nsudo tee /etc/sysctl.d/kubernetes.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF Finally, reload the sysctl configuration to apply the new parameters:\nsudo sysctl --system These steps ensure your system is properly prepared for the Kubernetes installation, providing a stable foundation for the cluster to run efficiently.\nInstalling Containerd Runtime on All Nodes A critical component of any Kubernetes cluster is the container runtime, which is responsible for running containers on each node. Containerd is a lightweight and powerful runtime that provides essential container lifecycle management, including image transfer, storage, and execution. Originally developed as part of Docker, it is now a key part of the Cloud Native Computing Foundation (CNCF) and is favored for Kubernetes environments due to its simplicity and performance.\nTo install containerd on all nodes in your Kubernetes cluster, follow these steps:\nFirst, ensure that the necessary packages and dependencies are installed:\nsudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates Add Docker‚Äôs official GPG key and Docker‚Äôs repository to your system:\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; Update your package lists to include the newly added Docker repository, and install containerd:\nsudo apt update sudo apt install -y containerd.io Once containerd is installed, you need to configure it to work with Kubernetes. Generate the default configuration file and enable SystemdCgroup, which ensures that containerd integrates smoothly with Kubernetes, particularly when using systemd for process management:\ncontainerd config default | sudo tee /etc/containerd/config.toml \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 sudo sed -i \u0026#39;s/SystemdCgroup = false/SystemdCgroup = true/g\u0026#39; /etc/containerd/config.toml Finally, restart and enable the containerd service so that it starts automatically on system boot:\nsudo systemctl restart containerd sudo systemctl enable containerd By following these steps, you‚Äôll have a robust and efficient container runtime in place, ready for Kubernetes. Repeat this process on each node to ensure consistency across the cluster.\nInstalling Kubeadm, Kubelet, and Kubectl Now that your system is prepared, it\u0026rsquo;s time to install the essential Kubernetes components on all of your machines: kubeadm, kubelet, and kubectl.\nkubeadm: This tool helps bootstrap the Kubernetes cluster. kubelet: The agent that runs on all nodes in the cluster, responsible for running pods and containers. kubectl: A command-line utility that lets you interact with the Kubernetes cluster. It\u0026rsquo;s important to note that kubeadm will not manage or install kubelet or kubectl for you, so you must ensure that all these tools are on the correct version. Mismatches between kubeadm, kubelet, and kubectl versions can result in instability. Kubernetes does allow a one-minor-version difference between the kubelet and the control plane, but the kubelet version should never exceed the API server version. For example, kubelet v1.7.0 can work with an API server running v1.8.0, but not the other way around.\nAdditionally, as of September 13, 2023, Kubernetes has moved to a new package repository hosted at pkgs.k8s.io, which you must use to install any Kubernetes versions after v1.24. The legacy repositories (apt.kubernetes.io) are deprecated and may be removed without notice.\nFollow the steps below to install these packages for Kubernetes v1.31:\n1. Update Package Index and Install Dependencies Start by updating the system‚Äôs package index and installing the necessary dependencies:\nsudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gpg 2. Add the Kubernetes Signing Key Download the Kubernetes signing key for the package repositories. If the directory /etc/apt/keyrings doesn\u0026rsquo;t exist, create it before running the following command:\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg This ensures that you are installing authentic Kubernetes packages.\n3. Add the Kubernetes v1.31 Repository Add the Kubernetes v1.31 repository to your system‚Äôs sources list. If you need a different version, modify the version number in the URL accordingly:\necho \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list 4. Install Kubelet, Kubeadm, and Kubectl Once the repository is added, update your package list and install kubeadm, kubelet, and kubectl:\nsudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl Marking the packages with apt-mark hold ensures they won\u0026rsquo;t be accidentally updated during system upgrades, which is important for maintaining version stability across your cluster.\n5. Enable the Kubelet Service (Optional) Before you bootstrap the Kubernetes cluster with kubeadm, you can enable the kubelet service to start automatically on boot:\nsudo systemctl enable --now kubelet These steps install the core tools required for setting up and managing your Kubernetes cluster. Make sure to follow them carefully on both your master and worker nodes.\nHere‚Äôs the blog paragraph based on your snippet:\nInitializing the Kubernetes Cluster with Kubeadm With kubeadm, kubelet, and kubectl installed on your master and worker nodes, it‚Äôs time to initialize the Kubernetes cluster.\n‚ÑπÔ∏è This step should only be executed on the master node, as it sets up the control plane that will manage the cluster.\nTo begin, use the following command on your master node to initialize the cluster:\nsudo kubeadm init \\ --pod-network-cidr=10.10.0.0/16 \\ --control-plane-endpoint=k8s-master.local \u0026ndash;pod-network-cidr=10.10.0.0/16: This specifies the CIDR range for the pod network. You can modify this value based on your network architecture. \u0026ndash;control-plane-endpoint=k8s-master.local: This is the DNS or IP address of your control plane (master node). Ensure that the DNS or IP is resolvable by all worker nodes in your cluster. After running this command, kubeadm will perform the following tasks:\nDownload and install the necessary control plane components such as etcd, kube-apiserver, kube-scheduler, and kube-controller-manager. Set up your cluster according to the parameters provided. Generate a join token that worker nodes can use to join the cluster. Once the initialization is complete, kubeadm will output instructions to finish setting up kubectl on the master node and provide the join command for your worker nodes.\nNotes:\nThe --pod-network-cidr value must align with the configuration of the pod network solution (e.g., Calico, Flannel) you plan to deploy. Make sure that the control plane endpoint (k8s-master.local) is properly configured in your DNS or /etc/hosts file so that all nodes can resolve it. At this point, the control plane will be ready, and the next step will be to install a network add-on to allow pod-to-pod communication within the cluster.\nOutput:\nazureuser@k8s-master:~$ sudo kubeadm init \\ --pod-network-cidr=10.10.0.0/16 \\ --control-plane-endpoint=k8s-master.local [init] Using Kubernetes version: v1.26.1 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.10] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master.local localhost] and IPs [192.168.1.10 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master.local localhost] and IPs [192.168.1.10 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 7.503422 seconds [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-master.local as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-master.local as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: daii9y.g4dq24u6irkz4pt0 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap,RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regularuser: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config== Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listedat: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following asroot: kubeadm join k8s-master.local:6443 --token daii9y.g4dq24u6irkz4pt0 \\ --discovery-token-ca-cert-hash sha256:58b9cc96ed57a5797fddea653756dbda830efbff55b720a10cffb3948d489148 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join k8s-master.local:6443 --token daii9y.g4dq24u6irkz4pt0 \\ --discovery-token-ca-cert-hash sha256:58b9cc96ed57a5797fddea653756dbda830efbff55b720a10cffb3948d489148 ‚ÑπÔ∏è Now, As shown in the output execute below command in master node.\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Verify the cluster status:\nkubectl cluster-info kubectl get nodes Output:\nazureuser@k8s-master:~$ kubectl cluster-info Kubernetes control plane is running at https://k8s-master.local:6443 CoreDNS is running at https://k8s-master.local:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. azureuser@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master.local Ready control-plane 3d3h v1.30.4 It seems the control plane is running, we will proceed to add worker nodes to this cluster.\nAdding Worker Nodes to the Kubernetes Cluster After initializing the Kubernetes cluster on the master node, it\u0026rsquo;s time to add your worker nodes to the cluster. This will allow the control plane to distribute workloads across the nodes and manage them.\n‚ÑπÔ∏è To add a worker node, you need to execute the kubeadm join command in worker nodes.\nThis command securely connects the worker node to the control plane. the command typically looks something like this:\nkubeadm join k8s-master.local:6443 --token daii9y.g4dq24u6irkz4pt0 \\ --discovery-token-ca-cert-hash sha256:58b9cc96ed57a5797fddea653756dbda830efbff55b720a10cffb3948d489148 k8s-master.local:6443: This is the control plane endpoint (master node\u0026rsquo;s address). \u0026ndash;token: The token generated during the kubeadm init process, which allows the worker node to authenticate with the control plane. \u0026ndash;discovery-token-ca-cert-hash: A hash that ensures the worker node securely discovers the control plane‚Äôs certificate authority. Once this command completes successfully, the worker node will be part of the Kubernetes cluster, ready to run workloads distributed by the control plane. You can verify that the node has joined the cluster by running the following command on the master node:\nkubectl get nodes Output:\nazureuser@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master.local Ready control-plane 3d3h v1.30.4 k8s-worker1.local Ready \u0026lt;none\u0026gt; 3d3h v1.30.4 This will list all nodes, including the newly added workers, along with their status in the cluster.\nRepeat the process for each worker node to ensure that all machines are part of the cluster.\nHere‚Äôs the updated blog paragraph with the sed command for editing the Calico manifest:\nInstalling Calico (v3.28.1) Pod Network for the Kubernetes Cluster In order to allow communication between the pods in your cluster, you\u0026rsquo;ll need to install a network add-on. One of the most popular options is Calico, which provides networking and network security capabilities for Kubernetes. Below, we\u0026rsquo;ll walk through how to install Calico on your Kubernetes cluster.\n‚ÑπÔ∏è These commands should be run only on the master node.\n1. Download the Calico Manifest File To begin, download the Calico manifest file, which is pre-configured for clusters with fewer than 50 nodes:\ncurl https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml -O This file contains all the necessary configuration to deploy Calico on your Kubernetes cluster.\n2. Edit the Calico Manifest Using sed To streamline the process of modifying the CALICO_IPV4POOL_CIDR in the calico.yaml file, you can use the following sed command. This automatically updates the pod network CIDR without manually opening the file:\nsed -i \u0026#39;s/value: \u0026#34;192.168.0.0\\/16\u0026#34;/value: \u0026#34;10.10.0.0\\/16\u0026#34;/\u0026#39; calico.yaml This command ensures that the pod network CIDR matches the one you specified during cluster initialization (10.10.0.0/16).\n3. Apply the Calico Manifest Once the manifest is updated, install Calico by applying the configuration using kubectl:\nkubectl apply -f calico.yaml Calico will be deployed on your cluster, enabling pod-to-pod communication and enforcing network policies.\nOutput:\nazureuser@k8s-master:~$ kubectl apply -f calico.yaml poddisruptionbudget.policy/calico-kube-controllers created serviceaccount/calico-kube-controllers created serviceaccount/calico-node created configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created deployment.apps/calico-kube-controllers created Verifying the K8s Installation You can verify that Calico is running correctly by checking the status of the pods in the kube-system namespace:\nkubectl get pods -n kube-system Output:\nazureuser@k8s-master:~$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-77d59654f4-25crd 1/1 Running 5 (3h59m ago) 3d3h calico-node-hqf82 1/1 Running 2 (3h59m ago) 3d3h calico-node-jxwbm 1/1 Running 4 (3h55m ago) 3d3h coredns-7db6d8ff4d-6f9cn 1/1 Running 2 (3h59m ago) 3d4h coredns-7db6d8ff4d-dnzq2 1/1 Running 2 (3h59m ago) 3d4h You should see Calico components such as calico-node and calico-kube-controllers running successfully.\nWith Calico installed, your Kubernetes cluster is now fully networked, allowing pods to communicate across nodes as necessary. You can also configure Calico for advanced network security features if needed.\nNow if we check the status of the nodes, the status will be Ready.\nkubectl get nodes Output:\nazureuser@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master.local Ready control-plane 3d4h v1.30.4 k8s-worker1.local Ready \u0026lt;none\u0026gt; 3d4h v1.30.4 Congrats, if you reach till the end üòä. You are a soldier ü™ñ.\n","permalink":"https://bookofdaniel.in/posts/how-to-setup-kubernetes-in-azure-virtual-machines/","summary":"\u003cp\u003eWelcome to the hard way of installing Kubernetes in Azure Virtual Machines. The instructions will be moreover same for On-prem. For many developers, using managed Kubernetes services like GKE, EKS, or AKS can be convenient, but they often abstract away the intricate details of how a cluster operates under the hood.\u003c/p\u003e\n\u003ch1 id=\"requirements\"\u003eRequirements\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003eNODES\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eIP\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eHOSTNAME\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eMACHINE TYPE\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eOPERATING SYSTEM\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003emaster\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e172.16.39.14\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ek8s-master.local\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eStandard B2ms\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eUbuntu 24.04\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003ek8s-worker1\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e172.16.39.23\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ek8s-worker1.local\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eStandard B4ms\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eUbuntu 24.04\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"provisioning-the-servers-in-azure\"\u003eProvisioning the Servers in Azure\u003c/h1\u003e\n\u003cp\u003eTo provision two Linux virtual machines (VMs) in Azure with the specified details, you can use the Azure CLI (az) to achieve this. Here‚Äôs how you can provision both machines using az vm create commands.\u003c/p\u003e","title":"How to Setup Kubernetes in Azure Virtual Machines"},{"content":"Yes, you read that right. The port you‚Äôre trying to use is already occupied by another application. If you‚Äôre a system or network administrator, you‚Äôve likely encountered this frustrating message more times than you care to remember. It‚Äôs challenging to pinpoint which application is hogging the port (though it‚Äôs not as hard as it seems, as I‚Äôll show you üòâ). These ghost messages can leave you in quite a pickle.\nJust yesterday, we faced this exact issue and wasted two hours troubleshooting so that you don‚Äôt have to!\nOur situation was this: We had installed SQL Server 2019 Reporting Services on one of our development servers, and the default port it selected for installation was 8082. Unfortunately, this port was already in use by our front-facing application, so we needed to free it up.\nAfter some frantic Googling, we found a solution: we updated the port from 8082 to 8083 in the config.json file located in the Reporting Services folder. (You can usually find the file here: C:\\Program Files\\Microsoft SQL Server Reporting Services\\SSRS\\RSHostingService\\RSHostingService.exe.) We then restarted the SQL Server Reporting Services Windows service, and voil√†‚ÄîRSHostingService.exe started running on port 8083.\nWe thought everything was resolved and that port 8082 was free. However, when we configured our front-end application and tried to access it, we were greeted with a 503 Server Unavailable error. üò¢\nTroubleshooting Port Availability To narrow down the issue, it‚Äôs helpful to identify which application is using the port.\nFinding the Process Using netstat The netstat command, short for \u0026ldquo;network statistics,\u0026rdquo; is a command-line tool that displays active network connections, routing tables, and various network interface statistics. By running the netstat command with the noa parameters, we can list all the listening ports with process ID information and pipe that command to search for the specific port.\nnetstat -noa | find \u0026#34;8082\u0026#34; Output:\nTCP 0.0.0.0:8082 0.0.0.0:0 LISTENING 4 TCP [::]:8082 [::]:0 LISTENING 4 The process ID 4 indicates that it‚Äôs a system process. Unfortunately, this doesn‚Äôt tell us which application is using the port, so we need another method.\nFinding the Process Using netsh Netsh, short for Network Shell, is a command-line utility that allows users to configure and manage network devices both locally and remotely. It‚Äôs particularly useful for tasks like changing IP addresses, resetting the TCP/IP stack, and managing wireless settings.\nWe can use netsh to query the HTTP request and find out which process is using port 8082:\nnetsh http show servicestate view=requestq Save the output of the above command to a file, open it using Notepad, and search for the port details:\nnetsh http show servicestate view=requestq \u0026gt; results.log notepad results.log Output:\nRequest queue name: Request queue is unnamed. Version: 2.0 State: Active Request queue 503 verbosity level: Basic Max requests: 1000 Number of active processes attached: 1 Process IDs: 2312 URL groups: URL group ID: BE00000340000001 State: Active Request queue name: Request queue is unnamed. Properties: Max bandwidth: inherited Max connections: inherited Timeouts: Timeout values inherited Number of registered URLs: 1 Registered URLs: HTTP://LOCALHOST:8082/ Server session ID: C000000320000001 Version: 2.0 State: Active Properties: Max bandwidth: 4294967295 Timeouts: Entity body timeout (secs): 120 Drain entity body timeout (secs): 120 Request queue timeout (secs): 120 Idle connection timeout (secs): 120 Header wait timeout (secs): 120 Minimum send rate (bytes/sec): 150 Now you can see that port 8082 is being used by process 2312. You can check Task Manager to see which process is associated with this PID, or use the following command:\ntasklist /FI \u0026#34;PID eq 2312\u0026#34; Output:\nImage Name PID Session Name Session# Mem Usage ========================= ======== ================ =========== ============ SQLServerReportingServices 2312 Services 0 42,232 K What the Heck?! üòÆ How is this possible? We changed the reporting tool config file, yet netsh still shows that port 8082 is in use. Upon rechecking the reporting service logs, we confirmed that the application is indeed running on port 8083. ü§î\nIt turns out that the URL with port 8082 was reserved by SQL Reporting Services with an Access Control List (ACL) during installation. Changing the config file alone wasn‚Äôt enough. To see the list of URL reservations for HTTP services, use this command:\nnetsh http show urlacl Output:\nReserved URL : https://*:8082/ User: NT SERVICE\\SQLServerReportingServices Listen: Yes Delegate: No SDDL: D:(A;;GX;;;BU)(A;;GX;;;LS) To free up port 8082 for our front-end application, we needed to delete this obsolete HTTP reservation:\nnetsh http delete urlacl url=http://+:8082/ Output:\nDeleted Successfully. To confirm that the port is free and available for use, run:\nnetstat -noa | find \u0026#34;8082\u0026#34; The expected output should be blank! üòâ\nFinally, the port is free!\n","permalink":"https://bookofdaniel.in/posts/port-not-available/","summary":"\u003cp\u003e\u003cstrong\u003eYes, you read that right.\u003c/strong\u003e The port you‚Äôre trying to use is already occupied by another application. If you‚Äôre a system or network administrator, you‚Äôve likely encountered this frustrating message more times than you care to remember. It‚Äôs challenging to pinpoint which application is hogging the port (though it‚Äôs not as hard as it seems, as I‚Äôll show you üòâ). These ghost messages can leave you in quite a pickle.\u003c/p\u003e","title":"Port Not Available: 503 Server Unavailable"},{"content":"Welcome to our exploration of Kubernetes architecture! Kubernetes, often abbreviated as K8s, is a powerful tool designed to manage containerized applications in a scalable and automated fashion. This blog post aims to provide a high-level understanding of Kubernetes architecture using an analogy of ships, which simplifies the complex interplay between its components. Special thanks to KodeKloud for their insightful lecture, which inspired this overview.\nThe Kubernetes Cluster: Ships at Sea At its core, a Kubernetes cluster comprises two types of nodes: worker nodes and master nodes. To understand their roles, let\u0026rsquo;s imagine a fleet of ships:\nWorker Nodes (Cargo Ships): These ships carry the actual load‚Äîin Kubernetes, this translates to running your containerized applications. Master Nodes (Control Ships): These ships oversee and manage the cargo ships, ensuring everything runs smoothly. Master Nodes: The Control Ships Master nodes are the brain of the Kubernetes cluster, managing the state and operations of the entire system. They contain several key components, collectively known as the control plane:\netcd: A highly available key-value store that keeps all cluster data. Think of it as the central database that logs the details of every ship and container.\nKube Scheduler: Similar to cranes that load containers onto ships, the Kube Scheduler assigns containers to nodes based on resource requirements, node capacity, and various constraints like node affinity and tolerations.\nControllers: These are specialized offices on the control ships handling specific tasks:\nNode Controller: Manages the state of nodes, ensuring they are operational and adding new ones to the cluster as needed. Replication Controller: Ensures the desired number of container replicas are running at all times. Kube API Server: The central management component that orchestrates all operations within the cluster. It exposes the Kubernetes API, allowing users and controllers to interact with the cluster, monitor its state, and make necessary changes.\nWorker Nodes: The Cargo Ships Worker nodes are where your applications run, managed by two key components:\nKubelet: The captain of the ship, responsible for communicating with the Kube API Server, receiving instructions, and managing containers on the node. The Kubelet also sends status reports back to the master nodes.\nKube-proxy: Ensures smooth communication between containers across different nodes by maintaining network rules on each node. This is essential for services to interact seamlessly, like a web server on one node communicating with a database server on another.\nContainer Runtime All nodes, whether master or worker, require a container runtime engine to run containers. While Docker is a popular choice, Kubernetes also supports other runtimes like containerd and CRI-O. This runtime environment is crucial for deploying containerized applications and components.\nConclusion Kubernetes architecture, with its intricate components and interactions, ensures efficient and scalable management of containerized applications. By likening it to a fleet of ships with dedicated roles and responsibilities, we can better understand the complex but elegant orchestration that Kubernetes provides.\nIn upcoming posts, we will delve deeper into each component, exploring their configurations and roles in greater detail. Stay tuned as we continue to navigate the fascinating world of Kubernetes!\nCredit: This content is inspired by a lecture from KodeKloud.\n","permalink":"https://bookofdaniel.in/posts/kubernetes-architecture-explained-the-ship-analogy-of-kodekloud/","summary":"\u003cp\u003eWelcome to our exploration of Kubernetes architecture! Kubernetes, often abbreviated as K8s, is a powerful tool designed to manage containerized applications in a scalable and automated fashion. This blog post aims to provide a high-level understanding of Kubernetes architecture using an analogy of ships, which simplifies the complex interplay between its components. Special thanks to KodeKloud for their insightful lecture, which inspired this overview.\u003c/p\u003e\n\u003ch3 id=\"the-kubernetes-cluster-ships-at-sea\"\u003eThe Kubernetes Cluster: Ships at Sea\u003c/h3\u003e\n\u003cp\u003eAt its core, a Kubernetes cluster comprises two types of nodes: worker nodes and master nodes. To understand their roles, let\u0026rsquo;s imagine a fleet of ships:\u003c/p\u003e","title":"Kubernetes Architecture Explained: the Ship Analogy of KodeKloud"},{"content":"In the hustle and bustle of our daily lives, it\u0026rsquo;s easy to let moments slip by without much thought. The idea of journaling, while appealing in theory, often feels daunting in practice. After a long day, the last thing many of us want to do is reflect on our experiences and plan for the next. But what if I told you there\u0026rsquo;s a way to reap the benefits of journaling in just five minutes a day? Enter the 5-Minute Death Journal.\nInspired by positive psychology research, this journaling method is designed to help you remember more, increase productivity, and foster happiness‚Äîall in just five minutes. It may sound too good to be true, but after incorporating it into my daily routine for the past month, I can attest to its transformative power.\nThe 5-Minute Death Journal method was introduced to me by SpoonFedStudy, whose insightful YouTube video served as the catalyst for my journey. In his video titled \u0026ldquo;the most important book in your entire life,\u0026rdquo; eloquently outlines the principles and exercises of the journal, offering practical tips for implementation.\nThe journal consists of five simple exercises, each taking just one minute to complete. The first exercise, \u0026ldquo;The Deathbed Time Machine Tour of Today,\u0026rdquo; prompts you to imagine yourself at the end of your life, looking back on the day. What are the top three moments you want your older self to cherish? This exercise encourages you to find meaning in the seemingly mundane and appreciate the small joys that make life rich.\nNext, \u0026ldquo;The Wise Old Man Who Teaches You Stuff\u0026rdquo; challenges you to identify the lessons learned throughout the day. Whether it\u0026rsquo;s a profound realization or a subtle insight, there\u0026rsquo;s always something to be gained from each experience.\nThen comes \u0026ldquo;The Evil Serial Kidnapper,\u0026rdquo; a whimsical yet effective exercise in productivity. Imagine a villain holding your beloved hamster hostage, demanding three concrete tasks for the following day. By setting clear goals, you not only prioritize your time but also gain clarity and focus.\nThe fourth exercise, \u0026ldquo;The Karmic Well of Universal Awesomeness,\u0026rdquo; prompts you to reflect on the positive contributions you\u0026rsquo;ve made to the world. From small acts of kindness to meaningful gestures, every action has the power to make a difference.\nFinally, \u0026ldquo;The Internal Springs of Never-Ending Gratitude\u0026rdquo; invites you to express gratitude for the day\u0026rsquo;s blessings. Research has shown that cultivating gratitude leads to increased happiness and overall well-being, making this exercise a crucial component of the journaling process.\nBy dedicating just five minutes to these exercises each day, you\u0026rsquo;ll develop a deeper appreciation for life\u0026rsquo;s moments and cultivate a mindset of gratitude and purpose. Each day becomes a cherished friend, worthy of celebration and remembrance.\nSo why wait? Start your own journey with the 5-Minute Death Journal today, inspired by SpoonFedStudy insightful guidance. After all, every moment is precious‚Äîlet\u0026rsquo;s make the most of them together.\n","permalink":"https://bookofdaniel.in/posts/the-5-minute-death-journal-method/","summary":"\u003cp\u003eIn the hustle and bustle of our daily lives, it\u0026rsquo;s easy to let moments slip by without much thought. The idea of journaling, while appealing in theory, often feels daunting in practice. After a long day, the last thing many of us want to do is reflect on our experiences and plan for the next. But what if I told you there\u0026rsquo;s a way to reap the benefits of journaling in just five minutes a day? Enter the 5-Minute Death Journal.\u003c/p\u003e","title":"The 5 Minute Death Journal Method"},{"content":"Are you struggling to stay focused and accomplish your goals amidst a sea of distractions? Do you find yourself procrastinating more often than not? If so, you\u0026rsquo;re not alone. Many of us face similar challenges in our daily lives, especially when working from home or in a busy environment. But fear not, because today I want to share with you a simple yet powerful daily plan inspired by insights from Cal Newport\u0026rsquo;s book \u0026ldquo;Deep Work.\u0026rdquo;\nUnderstanding Deep Work\nBefore we dive into the daily plan, let\u0026rsquo;s understand the concept of deep work. Deep work refers to focused, uninterrupted work that demands your full cognitive abilities. It\u0026rsquo;s the kind of work that moves the needle and brings you closer to your goals. Newport emphasizes the importance of scheduling dedicated time for deep work every day, ideally around 4 hours, to achieve meaningful progress. The Four Key Stages of the Daily Plan\nDeep Work: This is the cornerstone of productivity. Set aside focused blocks of time for tasks that require your utmost concentration and creativity. Define clear goals for each session to maximize productivity.\nShallow Work: These are tasks that are necessary but don\u0026rsquo;t require deep concentration, such as replying to emails or attending routine meetings. Streamline or batch these tasks to free up more time for deep work.\nPersonal Goals: Allocate time each day for activities that enhance your well-being and skill development. Whether it\u0026rsquo;s exercising, learning a new language, or pursuing a hobby, investing in personal goals boosts morale and productivity.\nRelaxation Time: Don\u0026rsquo;t overlook the importance of rest and relaxation. Engage in activities that recharge you, such as spending time with loved ones, reading a book, or engaging in hobbies. A refreshed mind is more productive and creative. Benefits of a Structured Daily Plan\nImplementing a structured daily plan based on these four stages offers several benefits:\nImproved focus and productivity Reduced procrastination and distractions Enhanced work-life balance Greater satisfaction and progress towards goals Final Thoughts\nAs I\u0026rsquo;ve embarked on this journey of implementing a daily plan inspired by \u0026ldquo;Deep Work,\u0026rdquo; I\u0026rsquo;ve seen significant improvements in my focus and productivity. While it\u0026rsquo;s a work in progress, the results are promising.\nIf you\u0026rsquo;re struggling to find your rhythm or boost productivity, consider adopting a similar daily plan tailored to your needs. Remember, consistency and dedication are key to success.\nI highly recommend delving into Cal Newport\u0026rsquo;s book for more insights and strategies on focused success in a distracted world.\nThank you for reading, and I hope this simple yet effective daily plan helps you unlock your full potential and achieve your goals. If you found this post valuable, don\u0026rsquo;t forget to subscribe for more content. Here\u0026rsquo;s to a focused and successful journey ahead! Cheers! üéâ\n","permalink":"https://bookofdaniel.in/posts/a-daily-plan-for-success/","summary":"\u003cp\u003eAre you struggling to stay focused and accomplish your goals amidst a sea of distractions? Do you find yourself procrastinating more often than not? If so, you\u0026rsquo;re not alone. Many of us face similar challenges in our daily lives, especially when working from home or in a busy environment. But fear not, because today I want to share with you a simple yet powerful daily plan inspired by insights from Cal Newport\u0026rsquo;s book \u0026ldquo;Deep Work.\u0026rdquo;\u003c/p\u003e","title":"A Daily Plan for Success"},{"content":"In a world where technology is ever-evolving, programmers are constantly seeking ways to enhance their skills and productivity. A treasure trove of wisdom that is often overlooked in the tech community is Stephen Covey\u0026rsquo;s iconic book, \u0026ldquo;7 Habits of Highly Effective People.\u0026rdquo; Sahil, a YouTuber, sheds light on how this book can be a game-changer for programmers aspiring to amplify their effectiveness tenfold.\n1. The Essence of Proactivity in Programming Covey distinguishes between reactive and proactive individuals, drawing a parallel with programmers. Reactive programmers might wait for a life-changing tutorial or a list of interview questions to boost their career. In contrast, proactive programmers take ownership of their growth. They invest time in understanding the fundamentals, practicing coding, and building projects. This mindset shift from dependency to self-reliance is crucial for any programmer aiming for excellence.\n2. Start with the End in Mind Strategizing is vital, and Covey\u0026rsquo;s advice to \u0026ldquo;begin with the end in mind\u0026rdquo; resonates well with software development. Before embarking on a project, understanding the desired outcome and the project\u0026rsquo;s requirements can save countless hours. Planning not only streamlines the development process but also ensures that the project\u0026rsquo;s goals are clear and achievable.\n3. Prioritization with the Eisenhower Matrix The Eisenhower Matrix, as recommended by Covey, is a powerful tool for managing tasks based on urgency and importance. Programmers can use this matrix to navigate the often overwhelming demands of software development, focusing on tasks that are crucial for long-term success. This method encourages developers to allocate time for learning and improving, which might otherwise be neglected amid pressing deadlines.\n4. Cultivating an Abundance Mindset Covey\u0026rsquo;s emphasis on developing an abundance mentality is particularly relevant in the competitive field of software engineering. Recognizing that success is not a zero-sum game enables programmers to collaborate more effectively and create win-win situations. This mindset fosters a healthy work environment and encourages collective success.\n5. Empathetic Communication Effective communication is paramount in programming. Covey\u0026rsquo;s principle of seeking first to understand then to be understood applies to code readability and user experience. Programmers who prioritize empathy in their coding and design processes contribute to more intuitive and user-friendly applications.\n6. Leveraging Synergy The concept of synergy, where the whole is greater than the sum of its parts, is critical in software development. By valuing team members\u0026rsquo; diverse strengths and working collaboratively, programmers can achieve remarkable results that surpass what could be accomplished individually.\n7. Continuous Learning: Sharpening the Saw Lastly, Covey\u0026rsquo;s habit of \u0026ldquo;sharpening the saw\u0026rdquo; underscores the importance of continuous learning in programming. With the rapid pace of technological advancements, staying updated and acquiring new skills is essential for a programmer\u0026rsquo;s growth and relevance in the field.\nConclusion Sahil\u0026rsquo;s insightful application of \u0026ldquo;7 Habits of Highly Effective People\u0026rdquo; to programming is a testament to the book\u0026rsquo;s universal relevance. By embracing these habits, programmers can enhance their effectiveness, foster meaningful collaborations, and navigate their careers with confidence and agility. For those looking to delve deeper into building consistent programming habits, Sahil also recommends \u0026ldquo;Atomic Habits\u0026rdquo; by James Clear, further enriching the toolkit for personal and professional development.\nWatch Sahil\u0026rsquo;s video to explore these concepts in detail and join him in the journey toward becoming a 10X programmer.\n","permalink":"https://bookofdaniel.in/posts/how-programmers-can-become-10x-more-effective/","summary":"\u003cp\u003eIn a world where technology is ever-evolving, programmers are constantly seeking ways to enhance their skills and productivity. A treasure trove of wisdom that is often overlooked in the tech community is Stephen Covey\u0026rsquo;s iconic book, \u0026ldquo;7 Habits of Highly Effective People.\u0026rdquo; Sahil, a YouTuber, sheds light on how this book can be a game-changer for programmers aspiring to amplify their effectiveness tenfold.\u003c/p\u003e\n\u003ch4 id=\"1-the-essence-of-proactivity-in-programming\"\u003e1. The Essence of Proactivity in Programming\u003c/h4\u003e\n\u003cp\u003eCovey distinguishes between reactive and proactive individuals, drawing a parallel with programmers. Reactive programmers might wait for a life-changing tutorial or a list of interview questions to boost their career. In contrast, proactive programmers take ownership of their growth. They invest time in understanding the fundamentals, practicing coding, and building projects. This mindset shift from dependency to self-reliance is crucial for any programmer aiming for excellence.\u003c/p\u003e","title":"How Programmers Can Become 10X More Effective by Applying the \"7 Habits of Highly Effective People\""},{"content":"I\u0026rsquo;ve had the privilege of navigating through a wide array of technical landscapes. Yet, the recent endeavor of configuring a Windows Server Failover Cluster (WSFC) for SQL Server AlwaysOn presented a learning curve that was both steep and enriching. In this piece, I hope to share my experiences, the challenges encountered, and the lessons learned, with a spirit of humility and the intention of contributing to our collective knowledge base.\nEncountering the First Major Challenge: Adding a Cloud Witness The journey began with an attempt to integrate a cloud witness into our WSFC configuration, a task that quickly unfolded into a series of troubleshooting steps. The primary node\u0026rsquo;s refusal to fail over automatically, sticking instead in a \u0026ldquo;resolving\u0026rdquo; state, was our first major roadblock. The error message pointed towards an authentication and network recognition issue:\n\u0026ldquo;An error was encountered while modifying the quorum settings. ERROR CODE: 0x80131500; NATIVE ERROR CODE: 1. WinRM cannot process the request\u0026hellip; Cannot find the computer proddbcluster.test.com.\u0026rdquo;\nNavigating Through the Storm The resolution required a combination of technical know-how and a willingness to delve deep into the problem. Here\u0026rsquo;s how we approached it:\nStep-by-Step Cloud Witness Configuration: I leaned on a comprehensive guide to configure a cloud witness via PowerShell, a testament to the importance of following detailed instructions meticulously üîó. Ensuring Secure Communications: Verifying SSL TLS 1.2 implementation was crucial, highlighting the necessity of adhering to secure communication protocols üîó. Centralizing Control from the Owner Node: Initiating configuration changes from the owner node underscored the importance of a centralized command approach in managing cluster configurations effectively. Clearing the Slate: Prior to setting up anew, I used PowerShell scripts to clear any existing cloud witness configurations, ensuring we started from a clean slate üîó. WinRM Configuration Check: The winrm quickconfig command was a crucial step in confirming the operational status of Windows Remote Management across all nodes, a critical component for remote management üîó. Testing Node-to-Storage Communication: It was essential to confirm that there were no communication barriers between the nodes and the cloud witness storage account, foundational for the health of the cluster. Utilizing Advanced Options for Cloud Witness Addition: During the cloud witness addition, selecting advanced options and ensuring all nodes were included was a critical step for a comprehensive setup üîó. Firewall Configuration Adjustments: Adjusting the firewall settings to facilitate WinRM communication and ensuring cloud witness storage accessibility from all networks were the breakthrough moments in this journey üîó. The Challenge of Failover Resolution Another significant challenge was the primary node\u0026rsquo;s failure to automatically fail over to the secondary node during testing. This was an unexpected issue that required further investigation and adjustment.\nFinding a Path Forward The solution involved adjusting the Maximum Failover Period within the SQL Server configurations, a reminder of the importance of reviewing and fine-tuning these settings to accommodate our testing needs üîó.\nEmbracing Humility and the Joy of Learning This experience has been a humbling reminder that no matter the length of time one spends in the field, there\u0026rsquo;s always more to learn. The challenges faced and the solutions found have reinforced a few core principles:\nPrecision Matters: The importance of attention to detail in every step of the configuration process cannot be overstated. Persistence Pays Off: Facing down errors and persisting through troubleshooting is part of the journey. Each challenge is an opportunity to learn. The Value of Community: The guidance and solutions shared by the broader community have been invaluable. It\u0026rsquo;s a reminder of the strength found in collective knowledge. Final Thoughts The process of configuring WSFC for SQL Server AlwaysOn is a complex one, filled with potential pitfalls but also opportunities for growth and learning. My hope is that by sharing my journey, I can help others navigate similar challenges more smoothly. In the ever-evolving field of cloud and database architecture, each new project is a chance to expand our horizons and deepen our understanding.\n","permalink":"https://bookofdaniel.in/posts/issue-faced-on-wsfc-configuration-for-sql-server-alwayson-setup/","summary":"\u003cp\u003eI\u0026rsquo;ve had the privilege of navigating through a wide array of technical landscapes. Yet, the recent endeavor of configuring a Windows Server Failover Cluster (WSFC) for SQL Server AlwaysOn presented a learning curve that was both steep and enriching. In this piece, I hope to share my experiences, the challenges encountered, and the lessons learned, with a spirit of humility and the intention of contributing to our collective knowledge base.\u003c/p\u003e","title":"Issues Faced During WSFC Configuration for SQL Server AlwaysOn Setup"},{"content":"Hello there! If you\u0026rsquo;ve ever wondered how to make your computer tasks faster and more efficient, you\u0026rsquo;re in the right place. This blog post is all about Batch scripting - a simple yet powerful way to automate tasks in Windows.\nWhether you\u0026rsquo;re new to programming or have some experience, \u0026ldquo;The Batchology Handbook\u0026rdquo; is designed to be easy to follow and understand. We\u0026rsquo;ll start with the basics, like setting up your workspace, and gradually move into more exciting stuff, like automating file management and using cool commands you might not know about.\nBatch scripting might sound a bit technical, but I promise to keep things light and straightforward. You\u0026rsquo;ll find practical examples that you can try out yourself, and I\u0026rsquo;ll explain everything step by step. By the end of this guide, you\u0026rsquo;ll be amazed at how much you can do with just a few simple commands.\nSo, let\u0026rsquo;s get started and explore the world of Batch scripting together!\n1. Introduction Batch programming, also known as batch scripting, is a method of automating repetitive tasks in the Windows operating system. This is achieved through the creation of a batch file, a text file containing a series of commands to be executed by the command-line interpreter. Batch files are identified by their .bat or .cmd extension.\nPurpose The primary purpose of batch programming is to simplify the execution of multiple commands. It\u0026rsquo;s a powerful tool for system administrators, developers, and power users to automate routine tasks like file management, software installation, and system configuration.\nAdvantages Efficiency: Automates repetitive tasks, saving time and effort. Ease of Use: Requires no special programming skills, making it accessible for beginners. Flexibility: Can be combined with other scripts and programs for more complex operations. Resource-Friendly: Consumes minimal system resources. Basic Components Command Prompt: The environment where batch files are executed. Commands: Instructions executed sequentially in a batch file. Scripting Elements: Include loops, conditional statements, and variables. 2. Requirements Batch programming doesn\u0026rsquo;t require any additional software installations on Windows operating systems as it utilizes the built-in Command Prompt. However, to create and edit batch files efficiently, the following are recommended:\nWindows OS: Any modern version (Windows 7, 8, 10, or 11). Text Editor: Notepad (built-in), Notepad++, or any other preferred text editor, Recommended: Visual Studio Code. Configuring the Command Prompt While not strictly necessary, configuring the Command Prompt for better usability can enhance the batch scripting experience:\nAccessing Command Prompt: Press Win + R, type cmd, and press Enter. Alternatively, search for \u0026ldquo;Command Prompt\u0026rdquo; in the start menu. Customization (Optional): Right-click the title bar of the Command Prompt window. Choose \u0026lsquo;Properties\u0026rsquo; to adjust settings like font size, layout, and color scheme for better visibility. Creating Your First Batch File Open a Text Editor: Right-click on your desktop or in any folder, select \u0026ldquo;New\u0026rdquo; \u0026gt; \u0026ldquo;Text Document.\u0026rdquo; Enter a Simple Command: For example, type echo Hello, world!. Save the File with a .bat Extension: Click \u0026lsquo;File\u0026rsquo; \u0026gt; \u0026lsquo;Save As\u0026rsquo;, name your file (e.g., HelloWorld.bat), and change the \u0026lsquo;Save as type\u0026rsquo; to \u0026lsquo;All Files\u0026rsquo;. Ensure the filename ends with .bat. Run Your Batch File: Double-click the file to execute or run it from the Command Prompt. Setting File Associations (Optional) If double-clicking the .bat file doesn\u0026rsquo;t execute it, ensure that .bat files are associated with the Command Prompt: Right-click on a .bat file and select \u0026lsquo;Open with\u0026rsquo; \u0026gt; \u0026lsquo;Choose another app.\u0026rsquo; Select \u0026lsquo;More apps\u0026rsquo;, scroll down, and choose \u0026lsquo;Look for another app on this PC.\u0026rsquo; Navigate to C:\\Windows\\System32 and select cmd.exe. Tips File Locations: Store your batch files in a dedicated folder for easy management. Testing: Always test new scripts in a controlled environment to prevent unintended actions. 3. Basic Commands Batch files execute a series of Command Prompt commands. Here, we\u0026rsquo;ll explore some fundamental commands that form the building blocks of batch scripting.\nCommand Purpose Example Description Echo Displays messages or turns command echoing on/off. echo Hello, world! Displays \u0026ldquo;Hello, world!\u0026rdquo; in Command Prompt. REM Adds comments in the script for readability. REM This is a comment Ignored during execution, used for notes. SET Creates or changes environment variables. SET name=John Sets the value of name to \u0026ldquo;John.\u0026rdquo; GOTO Directs the script to another section. GOTO END Jumps to the label :END in the script. Labels Marks a section of the script. :END Used with GOTO to create script sections. PAUSE Pauses the script, waiting for a key press. PAUSE Used for testing or script pauses. EXIT Exits the Command Prompt or a script. EXIT Closes the Command Prompt window in a script. Sample Batch File @ECHO OFF REM Sample batch script SET name=John echo Hello, %name%! PAUSE GOTO END :END echo Script is ending... EXIT This script introduces basic commands, including setting a variable, displaying messages, and controlling the script flow.\n4. Advanced Scripting Techniques After mastering basic commands, you can enhance your batch scripts with more sophisticated techniques. These advanced methods allow for greater flexibility and functionality in your scripts.\nUsing Variables Dynamic Variable Assignment:Variables can be set based on user input or other commands. Example: SET /P username=Enter your username: This command prompts the user to enter a username, which is stored in the username variable. Manipulating Strings: Batch scripts support basic string manipulation. Example: SET fullname=%firstname% %lastname% Concatenates two variables to create a full name. Conditional Statements IF Command: Used for making decisions in scripts. Syntax: IF [condition] [command] Example: IF %age% LEQ 18 echo You are a minor. Checks if the value of age is less than or equal to 18. Using ELSE: To specify an alternative action if the IF condition is false. Example: IF %number% EQU 10 (echo Number is 10) ELSE (echo Number is not 10) Loops FOR Command: Executes a command for each item in a set. Syntax: FOR %%parameter IN (set) DO command Example: FOR %%G IN (*.txt) DO echo %%G This will echo the name of each .txt file in the current directory. Subroutines CALL Command: Calls another batch file or a label within the same file. Example: CALL :subroutine Calls a label named :subroutine within the script. Creating a Subroutine: Example: :subroutine echo This is a subroutine. GOTO :EOF Error Handling ErrorLevel: Used to check the status of the last executed command. Example: IF %ERRORLEVEL% NEQ 0 echo Error occurred. Using EXIT /B: Exits the script or subroutine without closing the Command Prompt. Example: EXIT /B 1 Exits the subroutine or script and sets the ErrorLevel to 1. Delay Execution TIMEOUT Command: Pauses the script for a specified number of seconds. Example: TIMEOUT /T 10 Pauses the script for 10 seconds. Sample Advanced Batch File @ECHO OFF SET /P userinput=Enter a number: IF %userinput% EQU 10 ( echo The number is 10. ) ELSE ( echo The number is not 10. ) FOR %%G IN (*.docx) DO echo Found document: %%G CALL :subroutine EXIT :subroutine echo This is a subroutine. GOTO :EOF This script combines various advanced techniques, showcasing conditional logic, loops, subroutines, and user input.\nConclusion Advanced scripting techniques in batch programming significantly enhance the capabilities of your scripts, allowing you to automate more complex tasks and handle various scenarios efficiently.\n4. File Operations File operations are crucial in batch programming for managing files and directories. This section covers commands to perform various file operations such as creating, copying, moving, and deleting files and directories.\nCreating Files and Directories Creating a New File: Command: type nul \u0026gt; filename.ext Example: type nul \u0026gt; example.txt Creates an empty file named example.txt. Creating a Directory: Command: mkdir directoryname Example: mkdir MyFolder Creates a new directory named MyFolder. Copying Files and Directories Copying a File: Command: copy source destination Example: copy example.txt D:\\Backup\\example.txt Copies example.txt to the D:\\Backup directory. Copying a Directory: Command: xcopy source destination /E /H /C /I Example: xcopy MyFolder D:\\Backup\\MyFolder /E /H /C /I Recursively copies MyFolder to D:\\Backup, including hidden files and subdirectories. Moving and Renaming Files and Directories Moving a File: Command: move source destination Example: move example.txt D:\\Archive Moves example.txt to D:\\Archive. Renaming a File or Directory: Command: ren oldname newname Example: ren example.txt new_example.txt Renames example.txt to new_example.txt. Deleting Files and Directories Deleting a File: Command: del filename Example: del example.txt Deletes example.txt. Deleting a Directory: - Command: rmdir /S /Q directoryname - Example: rmdir /S /Q MyFolder Deletes MyFolder and all of its contents. Working with File Attributes Changing File Attributes: Command: attrib [+|-][R|A|S|H] filename Example: attrib +R example.txt Sets the read-only attribute to example.txt. Sample Script for File Operations @ECHO OFF mkdir MyDocuments type nul \u0026gt; MyDocuments\\doc1.txt xcopy MyDocuments D:\\Backup\\MyDocuments /E /H /C /I move MyDocuments\\doc1.txt MyDocuments\\document.txt del MyDocuments\\doc1.txt rmdir /S /Q MyDocuments This script demonstrates creating a directory and a file, copying the directory, renaming a file, deleting the file, and finally deleting the directory.\nConclusion Understanding file operations in batch programming is essential for effective script writing, especially when handling bulk file management tasks. These operations lay the foundation for automating routine file management tasks in a Windows environment.\n5. Cool Commands In addition to basic file operations and scripting techniques, batch programming offers a plethora of \u0026ldquo;cool commands\u0026rdquo; that can be used for a wide range of purposes, from system administration to advanced file manipulation. Let\u0026rsquo;s explore some of these commands and their functionalities.\nCommand Purpose Usage Reference Link FC Compares two files and displays the differences. FC file1 file2 üîó FORFILES Executes a command on each file in a set of files. FORFILES /P directory /M searchmask /C cmd üîó HELP Provides help information for batch commands. HELP command üîó MSG Sends a message to a user or session. MSG [username] üîó PROMPT Changes the command prompt. PROMPT [text] üîó QUERY Displays the status of a specified service/session. QUERY session üîó REG Manages registry keys and values. REG QUERY key üîó RUNAS Executes a program under a different user account. RUNAS /USER:user program üîó SC Manages Windows services. SC command service_name üîó SCHTASKS Schedules commands/programs to run periodically. SCHTASKS /CREATE /SC schedule /TN task /TR run üîó SHORTCUT Creates a Windows shortcut. SHORTCUT /F:filename /A:C /T:targetpath üîó SHUTDOWN Shuts down or restarts a computer. SHUTDOWN /s /t time üîó SUBST Associates a path with a drive letter. SUBST drive: path üîó SYSTEMINFO Displays detailed system information. SYSTEMINFO üîó TAKEOWN Takes ownership of a file or folder. TAKEOWN /F file_or_folder üîó TASKKILL Ends tasks or processes. TASKKILL /IM imagename üîó TASKLIST Displays a list of currently running tasks. TASKLIST üîó TELNET Communicates using the Telnet protocol. TELNET [host] [port] üîó TREE Graphically displays folder structure. TREE path üîó TSDISCON Disconnects a Terminal Services session. TSDISCON sessionid üîó WHERE Locates and displays file paths matching a pattern. WHERE pattern üîó WHOAMI Displays user, group, and privilege information. WHOAMI üîó References Windows CMD Command Syntax - SS64.com An A-Z Index of Windows CMD commands - SS64.com ","permalink":"https://bookofdaniel.in/posts/the-batchology-handbook/","summary":"\u003cp\u003eHello there! If you\u0026rsquo;ve ever wondered how to make your computer tasks faster and more efficient, you\u0026rsquo;re in the right place. This blog post is all about Batch scripting - a simple yet powerful way to automate tasks in Windows.\u003c/p\u003e\n\u003cp\u003eWhether you\u0026rsquo;re new to programming or have some experience, \u0026ldquo;The Batchology Handbook\u0026rdquo; is designed to be easy to follow and understand. We\u0026rsquo;ll start with the basics, like setting up your workspace, and gradually move into more exciting stuff, like automating file management and using cool commands you might not know about.\u003c/p\u003e","title":"The Batchology Handbook: Mastering the Art of Batch Scripting in Windows"},{"content":"We all aspire to be better, stronger, smarter, and healthier, yet many struggle to stick with their goals. In this post, we\u0026rsquo;ll explore a practical framework that addresses this common issue, allowing you to effortlessly surpass your goals. This system has been successfully used to develop the discipline needed to hold a 15-minute plank every day. Join me on this journey, and let\u0026rsquo;s unlock your raw power together.\nStep 1: Action-Oriented Goals Instead of setting vague and impractical goals, focus on specific actions within a defined time frame. For instance, rather than aiming to lose 20 pounds, set a goal to complete 10,000 push-ups by the end of the year. Break down the goal into actionable steps, visualize each step, and establish a non-negotiable daily minimum to make progress.\nExamples:\nGoal: Learn computer programming Action: Code 25 functional programs Goal: Get into a relationship Action: Have 100 interesting conversations with strangers Goal: Become a famous YouTuber Action: Make 100 videos packed with value Step 2: Minimize Friction Identify and eliminate real and imagined friction in your journey. If the gym is too far away, make the ground you stand on your gym. Remove unnecessary obstacles, such as the need for a personal trainer. Break down actions into 5-minute chunks, making them easily achievable and minimally taxing on your resources.\nExamples:\nGym too far away? Your ground is the gym. No personal trainer? Do 10 burpees without one. Intimidated by learning Spanish? Start with one word a day. Step 3: Anchor and Toll Anchor your new habits to existing routines in your daily life. Create a toll system where you attach a cost to certain activities, making them conditional on completing specific tasks. For example, learn one Spanish word before accessing the shower. Get creative with tolls to reinforce positive behaviors.\nExamples:\nAnchor: Listen to audiobooks while taking dumps. Toll: 10 push-ups before playing video games. Toll: Learn one Spanish word before taking a shower. Step 4: Pavlov\u0026rsquo;s Dog Conditioning Pair your daily actions with something incredibly enjoyable, creating a Pavlovian association. Follow the sacred rule: only engage in the enjoyable activity when performing the specified action. This conditions your brain to crave the task, making it enjoyable and effortless.\nExample:\nSacred rule: Play a phone game only while holding a plank. Step 5: The Feather Approach Gradually introduce small increments to your routine, making them feel almost negligible. If you started with a 20-second plank, increase to 25 seconds. This feather approach ensures a smooth transition, preventing burnout. Repeat this process until the combined weight of your actions becomes second nature.\nStep 6: Build the Skyscraper Continuously add new Lego bricks and feathers to your routine, creating a skyscraper of positive habits. With a solid anchor or toll, these habits will seamlessly integrate into your daily life. The compounding effect of these habits will lead to exponential growth and make you unstoppable in pursuing larger projects and goals.\nConclusion: By following this step-by-step framework, you can transform your life and become unstoppable in achieving your goals. The key is consistency, creativity, and a commitment to building a skyscraper of positive habits that elevate you to new heights. Start today, and watch as the compounding power of these habits propels you towards a limitless future.\nSource: https://youtu.be/fesslaSxdqs?si=40VEBURtZr6X9yJf\n","permalink":"https://bookofdaniel.in/posts/the-unstoppable-framework/","summary":"\u003cp\u003eWe all aspire to be better, stronger, smarter, and healthier, yet many struggle to stick with their goals. In this post, we\u0026rsquo;ll explore a practical framework that addresses this common issue, allowing you to effortlessly surpass your goals. This system has been successfully used to develop the discipline needed to hold a 15-minute plank every day. Join me on this journey, and let\u0026rsquo;s unlock your raw power together.\u003c/p\u003e\n\u003ch2 id=\"step-1-action-oriented-goals\"\u003eStep 1: Action-Oriented Goals\u003c/h2\u003e\n\u003cp\u003eInstead of setting vague and impractical goals, focus on specific actions within a defined time frame. For instance, rather than aiming to lose 20 pounds, set a goal to complete 10,000 push-ups by the end of the year. Break down the goal into actionable steps, visualize each step, and establish a non-negotiable daily minimum to make progress.\u003c/p\u003e","title":"The Unstoppable Framework: A Step-by-Step Guide to Achieving Your Goals"},{"content":"In a recent YouTube video, a former WhatsApp engineer shared his insights on success in the tech industry and the key lessons he learned during his time at the company. Here are the main takeaways from his experience, formatted with lists and paragraphs:\nKey Lessons Learned Influence and Likability: Influence in the tech industry is about being likable and credible. People who are dependable and pleasant to work with are more likely to be sought after for new opportunities and promotions. Focus on building a strong reputation by being a good team player and delivering results. üëç\nChoose Your Passion: The engineer emphasized the importance of choosing one thing to be known for and following your passions. Blindly following trends or chasing popularity can lead to failure. Instead, focus on your strengths and interests to drive your success. üî•\nSimplicity and Prioritization: WhatsApp\u0026rsquo;s engineering culture was centered around simplicity and prioritization. The company focused on a few core features and offerings, ensuring that everything worked seamlessly and efficiently. This approach helped WhatsApp grow from 20 million to 1.2 billion active monthly users. üöÄ\nUnderstanding Your Audience: WhatsApp\u0026rsquo;s success was also attributed to its ability to prioritize user experience. The company designed its app to work well even for users with poor internet connections and old devices, ensuring that everyone could use the platform. üì±\nClear Communication and Goal Setting: WhatsApp\u0026rsquo;s engineering culture emphasized clear communication and goal setting. The company\u0026rsquo;s leadership provided clear messaging about which features to prioritize, and engineers were encouraged to understand and align with the company\u0026rsquo;s long-term goals. üìà\nAdapting to Change: The engineer\u0026rsquo;s experience with WhatsApp\u0026rsquo;s acquisition by Facebook demonstrated the importance of adapting to change in the tech industry. Embrace new opportunities and learn from them, as they can lead to personal and professional growth. üåü\nQuote of the Video \u0026ldquo;I do recommend, even if you are really good at all of them, please choose one thing what do you want to be known for as.\u0026rdquo;\nChannel Credit Don\u0026rsquo;t forget to subscribe this channel that shared this valuable content: Entrepreneurship Opportunities. üëè\nIn conclusion, success in the tech industry requires a combination of influence, passion, simplicity, and prioritization. By focusing on these principles and learning from the experiences of others, you can increase your chances of success and create a meaningful career for yourself in the technology sector.\n","permalink":"https://bookofdaniel.in/posts/lessons-learned-from-a-former-whatsapp-engineer/","summary":"\u003cp\u003eIn a recent YouTube video, a former WhatsApp engineer shared his insights on success in the tech industry and the key lessons he learned during his time at the company. Here are the main takeaways from his experience, formatted with lists and paragraphs:\u003c/p\u003e\n\u003ch3 id=\"key-lessons-learned\"\u003eKey Lessons Learned\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInfluence and Likability\u003c/strong\u003e: Influence in the tech industry is about being likable and credible. People who are dependable and pleasant to work with are more likely to be sought after for new opportunities and promotions. Focus on building a strong reputation by being a good team player and delivering results. üëç\u003c/p\u003e","title":"Lessons Learned from a Former WhatsApp Engineer: Influence, Passion, and Prioritization in the Tech Industry üåü"},{"content":"Picture it: your bed, a chaotic landscape of tangled limbs and rogue pillows, mirroring the state of your life. Enter Admiral William McRaven\u0026rsquo;s \u0026ldquo;Make Your Bed,\u0026rdquo; a book that\u0026rsquo;s anything but your average fluff-and-fold self-help manual. Think less feng shui, more forging a battle cry in the crucible of Navy SEAL training. This ain\u0026rsquo;t about smoothing out your duvet; it\u0026rsquo;s about scaling metaphorical Mount Everest, one messy pillowcase at a time.\nMcRaven throws down ten life lessons that sting with truth and resonate with anyone who\u0026rsquo;s ever wrestled with life\u0026rsquo;s tangled sheets.\nLesson 1: Start small, conquer big. Making your bed is a tiny triumph, but it sends a ripple of accomplishment through your day. It whispers, \u0026ldquo;I can do this,\u0026rdquo; a mantra that empowers you to tackle seemingly insurmountable obstacles. You know, like that presentation that feels like climbing Everest in stilettos. One sheet at a time, you\u0026rsquo;ll conquer it.\nIf you want to change the world, start off by making your bed.\nLesson 2: Teamwork makes the dream (and boat) work. No SEAL goes solo. They rely on their squad, their cheerleaders with flippers, their \u0026ldquo;band of brothers\u0026rdquo; who navigate the choppy waters together. This translates to life as well. Find your tribe, those who\u0026rsquo;ll high-five your victories and hold your paddle when the current gets rough. Remember, even the mightiest battleship needs an anchor.\nIf you want to change the world, find someone to help you paddle.\nLesson 3: Size doesn\u0026rsquo;t matter, heart does. Forget judging people by their bank accounts or biceps. McRaven reminds us that true strength lies in character, resilience, and the size of your (figurative) heart. A kind word from the smallest person can change the world, so spread kindness like confetti. Remember, even the tiniest pebble can create the biggest ripples.\nIf you want to change the world, find someone to help you paddle. If you want to change the world, measure a person by the size of their heart, not the size of their flippers.\nLesson 4: Life is a sugar cookie crumble (and that\u0026rsquo;s okay). Sometimes, no matter how hard you bake, your life cookie crumbles. You bomb a presentation, trip in public, or stub your toe on the existential dread of the universe. It happens. The key is to dust yourself off, lick the icing sugar tears, and keep baking. Remember, even the most perfectly decorated cookies sometimes fall apart.\nIf you want to change the world, get over being a sugar cookie and keep moving.\nLesson 5: Embrace the circus (of failure). Life is a three-ring spectacle of awkward moments, missed opportunities, and flat tires. Don\u0026rsquo;t let fear of failure paralyze you. Embrace the stumbles, learn from the clowns, and remember, even the most graceful tightrope walkers wobble sometimes. So stumble gracefully, laugh at your mishaps, and remember, falling down is just another way of flying (sort of).\nIf you want to change the world, don‚Äôt be afraid of the Circuses.\nLesson 6: Sometimes, headfirst is best. Take risks, challenge the status quo, and don\u0026rsquo;t be afraid to slide headfirst down the metaphorical rope of life. You might surprise yourself, break a record, and land with a grin on your face (and maybe a few scrapes). Remember, playing it safe won\u0026rsquo;t get you to the trapeze, so swing boldly, even if it means a few bumps on the way.\nIf you want to change the world, sometimes you have to slide down the obstacle headfirst.\nLesson 7: Sharks are inevitable (but you can swim with them). The world is full of metaphorical sharks: bullies, doubters, and negativity. You can\u0026rsquo;t avoid them, but you can learn to navigate their waters. Stand your ground, be your best self, and remember, sometimes the best way to disarm a shark is with a confident smile and a well-placed tuna sandwich. Just like dolphins, swim with the sharks, but don\u0026rsquo;t become one.\nIf you want to change the world, don‚Äôt back down from the sharks.\nLesson 8: Be your lighthouse in the storm. When darkness descends, don\u0026rsquo;t let it extinguish your inner light. Stay calm, draw on your inner strength, and remember, even the faintest glimmer can guide others through the roughest seas. Be the beacon in the storm, the lighthouse in the fog, the reminder that even when the waves crash, hope keeps us afloat.\nIf you want to change the world, you must be your very best in the darkest moment.\nLesson 9: Sing in the mud (it helps). Hope is a powerful weapon. When you\u0026rsquo;re neck-deep in life\u0026rsquo;s muck, find a reason to sing. Share your light, inspire others, and remember, even a muddy chorus can change the world. So hum a tune, even if it\u0026rsquo;s off-key, because sometimes the only way to rise above the mud is to sing through it.\nIf you want to change the world, start singing when you‚Äôre up to your neck in mud.\nLesson 10: Never ring the bell (unless it\u0026rsquo;s for tea). Life throws challenges, but quitting is not an option. Push through the discomfort, persevere through the pain, and remember, the bell is always there, but the satisfaction of overcoming your fears is far sweeter than any escape. So keep climbing, keep fighting, keep singing in the mud, because the view from the top of the mountain is worth\nIf you want to change the world, don‚Äôt ever, ever ring the bell.\n","permalink":"https://bookofdaniel.in/posts/make-your-bed-summary/","summary":"\u003cp\u003ePicture it: your bed, a chaotic landscape of tangled limbs and rogue pillows, mirroring the state of your life. Enter \u003ca href=\"https://en.wikipedia.org/wiki/William_H._McRaven\"\u003eAdmiral William McRaven\u0026rsquo;s\u003c/a\u003e \u0026ldquo;\u003ca href=\"https://www.amazon.in/Make-Your-Bed-William-McRaven/dp/0718188861\"\u003eMake Your Bed\u003c/a\u003e,\u0026rdquo; a book that\u0026rsquo;s anything but your average fluff-and-fold self-help manual. Think less feng shui, more forging a battle cry in the crucible of Navy SEAL training. This ain\u0026rsquo;t about smoothing out your duvet; it\u0026rsquo;s about scaling metaphorical Mount Everest, one messy pillowcase at a time.\u003c/p\u003e","title":"From Messy Sheets to Mountain Everest: Navigating Life with Lessons from a Navy SEAL"},{"content":"In our current era of remote work and virtual learning, prioritizing physical and mental fitness is more crucial than ever. Luckily, there are numerous strategies you can incorporate into your daily routine without stepping outside your home. The following 15 ways, inspired by the insightful content from a YouTube video by Warikoo, will guide you towards a healthier and more balanced lifestyle.\nBasic Fitness Sit Straight: Maintain a good posture while working or studying. Use a memory foam pillow or a similar support to keep your back straight and avoid slouching.\nMaintain a Consistent Sleep Schedule: Your body should know when to sleep and when to wake up. Aim for at least 6-7 hours of sleep every day to ensure you are mentally and physically rested.\nEat Properly: Avoid eating out of boredom or for entertainment. Limit sugar and fried food, and try to eat three balanced meals a day. Stop eating three hours before sleeping and include as much protein as you can in your diet.\nStay Hydrated: Drink at least five, ideally eight glasses of water a day to keep your body hydrated and to help it absorb nutrients.\nCreate a Morning Routine: Start your day by dedicating time to yourself. This could include meditation, reading, exercising, or any other activity that you enjoy.\nPhysical Fitness Body Weight Exercises: These are exercises that you can do using your own body weight, such as push-ups, squats, tricep dips, and planks.\nResistance Band Exercises: Resistance bands can be used to add difficulty to your workouts. They come in different weights and can be used for exercises like bicep curls and shoulder exercises.\nWeights: If you\u0026rsquo;re ready for a more challenging workout, consider buying weights. You can use them for exercises like shoulder lifts and bicep curls.\nAnkle Weights: Ankle weights can be used to add resistance to your normal activities, like walking up and down the stairs.\nPomodoro Technique: This technique involves working for 25 minutes, then taking a 5-minute break. During your break, you could climb stairs or do other physical activities.\nMental Fitness Meditation: Meditation can help you become more aware of your thoughts. It involves focusing on your breath and observing your thoughts without trying to control them.\nJournaling: Writing about your emotions can help you understand them better. Try to write about any significant emotions you felt during the day and why you felt that way.\nGratitude Jar: Keep a jar where you write down things you\u0026rsquo;re grateful for. On difficult days, you can read these notes to remind yourself of the positive aspects of your life.\nBreathing Exercises: Whenever you feel stressed or need a break, take a moment to focus on your breath. This can help slow down your heart rate and provide your body with oxygen.\nCreate a Personal Space: Designate a space in your house where you can connect with yourself and your thoughts. This could be a chair, a corner, or a table.\nRemember, staying fit at home doesn\u0026rsquo;t require expensive equipment or a gym membership. All you need is a little creativity and the determination to maintain a healthy lifestyle.\n","permalink":"https://bookofdaniel.in/posts/15-ways-to-stay-fit-while-sitting-at-home/","summary":"\u003cp\u003eIn our current era of remote work and virtual learning, prioritizing physical and mental fitness is more crucial than ever. Luckily, there are numerous strategies you can incorporate into your daily routine without stepping outside your home. The following 15 ways, inspired by the insightful content from a YouTube video by \u003ca href=\"https://youtu.be/FKNRNY5ji_M?si=ueCpV5g2vEtG5bjS\"\u003eWarikoo\u003c/a\u003e, will guide you towards a healthier and more balanced lifestyle.\u003c/p\u003e\n\u003ch2 id=\"basic-fitness\"\u003eBasic Fitness\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSit Straight\u003c/strong\u003e: Maintain a good posture while working or studying. Use a memory foam pillow or a similar support to keep your back straight and avoid slouching.\u003c/p\u003e","title":"15 Ways to Stay Fit While Sitting at Home"},{"content":"Vim is a highly configurable text editor used in programming. It\u0026rsquo;s known for its efficiency, enabling users to navigate and edit documents with minimal use of the mouse. This blog post will introduce you to the most important Vim shortcuts and commands, making your coding journey smoother and more efficient.\nUnderstanding Vim Modes Before we dive into the commands, it\u0026rsquo;s crucial to understand Vim\u0026rsquo;s modes. Vim operates in several modes, each with a specific purpose:\nNormal Mode: The default mode when you open Vim. It\u0026rsquo;s used to execute commands. Insert Mode: Allows you to insert text into your document. Visual Mode: Used for selecting lines, blocks, and text in your document. Command-Line Mode: Allows you to enter Vim commands and/or search for text. You can switch between these modes using various commands, which we\u0026rsquo;ll cover in the following sections[1][2][4][9].\nCursor Movement In normal mode, you can move the cursor around with the following commands:\nh: move cursor left j: move cursor down k: move cursor up l: move cursor right w: jump forwards to the start of a word b: jump backwards to the start of a word 0: jump to the start of the line $: jump to the end of the line gg: go to the first line of the document G: go to the last line of the document Insert Mode To enter insert mode from normal mode, use the following commands:\ni: enter insert mode before the cursor a: enter insert mode after the cursor A: enter insert mode at the end of the line o: open a new line below the current line and enter insert mode O: open a new line above the current line and enter insert mode To return to normal mode from insert mode, press Esc.\nVisual Mode To select text, you can switch to visual mode using these commands:\nv: enter visual mode V: enter linewise visual mode Ctrl + v: enter blockwise visual mode Cut, Copy, and Paste Vim uses different terminology for these operations:\nx: delete character under the cursor dd: delete line D: delete from cursor to end of line yy: yank (copy) line p: paste after the cursor P: paste before the cursor Undo and Redo To undo or redo changes, use the following commands:\nu: undo Ctrl + r: redo Search and Replace Vim provides powerful search and replace functionality:\n/pattern: search for a pattern n: move to the next match N: move to the previous match :%s/old/new/g: replace all occurrences of \u0026lsquo;old\u0026rsquo; with \u0026rsquo;new\u0026rsquo; in the entire file File Operations You can perform various file operations using these commands:\n:e {file}: edit another file :w: write (save) file :wq: write file and exit :q!: exit without saving Miscellaneous Here are some additional useful commands:\n:help {keyword}: open help for a keyword :set number: show line numbers :set nonumber: hide line numbers :split {file}: split the window horizontally and open a file :vsplit {file}: split the window vertically and open a file Ctrl + w + arrow keys: navigate between split windows This guide covers the most commonly used Vim commands. As you become more proficient with Vim, you can explore additional commands and functionalities. Happy coding!\nReferences: [1] https://www.linuxfoundation.org/blog/blog/classic-sysadmin-vim-101-a-beginners-guide-to-vim [2] https://opensource.com/article/19/3/getting-started-vim [3] https://www.reddit.com/r/vim/comments/166q64q/an_effective_beginner_vim_tutorial_focusing_on/ [4] https://coderwall.com/p/adv71w/basic-vim-commands-for-getting-started [5] https://youtube.com/watch?v=ggSyF1SVFr4 [6] https://danielmiessler.com/p/vim/ [7] https://www.unomaha.edu/college-of-information-science-and-technology/computer-science-learning-center/_files/resources/CSLC-Helpdocs-Vim.pdf [8] https://www.reddit.com/r/vim/comments/lbjw3u/good_guides_on_vim/ [9] https://www.freecodecamp.org/news/vim-beginners-guide/ [10] https://www.reddit.com/r/vim/comments/k60da0/best_vim_tutorial_for_beginners/ [11] https://vim.rtorr.com [12] https://www.howtoforge.com/vim-basics [13] https://thevaluable.dev/vim-commands-beginner/ [14] https://youtube.com/watch?v=RZ4p-saaQkc [15] https://linuxhandbook.com/basic-vim-commands/ ","permalink":"https://bookofdaniel.in/posts/a-beginners-guide-to-vim-shortcuts/","summary":"\u003cp\u003eVim is a highly configurable text editor used in programming. It\u0026rsquo;s known for its efficiency, enabling users to navigate and edit documents with minimal use of the mouse. This blog post will introduce you to the most important Vim shortcuts and commands, making your coding journey smoother and more efficient.\u003c/p\u003e\n\u003ch2 id=\"understanding-vim-modes\"\u003eUnderstanding Vim Modes\u003c/h2\u003e\n\u003cp\u003eBefore we dive into the commands, it\u0026rsquo;s crucial to understand Vim\u0026rsquo;s modes. Vim operates in several modes, each with a specific purpose:\u003c/p\u003e","title":"A Beginners Guide to Vim Shortcuts"},{"content":"Original Post : https://cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it\nIn the dynamic world of cloud computing, the term \u0026lsquo;cloud-native architecture\u0026rsquo; is often synonymous with innovation and efficiency. This article, inspired by a Google Cloud blog post, expands the concept to encompass universal principles applicable across all cloud platforms, be it Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). üå¶Ô∏èüèóÔ∏è\nUnderstanding Cloud-Native Architecture Cloud-native architecture is about leveraging the unique capabilities of cloud environments. It goes beyond mere migration to the cloud; it\u0026rsquo;s about optimizing applications to thrive in this dynamic ecosystem. üå©Ô∏èüí°\nKey aspects include:\nFunctional Requirements: What the system needs to do. (e.g., processing orders) Non-Functional Requirements: Performance metrics like processing speed and efficiency. Constraints: Limitations or fixed aspects within which the system must operate. In a cloud-native context, these elements are approached with flexibility and innovation, ensuring systems are resilient, cost-effective, and easily maintainable. üõ†Ô∏è‚öñÔ∏è\nFive Universal Principles of Cloud-Native Architecture Design for Automation: Automation is key in cloud-native systems. Embrace tools and practices that facilitate automated infrastructure management, deployment, and scaling. Think Jenkins, Terraform, or GitLab CI for streamlined operations. ü§ñüîÑ\nBe Smart with State: Prioritize stateless components. They are easier to scale, repair, roll back, and balance. Stateless design simplifies complex cloud environments. üìäüîÅ\nFavor Managed Services: Opt for cloud-managed services when possible. They offer convenience and efficiency, reducing the headache of backend management. The choice between open-source compatible services, highly beneficial managed services, and others should be weighed against organizational needs. ‚òÅÔ∏èüõ†Ô∏è\nPractice Defense in Depth: In the cloud, security is multi-layered. Implement robust authentication, rate limiting, and protective measures across all components. This reduces vulnerability and strengthens the overall security posture. üõ°Ô∏èüîí\nAlways Be Architecting: Cloud architecture is an evolving journey. Adapt and refine your architecture in response to changing needs and technologies. This mindset ensures your systems remain relevant and efficient. üîÑüåü\nAdapting to Change In the cloud, adaptation is not just a strategy; it\u0026rsquo;s a necessity for survival. Like evolving species, cloud architectures must continuously adapt to their environment. This evolution isn\u0026rsquo;t a linear journey but a constant, dynamic process. üåøüìà\nConclusion Adopting cloud-native architecture principles isn\u0026rsquo;t just about leveraging new technologies; it\u0026rsquo;s about embracing a mindset of continual adaptation and optimization. These universal principles guide you to make the most of any cloud environment, ensuring your systems are resilient, agile, and future-ready. Change might be challenging, but it\u0026rsquo;s the key to thriving in the ever-evolving cloud landscape. üí™‚òÅÔ∏è\n","permalink":"https://bookofdaniel.in/posts/key-principles-of-any-cloud-environments/","summary":"\u003cp\u003eOriginal Post : \u003ca href=\"https://cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it\"\u003ehttps://cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIn the dynamic world of cloud computing, the term \u0026lsquo;cloud-native architecture\u0026rsquo; is often synonymous with innovation and efficiency. This article, inspired by a Google Cloud blog post, expands the concept to encompass universal principles applicable across all cloud platforms, be it Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). üå¶Ô∏èüèóÔ∏è\u003c/p\u003e\n\u003ch3 id=\"understanding-cloud-native-architecture\"\u003eUnderstanding Cloud-Native Architecture\u003c/h3\u003e\n\u003cp\u003eCloud-native architecture is about leveraging the unique capabilities of cloud environments. It goes beyond mere migration to the cloud; it\u0026rsquo;s about optimizing applications to thrive in this dynamic ecosystem. üå©Ô∏èüí°\u003c/p\u003e","title":"5 principles for cloud-native architecture: An Inspired Post üî•"},{"content":"The early days of the internet were marked by a sense of freedom and innovation. Platforms like Twitter, founded by Jack Dorsey and others, were initially open-source ecosystems where developers could contribute, leading to features like hashtags and trending topics. This era was characterized by decentralized protocols, like email, allowing for diverse applications and innovation.\nHowever, as platforms like Facebook, Twitter, and Google grew, they started organizing this vast information, turning social media into centralized repositories. This shift was efficient but led to the accumulation of power in the hands of a few companies, creating \u0026lsquo;walled gardens\u0026rsquo; where user data became a commodity for advertising revenue.\n\u0026ldquo;The digital revolution is not about technology; it\u0026rsquo;s about people.\u0026rdquo; - Howard Rheingold\nThe Problem: Addiction and Control üïπÔ∏è The primary business model of social media companies pivoted to advertising, fostering designs that promote addiction - the infinite scroll, tailored feeds, etc. This change led to a significant shift in how content is delivered and consumed, prioritizing user retention over quality content. Moreover, this centralization led to concerns about censorship and data privacy, as these platforms gained control over what content is visible and who gets to see it.\nEnter NOSTR: A Glimpse of Hope ‚ú® Amidst these challenges, NOSTR emerged as a potential game-changer. Endorsed by influential figures like Edward Snowden and Twitter\u0026rsquo;s founder Jack Dorsey, NOSTR aims to decentralize social media. By creating an open protocol, it allows for a distribution of control, returning power to the users.\nNOSTR functions similarly to the email protocol, where data is decentralized, and various apps can access and display this data in unique ways. This system enables users to choose how they consume content, bypassing the limitations and biases of a centralized platform\u0026rsquo;s algorithm.\nThe Revolutionary Potential of NOSTR üí™üèª NOSTR\u0026rsquo;s decentralized nature promises several benefits:\nUser Control and Innovation: Users can choose or even build applications that suit their preferences, fostering innovation. Data Ownership: Users have control over their data, reducing dependency on any single platform. Diverse Perspectives: Decentralization allows for a more diverse range of voices and content, reducing the echo chamber effect prevalent in current social media. The Broader Impact üî• NOSTR isn\u0026rsquo;t just about social media; it\u0026rsquo;s about redefining digital communication and interaction. From messaging to business services, its underlying protocol could revolutionize various aspects of our digital lives.\nConclusion: A Call to Action and Participation The journey of NOSTR is not just about technology; it\u0026rsquo;s about community participation and democratizing the digital space. As an open-source project, it invites contributions from everyone, echoing the early days of the internet where innovation and freedom were paramount.\nThe YouTube video, while informative, is just the tip of the iceberg. For those intrigued by NOSTR and its potential, the invitation is open to explore, contribute, and be part of this exciting new phase in the evolution of social media and digital communication.\n","permalink":"https://bookofdaniel.in/posts/the-rise-of-nostr-and-the-renaissance-of-social-media/","summary":"\u003cp\u003eThe early days of the internet were marked by a sense of freedom and innovation. Platforms like Twitter, founded by Jack Dorsey and others, were initially open-source ecosystems where developers could contribute, leading to features like hashtags and trending topics. This era was characterized by decentralized protocols, like email, allowing for diverse applications and innovation.\u003c/p\u003e\n\u003cp\u003eHowever, as platforms like Facebook, Twitter, and Google grew, they started organizing this vast information, turning social media into centralized repositories. This shift was efficient but led to the accumulation of power in the hands of a few companies, creating \u0026lsquo;walled gardens\u0026rsquo; where user data became a commodity for advertising revenue.\u003c/p\u003e","title":"Reclaiming Our Digital Space: The Rise of NOSTR and the Renaissance of Social Media üí™üèª"},{"content":" \u0026ldquo;Minimalism is not about having less. It\u0026rsquo;s about making room for more of what matters.\u0026rdquo; - Unknown\nWhen I first explored minimalism, I was struck by a pattern: many minimalists, myself included, often opt for a simple, repeatable outfit, such as a black t-shirt. Initially, this seemed almost like a uniform. But diving deeper into minimalism, I understood the profound reasons behind this choice.\nüïµÔ∏è‚Äç‚ôÇÔ∏è Understanding Minimalism: Minimalism is more than a fashion statement; it\u0026rsquo;s a mindset that encourages us to find beauty and satisfaction in simplicity. It\u0026rsquo;s about stripping away the non-essential to make room for what truly adds value to our lives.\n1Ô∏è‚É£ Time-Saving Realizations: Choosing what to wear for work used to be a daily struggle. Adopting a minimalist uniform ‚Äì my trusty black t-shirt and grey pants ‚Äì turned these moments of indecision into a smooth, efficient routine, saving me valuable time each morning. ‚è∞\n2Ô∏è‚É£ Financial Wisdom: Quality Over Quantity: My journey into minimalism reshaped my approach to shopping. I shifted from quantity to quality, focusing on durable, timeless pieces. This not only saved me money but also aligned with my minimalist values of sustainability and conscious consumption. üí∞üå±\n3Ô∏è‚É£ Reduced Stress: A Calmer Morning: Simplifying my wardrobe choices drastically cut down my morning stress. This predictability in my routine has added a peaceful calm to the start of my day, allowing me to focus on more important tasks. üßò‚Äç‚ôÇÔ∏è\n4Ô∏è‚É£ Colleague Reactions: Insightful Surprises: I was initially curious about how my colleagues would react. To my surprise, while a few were inquisitive, most didn‚Äôt pay much attention. This reinforced a key minimalist lesson: often, our concerns about others\u0026rsquo; perceptions are unfounded. üë•\nüåç Embracing Sustainability In choosing a minimalist wardrobe, I\u0026rsquo;m also making a statement about sustainability. By reducing my participation in fast fashion, I‚Äôm contributing to a more sustainable future, one outfit at a time. üå±\nüëî My Uniform: A Symbol of Efficiency My minimalist uniform is more than just clothing. It represents efficiency, clarity, and a focus on what\u0026rsquo;s truly important. It\u0026rsquo;s a daily reminder that in simplicity, there\u0026rsquo;s a profound sense of freedom and purpose.\nüå± Wrapping Up: The Essence of Minimalism Minimalism isn\u0026rsquo;t about restricting yourself; it‚Äôs about freeing yourself to focus on what matters. It‚Äôs about finding joy in less and making room for more meaningful experiences and connections.\nIn my search for the perfect minimalist wardrobe, I discovered two incredible brands: Aristobrat and Pant Project.Their commitment to quality and sustainable practices resonated with my new lifestyle. I was particularly drawn to Aristobrat for their comfortable, durable black t-shirts and Pant Project for their perfectly fitting grey pants.\nAs I continue on this path, I invite you to reflect on how minimalism could reshape aspects of your life. It‚Äôs not just about clothes; it‚Äôs about embracing a life of purpose and intention.\nRemember, it\u0026rsquo;s not just about the clothes we wear but the life we choose to live. Keep growing, keep learning, and embrace your unique minimalist journey. ‚ú®\n","permalink":"https://bookofdaniel.in/posts/why-am-wearing-same-outfit-everyday/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;Minimalism is not about having less. It\u0026rsquo;s about making room for more of what matters.\u0026rdquo; - Unknown\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen I first explored minimalism, I was struck by a pattern: many minimalists, myself included, often opt for a simple, repeatable outfit, such as a black t-shirt. Initially, this seemed almost like a uniform. But diving deeper into minimalism, I understood the profound reasons behind this choice.\u003c/p\u003e\n\u003ch2 id=\"-understanding-minimalism\"\u003eüïµÔ∏è‚Äç‚ôÇÔ∏è Understanding Minimalism:\u003c/h2\u003e\n\u003cp\u003eMinimalism is more than a fashion statement; it\u0026rsquo;s a mindset that encourages us to find beauty and satisfaction in simplicity. It\u0026rsquo;s about stripping away the non-essential to make room for what truly adds value to our lives.\u003c/p\u003e","title":"Why I'm Wearing the Same Outfit to Work Every Day - My Minimalist Journey üåø"},{"content":"In the high-speed hustle of modern professional life, we often find ourselves chasing time, trying to grasp every fleeting minute. Productivity isn\u0026rsquo;t just a buzzword; it\u0026rsquo;s a lifeline that keeps us afloat in a sea of deadlines, meetings, and endless tasks. If you\u0026rsquo;re like me, a busy professional always on the lookout for the best way to streamline your day, you know that most productivity systems seem to add more to our plates rather than taking off the load.\nThat\u0026rsquo;s why I\u0026rsquo;ve refined a no-fuss, high-impact productivity system that cuts through the noise. It\u0026rsquo;s a straightforward approach that resonates with the daily grind of our lives. This system doesn\u0026rsquo;t require complex apps or the latest gadgets; it thrives on the bare essentials, making it perfect for the realist who needs to get things done now.\nStrategy 1: The Inverted Pyramid Method üïí Picture your day as an inverted pyramid. This is about prioritization. The most critical task sits at the wide top, where you pour a generous three hours of focused effort. As the pyramid tapers, so do the time allocations for less critical tasks, going down to two hours and then one.\nHow It Works: Identify the Most Important Task (MIT): Start with what would make the most significant impact on your day or your overall goal. Allocate Time: Dedicate a solid block of time to this MIT. No distractions, just deep work. Descend the Pyramid: Move on to the next priorities in decreasing time increments. The inverted pyramid is flexible. You can adjust the time allocations based on your workload and energy levels, but the principle remains: start with the most significant task and work your way down.\nStrategy 2: The Pareto Principle or the 80/20 Rule üìä The Pareto Principle is the idea that 80% of effects come from 20% of causes. It\u0026rsquo;s about leveraging the most impactful actions to yield the greatest results.\nApplying the Principle: Evaluate Your To-Do List: Look at your tasks and ask which ones will have the most considerable impact. Focus on High-Leverage Activities: Direct your efforts toward these high-impact activities. Minimize Low-Impact Work: Avoid spending too much time on tasks that don\u0026rsquo;t contribute significantly to your goals. Remember, the Pareto Principle isn\u0026rsquo;t about doing less work; it\u0026rsquo;s about doing more of the right work. Strategy 3: Intelligent Breaks üö∂‚Äç‚ôÇÔ∏è Work smart, not hard. That means taking breaks ‚Äî but not just any breaks. Smart breaks can enhance productivity, while poor ones can detract from it.\nWhat to Do on a Break: High-Intensity Exercise: Even a few minutes can significantly boost BDNF, which is great for brain health and focus. Mindfulness Practices: Meditation or deep breathing can reset your mental state. Nature Walks: A stroll outside can rejuvenate your mind and body. The idea is to step away from your work in a way that refreshes you, not one that further fragments your attention. Strategy 4: Focused Work Blocks üßò‚Äç‚ôÄÔ∏è When it\u0026rsquo;s time to work, really work. This is about undivided attention and eliminating multitasking.\nTips for Focused Work: Create a Distraction-Free Environment: Silence your phone, close unnecessary tabs, and clear your workspace. Use Tools to Enhance Focus: Noise-cancelling headphones or apps that block distracting websites can help. Single-Task: Focus entirely on one task at a time to maximize efficiency and quality of work. In summary, the essence of this productivity system lies in its simplicity and flexibility. It\u0026rsquo;s about making the most of your time and effort by focusing on what truly matters. Whether you\u0026rsquo;re aiming for academic excellence, professional growth, or personal development, these strategies can be tailored to fit your needs and help you achieve your goals.\nUntil next time, keep it productive, folks! üî•\n","permalink":"https://bookofdaniel.in/posts/the-ultimate-productive-system/","summary":"\u003cp\u003eIn the high-speed hustle of modern professional life, we often find ourselves chasing time, trying to grasp every fleeting minute. Productivity isn\u0026rsquo;t just a buzzword; it\u0026rsquo;s a lifeline that keeps us afloat in a sea of deadlines, meetings, and endless tasks. If you\u0026rsquo;re like me, a busy professional always on the lookout for the best way to streamline your day, you know that most productivity systems seem to add more to our plates rather than taking off the load.\u003c/p\u003e","title":"The Ultimate Productivity System: Get Things Done Effortlessly! üöÄ"},{"content":"Have you ever been a part of a hardworking team that, despite their efforts, ends up shouldering blame? If yes, our situation in operations might resonate with you. On any given day, our Zoho Desk is inundated with approximately 300 tickets, each falling under various classifications and priorities. Many of these tickets are complex and time-consuming to resolve, causing them to carry over to the next day and become overdue.\nThe challenge we face is the lack of visibility into critical metrics. Questions like the total number of tickets created, the various classifications in use, the resource who close the most tickets, and pending tickets are difficult to answer definitively. Convincing others about the work overload has proven to be quite a task.\nWhile Zoho Desk does offer predefined dashboards and customization options, our workflow relies heavily on custom fields. Additionally, the platform lacks real-time monitoring capabilities.\nIf you\u0026rsquo;ve followed along this far, you may find yourself in a similar scenario or be seeking solutions for building real-time monitoring functionality within Zoho Desk. Keep reading!\nChallenges As a team, we set out to tackle these challenges head-on, with the ultimate goal of maximizing throughput and efficiency to ensure customer satisfaction. Here are the key challenges we identified on our team board:\n1. Large Team Management: With a growing client base, our team size has expanded proportionally. Managing a larger team is becoming increasingly challenging without access to key metrics.\n2. Ticket Assignment Automation: Currently, our ticket assignment process involves agents or team members manually assigning tickets from a common email account. Some agents assign tickets to themselves, while others choose to assign them to the next team member. This variation in assignment might be due to differences in ticket complexity, with some opting for the low-hanging fruit.\n3. Timely Escalation: To maintain SLA compliance, we required a solution which will escalate the tickets to a manager when there is breach in a predefined time threshold (e.g., x minutes/hours).\n4. Email Tickets with Mandatory Fields: Zoho Desk offers a convenient feature that allows users to raise tickets via email. However, this feature creates tickets without mandatory fields, such as client or environment name. This calls for communicating the team over and over again resulting in time consumption.\n5. Real-Time Monitoring \u0026amp; Scoreboard: The absence of a real-time dashboard has caused inefficiencies in tracking productivity.\nImproving Daily work is even more important than doing daily work. - The Phoenix Project\nSolutions I have slept on these challenges for a day and explored the Zoho documentation along with some Google search. It seemed like I could solve some of these issues using zoho\u0026rsquo;s features, while others might require development.\nChallenge Solution Large Team Management Proposed to split the team into multiple pods or pools. So that it can be managed by a senior / lead with minimal members and work can be monitored efficiently. Pool segregation will be based on the number of clients. Ticket Assignment Automation Since the team is arranged into multiple pools, based on the client hierarchy, whenever a ticket is raised for the specific client it should be allocated directly to the respecitve pool. Then the pool lead / member can assign and work accordingly. To Automate this process Zoho Desk provides a feature called Direct Assignment. Create an assignment rule with client name and select respective pool account. Viola! The ticket will be assigned automatically on creation. Timely Escalation Zoho provides a feature called SLAs escalation. If an escalation occurs within x minutes it will be escalated to 2 levels. We chose to adopt it, so that the breached ticket will be esclated to the respective lead / manager of the pool within 30 mins. Email Ticket with mandatory fields We created a Zoho Task to close the email ticket with an automated reply to fill the mandatory fields. So the tickets in which the mandatory fields are not filled will be closed automatically. Real-Time Monitoring \u0026amp; Scoreboard Zoho analytics with additional license offers Dashboard experience but I needed something more interesting like a real-time dashboard to proactively act on tickets which does not require additional license.\nSo I came up with a solution which is powered by Grafana Dashboard and projected it on a monitor. For that, I used zoho web hook, instead of polling ticket meta details from Zoho desk.\nThe Webhook listener is developed using a simple java script. The json payload recieved will be deserialized and stored in a relational database like SQL Server.\nThen I created a Grafana Dashboard to pull the details from the database.\nArchitecture Diagram Final Results By incorporating real-time monitoring for Zoho tickets, my team\u0026rsquo;s productivity and efficiency increased tenfold. This enhancement not only bolstered accountability but also motivated everyone to aim higher and soar!\nA great team doesn‚Äôt mean that they had the smartest people. What made those teams great is that everyone trusted one another. It can be a powerful thing when that magic dynamic exists. - The Phoenix Project\n","permalink":"https://bookofdaniel.in/posts/integration-of-zoho-desk-and-grafana/","summary":"\u003cp\u003eHave you ever been a part of a hardworking team that, despite their efforts, ends up shouldering blame? If yes, our situation in operations might resonate with you. On any given day, our Zoho Desk is inundated with approximately 300 tickets, each falling under various classifications and priorities. Many of these tickets are complex and time-consuming to resolve, causing them to carry over to the next day and become overdue.\u003c/p\u003e","title":"Push your team to be Good to Great: Integration of Zoho Desk and Grafana"}]