[{"content":"Joshua had been waiting anxiously for weeks. As an aspiring clerk, he knew that the government official\u0026rsquo;s appointment order could arrive any day. This appointment was crucial—it would determine his future and career. Every morning, Joshua checked his local post office, hoping that today would be the day the letter arrived, confirming his new position.\nIn many ways, Joshua’s experience mirrors how Kafka, a powerful distributed streaming platform, manages and delivers data. Just like how Joshua relies on the postal system to receive his appointment letter, modern data systems depend on Kafka to ensure that critical information reaches its intended recipients quickly and reliably.\nIn this post, we’ll explore how Kafka functions as the backbone of modern data streaming, using Joshua’s story and the analogy of a postal service to make these concepts easy to understand.\nKafka Architecture Kafka’s architecture is designed for high throughput, low latency, and fault tolerance, much like a well-organized postal system that ensures messages are delivered efficiently and accurately. Here’s a breakdown of how Kafka\u0026rsquo;s architecture supports these goals:\nBrokers as Post Offices: Kafka brokers handle the heavy lifting of managing the data flow between producers and consumers. Each broker in a Kafka cluster is like a post office, managing and storing messages, and ensuring they are delivered to the correct topic (county) and partition (district).\nTopics as Counties and Partitions as Districts: Data in Kafka is categorized into topics, which are further divided into partitions. This division allows Kafka to scale horizontally, distributing the load across multiple brokers, much like how letters are sorted and delivered across different districts within a county.\nProducers and Consumers: Producers send messages to specific topics, deciding which partition to use based on a key or other logic. Consumers then retrieve these messages from the partitions they are assigned to, ensuring that each message is processed in the correct order.\nKRaft (Kafka Raft): Kafka has evolved from using Zookeeper to manage metadata and leader elections to using KRaft, an integrated consensus algorithm based on Raft. KRaft simplifies Kafka’s architecture, making it more efficient and scalable, akin to upgrading from a manual postal system to an automated one.\nKafka Connect and Kafka Streams As Kafka grew in popularity, additional tools were developed to extend its functionality, making it even more powerful for real-time data processing.\nKafka Connect: Kafka Connect is a framework for connecting Kafka with external systems such as databases, key-value stores, search indexes, and file systems. Think of Kafka Connect as an intermediary that helps Kafka integrate with other systems, much like a post office that not only delivers letters but also handles packages, money orders, and other services. It allows you to easily move large amounts of data in and out of Kafka without writing complex code.\nKafka Streams: Kafka Streams is a powerful library for building real-time applications that process data in Kafka. It allows you to perform complex transformations, aggregations, and joins on the data as it flows through Kafka. Imagine Kafka Streams as an automated sorting system within the post office that not only delivers the mail but also opens it, reads it, and takes action based on its contents. With Kafka Streams, you can build applications that react to data in real time, providing immediate insights and responses.\nReal-World Applications Kafka’s role in data streaming is akin to the postal service’s role in communication. Whether it’s real-time analytics, event sourcing, or data integration, Kafka ensures that data reaches its intended recipients efficiently and reliably.\nKafka’s architecture, combined with tools like Kafka Connect and Kafka Streams, makes it a powerful platform for handling a wide range of data processing tasks. From integrating different data systems to building real-time processing pipelines, Kafka is the backbone of many modern data infrastructures.\nConclusion Just as Joshua relies on a well-organized postal system to receive his appointment letter, modern data systems depend on Kafka to ensure that critical information is delivered accurately and efficiently. By understanding Kafka through this postal service analogy, we can appreciate how it organizes, processes, and delivers data across distributed systems.\nWhether you’re a developer, architect, or simply curious about how modern data systems work, Kafka’s ability to deliver data reliably and efficiently is worth exploring. Ready to dive deeper? Check out our other posts or Kafka’s official documentation to learn more.\nReady to dive deeper into Kafka? Start by setting up your own Kafka and Kafka Connect with our installation guide, and begin exploring the endless possibilities of real-time data processing.\n","permalink":"https://bookofdaniel.in/posts/kafka-explained-the-post-office-of-modern-data-streaming/","summary":"\u003cp\u003eJoshua had been waiting anxiously for weeks. As an aspiring clerk, he knew that the government official\u0026rsquo;s appointment order could arrive any day. This appointment was crucial—it would determine his future and career. Every morning, Joshua checked his local post office, hoping that today would be the day the letter arrived, confirming his new position.\u003c/p\u003e\n\u003cp\u003eIn many ways, Joshua’s experience mirrors how Kafka, a powerful distributed streaming platform, manages and delivers data. Just like how Joshua relies on the postal system to receive his appointment letter, modern data systems depend on Kafka to ensure that critical information reaches its intended recipients quickly and reliably.\u003c/p\u003e","title":"Kafka Explained: The Post Office of Modern Data Streaming"},{"content":"A few weeks ago, I got a task from my manager that seemed straightforward at first: set up a development environment for one of our upcoming projects. The catch? It needed to be accessible over HTTPS, just like our production sites. Now, getting a certificate from a trusted Certificate Authority (CA) for a development environment didn’t make much sense, and we certainly didn’t want to incur extra costs or deal with the complexities of a CA for something internal.\nSo, I figured I’d go the route of creating a self-signed certificate. This way, we could get our development site up and running with HTTPS quickly and securely, without any unnecessary overhead. After a bit of tinkering and some PowerShell magic, I had everything set up. And since it worked so well, I thought I’d share the process with you.\nHere’s how you can create a self-signed certificate for IIS and host a website on your development environment, using a simple PowerShell script.\nThe Script Let\u0026rsquo;s dive into the script that makes all this possible:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 param( [Parameter(Mandatory=$true)] [string]$DomainName, [Parameter(Mandatory=$true)] [string]$WebsiteName ) # Import the WebAdministration module to work with IIS Import-Module WebAdministration # Generate a new self-signed certificate $cert = New-SelfSignedCertificate -DnsName $DomainName -CertStoreLocation \u0026#34;cert:\\LocalMachine\\My\u0026#34; # Export the certificate\u0026#39;s thumbprint $certThumbprint = $cert.Thumbprint # Get the site object in IIS $site = Get-Item \u0026#34;IIS:\\Sites\\$WebsiteName\u0026#34; # Remove existing HTTPS bindings (if any) $bindings = $site.Bindings.Collection | Where-Object { $_.protocol -eq \u0026#34;https\u0026#34; } foreach ($binding in $bindings) { Remove-WebBinding -Name $WebsiteName -BindingInformation $binding.bindingInformation -Protocol https } # Create a new HTTPS binding New-WebBinding -Name $WebsiteName -IPAddress \u0026#34;*\u0026#34; -Port 443 -HostHeader $DomainName -Protocol https # Assign the certificate to the new binding $binding = Get-WebBinding -Name $WebsiteName -Protocol https -Port 443 $binding.AddSslCertificate($certThumbprint, \u0026#34;My\u0026#34;) Write-Host \u0026#34;Self-signed certificate created and bound to site \u0026#39;$WebsiteName\u0026#39; with domain \u0026#39;$DomainName\u0026#39;.\u0026#34; Step-by-Step Breakdown Parameter Setup: The script starts by asking for two mandatory parameters: the domain name (DomainName) and the website name (WebsiteName). These parameters will guide the rest of the script.\nLoad the WebAdministration Module: This module is necessary to interact with IIS through PowerShell. It’s what allows us to create and manage sites, bindings, and other IIS configurations.\nCreate a Self-Signed Certificate: Using the New-SelfSignedCertificate cmdlet, the script generates a self-signed certificate for the specified domain. The certificate is stored in the local machine\u0026rsquo;s \u0026ldquo;My\u0026rdquo; certificate store.\nRetrieve the Certificate Thumbprint: The thumbprint is a unique identifier for the certificate, which is necessary to link it to the site’s binding.\nGet the IIS Site Object: The script uses Get-Item to grab the IIS site object for the specified website. This allows us to work directly with the site\u0026rsquo;s properties, including its bindings.\nRemove Existing HTTPS Bindings: Before adding a new HTTPS binding, any existing ones are removed to ensure there are no conflicts or duplicate bindings.\nCreate a New HTTPS Binding: The script sets up a new HTTPS binding on port 443, using the provided domain name.\nBind the Certificate: Finally, the script attaches the self-signed certificate to the HTTPS binding using its thumbprint, ensuring that the site is now accessible via HTTPS.\nConfirmation: After everything is done, the script outputs a confirmation message to let you know the certificate was successfully created and bound to the website.\nAvoiding Browser Warnings Due to the Self-Signed Certificate When using a self-signed certificate, most browsers will display a warning because the certificate isn’t issued by a trusted Certificate Authority (CA). To avoid these warnings, you can import the certificate into your browser\u0026rsquo;s trusted root certificate store. Here’s how you can do that in major browsers:\nHow to Import a Self-Signed Certificate into Your Browser 1. Google Chrome Windows/macOS: Open Chrome and go to chrome://settings. Scroll down and click on \u0026ldquo;Advanced\u0026rdquo; to expand more settings. Under \u0026ldquo;Privacy and security,\u0026rdquo; click on \u0026ldquo;Manage certificates\u0026rdquo; (or \u0026ldquo;Security\u0026rdquo; \u0026gt; \u0026ldquo;Manage certificates\u0026rdquo; on macOS). In the \u0026ldquo;Certificates\u0026rdquo; window, click on the \u0026ldquo;Trusted Root Certification Authorities\u0026rdquo; tab. Click \u0026ldquo;Import\u0026rdquo; and follow the wizard to import the self-signed certificate. Once imported, restart Chrome, and the browser should no longer show a warning for sites using this certificate. 2. Mozilla Firefox Windows/macOS/Linux: Open Firefox and go to about:preferences. Scroll down to \u0026ldquo;Privacy \u0026amp; Security.\u0026rdquo; Under the \u0026ldquo;Certificates\u0026rdquo; section, click on \u0026ldquo;View Certificates.\u0026rdquo; In the \u0026ldquo;Certificate Manager\u0026rdquo; window, go to the \u0026ldquo;Authorities\u0026rdquo; tab. Click on \u0026ldquo;Import\u0026rdquo; and select the self-signed certificate file. Choose \u0026ldquo;Trust this CA to identify websites\u0026rdquo; and click \u0026ldquo;OK.\u0026rdquo; The certificate is now trusted by Firefox. 3. Microsoft Edge (Chromium-based) Windows/macOS: Open Edge and go to edge://settings. Scroll down and click on \u0026ldquo;Advanced settings.\u0026rdquo; Click on \u0026ldquo;Manage certificates\u0026rdquo; under \u0026ldquo;Privacy and services.\u0026rdquo; In the \u0026ldquo;Certificates\u0026rdquo; window, go to the \u0026ldquo;Trusted Root Certification Authorities\u0026rdquo; tab. Click \u0026ldquo;Import\u0026rdquo; and follow the wizard to import the certificate. Restart Edge, and the warning should disappear for sites using the self-signed certificate. 4. Safari (macOS) Double-click the self-signed certificate file (.cer or .crt) to open it in Keychain Access. In the Keychain Access window, locate the certificate, which will be listed under the \u0026ldquo;Certificates\u0026rdquo; category. Double-click on the certificate, and a new window will open. Expand the \u0026ldquo;Trust\u0026rdquo; section. Change \u0026ldquo;When using this certificate\u0026rdquo; to \u0026ldquo;Always Trust.\u0026rdquo; Close the window and enter your macOS password to confirm the change. Safari will now trust the certificate, and you won’t see any warnings when visiting the site. 5. Internet Explorer Windows: Open Internet Explorer and go to Internet Options. Go to the \u0026ldquo;Content\u0026rdquo; tab and click on \u0026ldquo;Certificates.\u0026rdquo; Go to the \u0026ldquo;Trusted Root Certification Authorities\u0026rdquo; tab and click \u0026ldquo;Import.\u0026rdquo; Follow the wizard to import the self-signed certificate. Once imported, restart Internet Explorer. Wrapping Up Setting up HTTPS for a development environment doesn’t have to be complicated or expensive. By creating a self-signed certificate, you can quickly secure your site and make sure it mirrors your production environment as closely as possible. Whether you’re preparing for a presentation or just want to ensure everything is configured correctly before going live, this PowerShell script has you covered.\nSo, next time your manager asks you to spin up a development environment with HTTPS, you’ll know exactly what to do! 😊\n","permalink":"https://bookofdaniel.in/posts/how-to-create-self-signed-certificate-for-iis/","summary":"\u003cp\u003eA few weeks ago, I got a task from my manager that seemed straightforward at first: set up a development environment for one of our upcoming projects. The catch? It needed to be accessible over HTTPS, just like our production sites. Now, getting a certificate from a trusted Certificate Authority (CA) for a development environment didn’t make much sense, and we certainly didn’t want to incur extra costs or deal with the complexities of a CA for something internal.\u003c/p\u003e","title":"How to Create a Self-Signed Certificate for IIS and Host a Website"},{"content":"I remember the first time I encountered YAML—it was during a seemingly regular workday when a Kubernetes task landed on my desk. I had been managing infrastructure the traditional way for years, but suddenly, here was this new ecosystem that used a format I hadn\u0026rsquo;t seen before: YAML. It was frustrating at first. The simple indentation of a line could break everything, and I was far too comfortable with JSON and XML. But as Kubernetes became indispensable, and Ansible, Docker Compose, and other tools followed suit, it became clear that YAML wasn\u0026rsquo;t just a passing trend—it was the new standard.\nIn the world of cloud computing, DevOps, and configuration management, YAML (which stands for “YAML Ain’t Markup Language”) is now everywhere. You’re likely reading this because you, too, have found yourself forced into learning YAML to keep up with the modern tools shaping the industry. The good news is that once you get the hang of it, YAML is not only easy to use, but it can also become a powerful ally in simplifying complex configurations.\nIn this article, I’ll take you through the journey of understanding YAML, explain why it became so essential, and share some tips and tricks to ace it. And, as a bonus for my fellow Vim users, I’ll show you how to configure Vim to make working with YAML a breeze.\n\u0026ldquo;YAML is deceptively simple—until you miss a single space.\u0026rdquo; - Unknown\nWhy YAML? 🤔 When I first dived into YAML, I kept asking myself, \u0026ldquo;Why has this simple format taken over the tech world?\u0026rdquo; The more I used it, the clearer the answer became: simplicity. YAML is designed to be human-readable, intuitive, and less verbose than alternatives like JSON and XML. It’s built on the premise that configuration files should be easy to understand not just for machines, but for humans as well.\nBut let’s break down some specific reasons why YAML has become the go-to format in the world of DevOps, cloud computing, and beyond:\nHuman-Readable: YAML’s primary goal is to be readable by humans. When you look at a YAML file, the hierarchy and structure are immediately apparent. There\u0026rsquo;s no need for brackets, commas, or extra symbols to define relationships, making it a cleaner, more straightforward format compared to JSON and XML.\nFlexible Data Representation: YAML can easily represent complex data structures like lists, dictionaries, and scalars. Whether you\u0026rsquo;re defining a simple list of tasks in Ansible or configuring a Kubernetes pod with multiple containers, YAML makes it easy to organize information logically and cleanly.\nConcise Yet Powerful: YAML allows you to do a lot with very little. The lack of extra punctuation means you can define configurations in fewer lines compared to JSON or XML. But it doesn\u0026rsquo;t sacrifice power—you can still handle everything from basic variables to advanced configurations using YAML’s compact structure.\nWidely Supported: From Kubernetes to Ansible, Docker Compose to CI/CD pipelines, YAML has become the standard configuration format across a wide variety of tools and platforms. Its wide adoption ensures that, once you learn it, you can apply your skills across multiple technologies.\nUsed in Modern Tech: Many of the technologies driving innovation today—like Kubernetes, Ansible, and cloud platforms—rely on YAML for their configuration files. You can’t avoid YAML if you\u0026rsquo;re working with infrastructure as code, and learning it has become almost synonymous with modern DevOps practices.\nBasics of YAML 📝 When I finally sat down to learn YAML, I was surprised by how straightforward it was once I got past the initial frustration. YAML’s charm lies in its simplicity. It’s clean and easy to understand, but as with anything in tech, the devil is in the details. Let’s explore the basics so you can get comfortable with the fundamentals of YAML.\n1. YAML Syntax and Structure The structure of YAML is all about indentation. It uses whitespace to represent the hierarchy and relationships between different pieces of data. Unlike JSON, where you need braces ({}) and commas, YAML relies on simple indentation to create its structure.\nKey-Value Pairs: The most basic YAML syntax is a key-value pair. This is often how configurations are set.\nExample:\n1 2 3 name: John Doe age: 30 occupation: Cloud Architect Lists: YAML allows you to easily create lists (or arrays). Lists are represented by dashes (-).\nExample:\n1 2 3 4 fruits: - apple - banana - orange Nested Data: YAML supports nested structures with simple indentation. You can define complex configurations by indenting elements under their parent.\nExample:\n1 2 3 4 5 6 7 8 9 user: name: John Doe details: age: 30 occupation: Cloud Architect skills: - Terraform - Kubernetes - Ansible 2. Indentation Rules In YAML, consistent indentation is key. YAML uses spaces for indentation (not tabs), and inconsistent spacing will result in syntax errors. It’s common to use two spaces per indentation level, but this can vary depending on preference, as long as you\u0026rsquo;re consistent throughout the file.\nExample of proper indentation:\n1 2 3 4 5 6 7 8 9 services: frontend: image: nginx ports: - \u0026#34;8080:80\u0026#34; backend: image: node environment: - NODE_ENV=production 3. Comments Adding comments to YAML files is straightforward and useful, especially when dealing with large configurations. Comments begin with the # symbol.\nExample:\n1 2 3 4 # This is a list of users users: - name: Alice - name: Bob 4. Basic Types in YAML YAML supports a variety of data types, such as strings, integers, floats, booleans, and nulls:\nStrings: Can be quoted or unquoted. Double or single quotes can be used for strings.\nExample:\n1 2 city: \u0026#34;San Francisco\u0026#34; language: \u0026#39;English\u0026#39; Numbers: Both integers and floats are supported.\nExample:\n1 2 age: 30 price: 19.99 Booleans: Represented by true/false.\nExample:\n1 active: true Null Values: Represented by the word null or a tilde ~.\nExample:\n1 middle_name: null How to Ace YAML: Best Practices 🎖️ As I spent more time working with YAML, I started noticing a few patterns that could make or break a YAML file. Getting YAML right isn’t just about knowing the syntax; it’s about developing habits that help you avoid common pitfalls and create clean, maintainable configurations. Here are some best practices that will help you ace YAML and ensure your files are both efficient and error-free.\n1. Consistency with Indentation YAML’s sensitivity to indentation can be a double-edged sword. While it makes files more readable, inconsistent use of spaces can lead to frustrating errors. The key to mastering YAML is being religious about indentation:\nAlways Use Spaces, Never Tabs: YAML files don’t tolerate tabs. If you accidentally use them, it will cause parsing errors. Make sure your editor is set to replace tabs with spaces. Stick to One Indentation Level: Whether you prefer two spaces or four, stick to that indentation level throughout the entire file. Consistency is more important than the size of the indentation. Example of proper indentation:\n1 2 3 4 5 6 7 environments: production: db_host: prod.db.example.com db_user: admin staging: db_host: stage.db.example.com db_user: dev 2. Avoid Over-Complicating the Structure One of the biggest advantages of YAML is its simplicity, so keep it simple! Avoid deeply nested structures unless absolutely necessary. If you notice your file becoming overly complex, consider splitting it into multiple files or refactoring the data structure.\nExample of a simple structure:\n1 2 3 4 5 database: host: localhost port: 5432 username: admin password: password123 3. Use Comments Generously YAML files are often used to configure critical infrastructure or services. Adding comments that explain your decisions can be a lifesaver for future you (or anyone else working on the file). Don’t assume that the structure or values will always be self-explanatory.\nExample of well-commented YAML:\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Define the services for our Docker setup services: # Frontend service with Nginx frontend: image: nginx ports: - \u0026#34;8080:80\u0026#34; # Backend service running Node.js backend: image: node environment: - NODE_ENV=production 4. Use Tools to Lint and Validate YAML One of the easiest ways to avoid mistakes in your YAML files is to use a linting tool. These tools automatically check for syntax errors and inconsistencies, ensuring that your files are properly formatted before you deploy them.\nYamllint: A popular tool that checks your YAML syntax and helps catch issues before they cause problems. It’s great for catching indentation issues, unused variables, and more.\nExample of using yamllint:\n1 yamllint myfile.yaml Many text editors and IDEs (like VSCode) have built-in YAML validation, which can also help prevent mistakes before they’re committed.\n5. Use Anchors and Aliases for Repeated Data YAML provides a feature called \u0026ldquo;anchors\u0026rdquo; and \u0026ldquo;aliases\u0026rdquo; to avoid repetition. Anchors allow you to define a piece of data once and reuse it throughout your YAML file. This not only reduces duplication but also makes it easier to manage changes.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 defaults: \u0026amp;defaults log_level: INFO timeout: 30 development: \u0026lt;\u0026lt;: *defaults db_host: dev.db.example.com production: \u0026lt;\u0026lt;: *defaults db_host: prod.db.example.com In this example, the defaults anchor is reused in both the development and production environments, making the configuration cleaner and easier to maintain.\n6. Test, Test, Test Before deploying a YAML configuration file, always test it in a staging or development environment first. YAML errors can be tricky to debug, so catching them early is crucial. Ensure that everything parses correctly and works as expected before pushing it into production.\nCommon Pitfalls and How to Avoid Them 😐 Even though YAML is designed to be simple, there are a few quirks that can trip you up if you’re not careful. Let’s look at some common pitfalls and how to avoid them.\n1. Mixing Spaces and Tabs This is the most common error when working with YAML. YAML files are indentation-sensitive, and while it may look like everything is aligned correctly, using tabs instead of spaces can lead to parsing errors.\nSolution: Always configure your text editor to convert tabs to spaces. In most editors, this can be done through settings, and using a linter like yamllint will also help catch these mistakes.\nExample of improper use of tabs:\n1 2 3 4 5 services: frontend: # Tabs instead of spaces here will cause an error image: nginx ports: - \u0026#34;8080:80\u0026#34; Proper version:\n1 2 3 4 5 services: frontend: image: nginx ports: - \u0026#34;8080:80\u0026#34; 2. Inconsistent Indentation Inconsistent indentation—using two spaces in one section and four in another—can break your YAML file. Since YAML relies on indentation to define structure, any inconsistency can lead to unpredictable results.\nSolution: Stick to one indentation level (two spaces or four spaces) throughout the file. Use your editor’s settings to make sure indentation is consistent.\nExample of inconsistent indentation:\n1 2 3 4 database: host: localhost port: 5432 # This extra indentation will break the structure username: admin 3. Confusion with Quotes In YAML, quotes can be tricky. You can use both single quotes (') and double quotes (\u0026quot;), but they behave slightly differently. Double quotes allow for escape sequences (e.g., \\n for a new line), while single quotes are more literal. Sometimes, people mix these up, especially when dealing with special characters.\nSolution: Use quotes consistently, and only when necessary. If you\u0026rsquo;re dealing with special characters, double quotes are usually safer.\nExample:\n1 2 message: \u0026#34;Hello, World!\u0026#34; # Double quotes allow escape characters path: \u0026#39;/usr/local/bin\u0026#39; # Single quotes for literal paths 4. Large and Complex Files As your configurations grow, YAML files can become unwieldy. Large files with deeply nested structures can be difficult to manage and understand, and it’s easy to lose track of relationships between data.\nSolution: Break large YAML files into smaller, modular files when possible. For example, in Kubernetes, it’s common to have separate files for different resources (pods, services, deployments). You can also use YAML’s include functionality, which allows you to pull in external files.\nReal-World Examples 🧐 Let’s take a look at a few real-world examples to see how YAML is used in popular tools.\n1. Kubernetes Pod Definition Here’s a simple Kubernetes pod definition using YAML. It defines a pod with two containers (one running Nginx and the other running Redis):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata: name: example-pod labels: app: my-app spec: containers: - name: nginx-container image: nginx ports: - containerPort: 80 - name: redis-container image: redis ports: - containerPort: 6379 In this example, the pod’s metadata, specification, and containers are all clearly defined using YAML’s clean structure.\n2. Ansible Playbook Here’s a basic Ansible playbook for installing Nginx on a server:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- - name: Install Nginx hosts: webservers become: yes tasks: - name: Ensure Nginx is installed apt: name: nginx state: present - name: Start Nginx service service: name: nginx state: started Ansible’s playbooks rely heavily on YAML, and here you can see how the simple structure makes it easy to understand the flow of the configuration.\nMastering YAML with Vim: Beat the Pitfalls with the Right Setup ✅ As you begin to work more with YAML, especially in Vim, the last thing you want is to be tripped up by simple formatting errors like inconsistent indentation or accidental use of tabs. Thankfully, Vim offers a wide range of configuration options and plugins to make YAML editing easier and error-free. Here’s how you can set up Vim to master YAML and avoid its common pitfalls.\n1. Enable Syntax Highlighting Vim’s built-in syntax highlighting makes it easier to spot errors like incorrect indentation or misused quotes. Simply turn on syntax highlighting to get started:\n1 :syntax on This will provide basic YAML syntax highlighting out of the box, making the structure of your files more visible.\n2. Set Auto-Indentation To help maintain consistent indentation throughout your YAML files, enable automatic indentation:\n1 :set ai This will automatically indent new lines based on the previous lines, making it easier to follow the proper structure.\n3. Use Spaces, Not Tabs YAML files are strict about indentation, and they only accept spaces—not tabs. Set Vim to automatically convert tabs into spaces by adding this to your .vimrc file:\n1 :set expandtab This command ensures that whenever you hit the tab key, Vim inserts spaces instead. To match common YAML standards, you can also define how many spaces Vim should use for each level of indentation:\n1 :set tabstop=2 shiftwidth=2 This will use two spaces per indentation level, which is a widely accepted standard for YAML files.\n4. Show Invisible Characters Sometimes, identifying spaces versus tabs or extra whitespace can be tricky because they are invisible. You can make these characters visible in Vim by enabling listchars:\n1 :set list listchars=tab:\\▸\\ ,trail:. This will show tabs as arrows and highlight any trailing spaces, making it easier to spot formatting errors.\n5. Install YAML-Specific Plugins For an even better YAML editing experience, consider installing a plugin that’s specifically designed for YAML. One popular option is vim-yaml. If you’re using a Vim plugin manager like vim-plug, you can install it by adding this to your .vimrc:\n1 Plug \u0026#39;stephpy/vim-yaml\u0026#39; Once installed, this plugin improves YAML support in Vim by offering better syntax highlighting, indentation, and folding.\n6. Autoformatting YAML Sometimes, after a lot of edits, your YAML file can get messy with inconsistent indentation. Vim has auto-formatting capabilities that can clean up your file. You can re-indent your YAML file by running the following command within Vim:\n1 gg=G This command re-indents the entire file, ensuring that all lines follow the same indentation pattern.\n7. Lint Your YAML While Vim doesn’t have built-in linting for YAML, you can combine it with external linting tools like yamllint to validate your YAML files. Simply run the following command to check your file for errors:\n1 !yamllint % This command runs yamllint on your current file, providing feedback on any formatting issues that could cause problems.\nWith these Vim settings in place, you’ll be well-equipped to handle YAML files without fear of indentation errors, tabs, or whitespace-related issues. Vim is an incredibly powerful editor, and with the right configuration, it can help you ace YAML every time.\nMy vim settings file ~/.vimrc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 set expandtab set tabstop=2 set shiftwidth=2 set autoindent set number set paste syntax on set cursorline set incsearch set ignorecase set smartcase set hlsearch set smartindent set nowrap set background=dark Conclusion YAML may have started as something I was forced to learn, but over time, I realized why it has become a cornerstone in so many modern tools. Its simplicity, readability, and flexibility make it the perfect format for configuration files in an increasingly complex world of cloud computing and infrastructure management. While it has its quirks—like strict indentation rules—mastering YAML can significantly improve your efficiency when working with technologies like Kubernetes, Ansible, and Docker.\nBy understanding the basics, following best practices, and using tools like Vim to avoid common pitfalls, you can transform YAML from a frustrating obstacle into a valuable tool. Whether you’re defining configurations for a Kubernetes cluster or writing an Ansible playbook, learning YAML is no longer optional—it’s a skill that every developer and systems administrator must have in their toolkit.\nAnd who knows? One day, like me, you might look back and realize that YAML, the language you once reluctantly learned, has become second nature—helping you master the very systems that once felt overwhelming.\nNow, go ace YAML with confidence! 👍🏻\n","permalink":"https://bookofdaniel.in/posts/why-yaml-and-how-to-ace-it/","summary":"\u003cp\u003eI remember the first time I encountered YAML—it was during a seemingly regular workday when a Kubernetes task landed on my desk. I had been managing infrastructure the traditional way for years, but suddenly, here was this new ecosystem that used a format I hadn\u0026rsquo;t seen before: YAML. It was frustrating at first. The simple indentation of a line could break everything, and I was far too comfortable with JSON and XML. But as Kubernetes became indispensable, and Ansible, Docker Compose, and other tools followed suit, it became clear that YAML wasn\u0026rsquo;t just a passing trend—it was the new standard.\u003c/p\u003e","title":"Why YAML and How to Ace It?"},{"content":"In today\u0026rsquo;s interconnected world, securing online communication is paramount. Public Key Infrastructure (PKI) acts as the backbone of this security, ensuring that sensitive information transferred over networks remains encrypted and trustworthy, facilitating secure communication between servers and users. A crucial component of this infrastructure is the TLS certificate, widely used to secure websites. Let\u0026rsquo;s delve into how PKI and TLS certificates work to provide encryption and trust in digital transactions.\nThe Role of PKI in Securing Communication PKI is a system that establishes trust between two parties, typically a user and a server, by guaranteeing that communication is encrypted and transferred securely. It employs two key encryption methods: symmetric and asymmetric encryption.\nSymmetric Encryption Symmetric encryption uses a single shared key to both encrypt and decrypt the data. While efficient, it presents a vulnerability: if a hacker intercepts the key, they can easily decrypt the message. The challenge lies in securely sharing the symmetric key without exposing it to attackers.\nAsymmetric Encryption To address the limitations of symmetric encryption, PKI utilizes asymmetric encryption. This involves two mathematically linked keys: a public key, which is openly available, and a private key, known only to the owner (server or user). The public key encrypts messages, but only the corresponding private key can decrypt them.\nImagine a secure mailbox with two locks. You can give the combination to the first lock (public key) to anyone, allowing them to put messages inside. However, only you possess the key to the second lock (private key), which is needed to retrieve messages.\nAsymmetric encryption forms the basis for secure communication protocols like Secure Shell (SSH). In SSH, the public key resides on the server, and users log in using their private key. This ensures that only users with the matching private key can access the server.\nHow TLS Certificates Work in PKI When a user visits a secure website (indicated by \u0026ldquo;https\u0026rdquo; in the URL), the web server and the user\u0026rsquo;s browser establish a secure communication channel using TLS (Transport Layer Security) certificates. Here\u0026rsquo;s a simplified breakdown of the process:\nInitial Connection: When a user accesses a website, the server provides its public key along with a digital certificate, which includes the server\u0026rsquo;s identity and its public key.\nEncrypting the Symmetric Key: The browser then generates a temporary symmetric key for encrypting further communication. However, instead of sending this symmetric key in plain text, the browser encrypts it using the server\u0026rsquo;s public key.\nSecure Key Exchange: The encrypted symmetric key is sent to the server. Since the server holds the corresponding private key, it can decrypt the symmetric key and establish a secure connection. A hacker, even with access to the public key, cannot decrypt the symmetric key due to the mathematical relationship between the keys.\nOngoing Secure Communication: Now that both the server and browser have the symmetric key, they can use it to encrypt and decrypt data exchanged between them, ensuring secure and fast communication.\nProtection from Fake Websites Despite this system, it\u0026rsquo;s not foolproof. Malicious actors can create phishing websites designed to mimic legitimate sites and trick users into revealing their credentials. They could even potentially generate fake certificates. So, how do users know the certificate is genuine and not from a hacker?\nDigital Certificates and Certificate Authorities (CAs) A digital certificate serves as proof that a website\u0026rsquo;s public key is legitimate and issued by a trusted authority. When a user visits a website, the browser checks whether the website’s certificate matches the domain name and whether it has been issued by a legitimate Certificate Authority (CA).\nCertificate Signing Authority To create a trusted certificate, organizations generate a Certificate Signing Request (CSR). The CSR contains information about the organization and its domain name, which is then sent to a CA for verification.\n1 2 openssl req -new -key my-bank.key -out my-bank.csr -subj \u0026#34;/C=US/ST=CA/O=MyOrg, Inc./CN=my-bank.com\u0026#34; # Output will be: my-bank.key my-bank.csr The CA validates the information and, if approved, signs the certificate with its private key. This signed certificate is sent back to the organization, which can then use it to secure its website. If a hacker tries to obtain a certificate, the CA’s validation process ensures that fake certificates are rejected.\nBrowser Trust in CAs Browsers are pre-configured to trust certificates from well-known CAs, such as Symantec, DigiCert, Comodo,GlobalSign, etc. Each CA has its own public and private key pair. The browser uses the CA\u0026rsquo;s public key (built into the browser) to verify the authenticity of the website’s certificate. If the certificate is signed by a trusted CA, the browser allows access. If not, the browser displays a warning, alerting the user that the site may not be secure.\nYou can view the trusted CAs in your browser\u0026rsquo;s settings under \u0026ldquo;Trusted Root Certification Authorities.\u0026rdquo;\nPrivate CAs for Internal Networks While public CAs are essential for securing public websites, they may not be practical for internal networks, such as those used for payroll or intranet applications. For this, organizations can deploy their own private CAs to sign certificates for internal websites. Employees’ browsers can be configured to trust these private CAs, establishing secure connections within the organization.\nConclusion PKI and TLS certificates are critical components of modern internet security. By leveraging asymmetric encryption, digital certificates, and trusted Certificate Authorities, PKI ensures that communication between users and servers is encrypted and that the server\u0026rsquo;s identity is verified. Whether you are securing a public website or an internal network, PKI and TLS certificates provide the foundation for establishing trust and safeguarding sensitive information online.\nAlways remember: while public keys are widely distributed, private keys should remain confidential, ensuring that only the intended recipient can decrypt sensitive data.\nAdditional Notes:\nCertificate Revocation List (CRL) and Online Certificate Status Protocol (OCSP): To inform browsers about revoked certificates, CAs use CRLs or OCSP. CRLs are lists of revoked certificates, while OCSP provides real-time status checks. Private Key Security: It\u0026rsquo;s crucial to store private keys securely and protect them from unauthorized access. Certificate Expiration: Certificates have a limited lifespan. Organizations must regularly renew their certificates to maintain security. Best Practices: Follow industry best practices for PKI implementation and management, including using strong cryptographic algorithms, regularly updating certificates, and implementing robust key management practices. By understanding the fundamentals of PKI and TLS, you can ensure that your online interactions are secure and protected from unauthorized access.\n","permalink":"https://bookofdaniel.in/posts/understanding-public-key-infrastructure-and-tls-certificates/","summary":"\u003cp\u003eIn today\u0026rsquo;s interconnected world, securing online communication is paramount. Public Key Infrastructure (PKI) acts as the backbone of this security, ensuring that sensitive information transferred over networks remains encrypted and trustworthy, facilitating secure communication between servers and users. A crucial component of this infrastructure is the TLS certificate, widely used to secure websites. Let\u0026rsquo;s delve into how PKI and TLS certificates work to provide encryption and trust in digital transactions.\u003c/p\u003e","title":"Understanding Public Key Infrastructure and TLS Certificates"},{"content":" \u0026ldquo;Minimalism is not about having less. It\u0026rsquo;s about making room for more of what matters.\u0026rdquo; - Unknown\nWhen I first explored minimalism, I was struck by a pattern: many minimalists, myself included, often opt for a simple, repeatable outfit, such as a black t-shirt. Initially, this seemed almost like a uniform. But diving deeper into minimalism, I understood the profound reasons behind this choice.\n🕵️‍♂️ Understanding Minimalism: Minimalism is more than a fashion statement; it\u0026rsquo;s a mindset that encourages us to find beauty and satisfaction in simplicity. It\u0026rsquo;s about stripping away the non-essential to make room for what truly adds value to our lives.\n1️⃣ Time-Saving Realizations: Choosing what to wear for work used to be a daily struggle. Adopting a minimalist uniform – my trusty black t-shirt and grey pants – turned these moments of indecision into a smooth, efficient routine, saving me valuable time each morning. ⏰\n2️⃣ Financial Wisdom: Quality Over Quantity: My journey into minimalism reshaped my approach to shopping. I shifted from quantity to quality, focusing on durable, timeless pieces. This not only saved me money but also aligned with my minimalist values of sustainability and conscious consumption. 💰🌱\n3️⃣ Reduced Stress: A Calmer Morning: Simplifying my wardrobe choices drastically cut down my morning stress. This predictability in my routine has added a peaceful calm to the start of my day, allowing me to focus on more important tasks. 🧘‍♂️\n4️⃣ Colleague Reactions: Insightful Surprises: I was initially curious about how my colleagues would react. To my surprise, while a few were inquisitive, most didn’t pay much attention. This reinforced a key minimalist lesson: often, our concerns about others\u0026rsquo; perceptions are unfounded. 👥\n🌍 Embracing Sustainability In choosing a minimalist wardrobe, I\u0026rsquo;m also making a statement about sustainability. By reducing my participation in fast fashion, I’m contributing to a more sustainable future, one outfit at a time. 🌱\n👔 My Uniform: A Symbol of Efficiency My minimalist uniform is more than just clothing. It represents efficiency, clarity, and a focus on what\u0026rsquo;s truly important. It\u0026rsquo;s a daily reminder that in simplicity, there\u0026rsquo;s a profound sense of freedom and purpose.\n🌱 Wrapping Up: The Essence of Minimalism Minimalism isn\u0026rsquo;t about restricting yourself; it’s about freeing yourself to focus on what matters. It’s about finding joy in less and making room for more meaningful experiences and connections.\nIn my search for the perfect minimalist wardrobe, I discovered two incredible brands: Aristobrat and Pant Project.Their commitment to quality and sustainable practices resonated with my new lifestyle. I was particularly drawn to Aristobrat for their comfortable, durable black t-shirts and Pant Project for their perfectly fitting grey pants.\nAs I continue on this path, I invite you to reflect on how minimalism could reshape aspects of your life. It’s not just about clothes; it’s about embracing a life of purpose and intention.\nRemember, it\u0026rsquo;s not just about the clothes we wear but the life we choose to live. Keep growing, keep learning, and embrace your unique minimalist journey. ✨\n","permalink":"https://bookofdaniel.in/posts/why-am-wearing-same-outfit-everyday/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;Minimalism is not about having less. It\u0026rsquo;s about making room for more of what matters.\u0026rdquo; - Unknown\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen I first explored minimalism, I was struck by a pattern: many minimalists, myself included, often opt for a simple, repeatable outfit, such as a black t-shirt. Initially, this seemed almost like a uniform. But diving deeper into minimalism, I understood the profound reasons behind this choice.\u003c/p\u003e\n\u003ch2 id=\"-understanding-minimalism\"\u003e🕵️‍♂️ Understanding Minimalism:\u003c/h2\u003e\n\u003cp\u003eMinimalism is more than a fashion statement; it\u0026rsquo;s a mindset that encourages us to find beauty and satisfaction in simplicity. It\u0026rsquo;s about stripping away the non-essential to make room for what truly adds value to our lives.\u003c/p\u003e","title":"Why Am Wearing Same Outfit Everyday"},{"content":"We all aspire to be better, stronger, smarter, and healthier, yet many struggle to stick with their goals. In this post, we\u0026rsquo;ll explore a practical framework that addresses this common issue, allowing you to effortlessly surpass your goals. This system has been successfully used to develop the discipline needed to hold a 15-minute plank every day. Join me on this journey, and let\u0026rsquo;s unlock your raw power together.\nStep 1: Action-Oriented Goals Instead of setting vague and impractical goals, focus on specific actions within a defined time frame. For instance, rather than aiming to lose 20 pounds, set a goal to complete 10,000 push-ups by the end of the year. Break down the goal into actionable steps, visualize each step, and establish a non-negotiable daily minimum to make progress.\nExamples:\nGoal: Learn computer programming Action: Code 25 functional programs Goal: Get into a relationship Action: Have 100 interesting conversations with strangers Goal: Become a famous YouTuber Action: Make 100 videos packed with value Step 2: Minimize Friction Identify and eliminate real and imagined friction in your journey. If the gym is too far away, make the ground you stand on your gym. Remove unnecessary obstacles, such as the need for a personal trainer. Break down actions into 5-minute chunks, making them easily achievable and minimally taxing on your resources.\nExamples:\nGym too far away? Your ground is the gym. No personal trainer? Do 10 burpees without one. Intimidated by learning Spanish? Start with one word a day. Step 3: Anchor and Toll Anchor your new habits to existing routines in your daily life. Create a toll system where you attach a cost to certain activities, making them conditional on completing specific tasks. For example, learn one Spanish word before accessing the shower. Get creative with tolls to reinforce positive behaviors.\nExamples:\nAnchor: Listen to audiobooks while taking dumps. Toll: 10 push-ups before playing video games. Toll: Learn one Spanish word before taking a shower. Step 4: Pavlov\u0026rsquo;s Dog Conditioning Pair your daily actions with something incredibly enjoyable, creating a Pavlovian association. Follow the sacred rule: only engage in the enjoyable activity when performing the specified action. This conditions your brain to crave the task, making it enjoyable and effortless.\nExample:\nSacred rule: Play a phone game only while holding a plank. Step 5: The Feather Approach Gradually introduce small increments to your routine, making them feel almost negligible. If you started with a 20-second plank, increase to 25 seconds. This feather approach ensures a smooth transition, preventing burnout. Repeat this process until the combined weight of your actions becomes second nature.\nStep 6: Build the Skyscraper Continuously add new Lego bricks and feathers to your routine, creating a skyscraper of positive habits. With a solid anchor or toll, these habits will seamlessly integrate into your daily life. The compounding effect of these habits will lead to exponential growth and make you unstoppable in pursuing larger projects and goals.\nConclusion: By following this step-by-step framework, you can transform your life and become unstoppable in achieving your goals. The key is consistency, creativity, and a commitment to building a skyscraper of positive habits that elevate you to new heights. Start today, and watch as the compounding power of these habits propels you towards a limitless future.\nSource: https://youtu.be/fesslaSxdqs?si=40VEBURtZr6X9yJf\n","permalink":"https://bookofdaniel.in/posts/the-unstoppable-framework/","summary":"\u003cp\u003eWe all aspire to be better, stronger, smarter, and healthier, yet many struggle to stick with their goals. In this post, we\u0026rsquo;ll explore a practical framework that addresses this common issue, allowing you to effortlessly surpass your goals. This system has been successfully used to develop the discipline needed to hold a 15-minute plank every day. Join me on this journey, and let\u0026rsquo;s unlock your raw power together.\u003c/p\u003e\n\u003ch2 id=\"step-1-action-oriented-goals\"\u003eStep 1: Action-Oriented Goals\u003c/h2\u003e\n\u003cp\u003eInstead of setting vague and impractical goals, focus on specific actions within a defined time frame. For instance, rather than aiming to lose 20 pounds, set a goal to complete 10,000 push-ups by the end of the year. Break down the goal into actionable steps, visualize each step, and establish a non-negotiable daily minimum to make progress.\u003c/p\u003e","title":"The Unstoppable Framework: A Step-by-Step Guide to Achieving Your Goals"},{"content":"In the high-speed hustle of modern professional life, we often find ourselves chasing time, trying to grasp every fleeting minute. Productivity isn\u0026rsquo;t just a buzzword; it\u0026rsquo;s a lifeline that keeps us afloat in a sea of deadlines, meetings, and endless tasks. If you\u0026rsquo;re like me, a busy professional always on the lookout for the best way to streamline your day, you know that most productivity systems seem to add more to our plates rather than taking off the load.\nThat\u0026rsquo;s why I\u0026rsquo;ve refined a no-fuss, high-impact productivity system that cuts through the noise. It\u0026rsquo;s a straightforward approach that resonates with the daily grind of our lives. This system doesn\u0026rsquo;t require complex apps or the latest gadgets; it thrives on the bare essentials, making it perfect for the realist who needs to get things done now.\nStrategy 1: The Inverted Pyramid Method 🕒 Picture your day as an inverted pyramid. This is about prioritization. The most critical task sits at the wide top, where you pour a generous three hours of focused effort. As the pyramid tapers, so do the time allocations for less critical tasks, going down to two hours and then one.\nHow It Works: Identify the Most Important Task (MIT): Start with what would make the most significant impact on your day or your overall goal. Allocate Time: Dedicate a solid block of time to this MIT. No distractions, just deep work. Descend the Pyramid: Move on to the next priorities in decreasing time increments. The inverted pyramid is flexible. You can adjust the time allocations based on your workload and energy levels, but the principle remains: start with the most significant task and work your way down.\nStrategy 2: The Pareto Principle or the 80/20 Rule 📊 The Pareto Principle is the idea that 80% of effects come from 20% of causes. It\u0026rsquo;s about leveraging the most impactful actions to yield the greatest results.\nApplying the Principle: Evaluate Your To-Do List: Look at your tasks and ask which ones will have the most considerable impact. Focus on High-Leverage Activities: Direct your efforts toward these high-impact activities. Minimize Low-Impact Work: Avoid spending too much time on tasks that don\u0026rsquo;t contribute significantly to your goals. Remember, the Pareto Principle isn\u0026rsquo;t about doing less work; it\u0026rsquo;s about doing more of the right work. Strategy 3: Intelligent Breaks 🚶‍♂️ Work smart, not hard. That means taking breaks — but not just any breaks. Smart breaks can enhance productivity, while poor ones can detract from it.\nWhat to Do on a Break: High-Intensity Exercise: Even a few minutes can significantly boost BDNF, which is great for brain health and focus. Mindfulness Practices: Meditation or deep breathing can reset your mental state. Nature Walks: A stroll outside can rejuvenate your mind and body. The idea is to step away from your work in a way that refreshes you, not one that further fragments your attention. Strategy 4: Focused Work Blocks 🧘‍♀️ When it\u0026rsquo;s time to work, really work. This is about undivided attention and eliminating multitasking.\nTips for Focused Work: Create a Distraction-Free Environment: Silence your phone, close unnecessary tabs, and clear your workspace. Use Tools to Enhance Focus: Noise-cancelling headphones or apps that block distracting websites can help. Single-Task: Focus entirely on one task at a time to maximize efficiency and quality of work. In summary, the essence of this productivity system lies in its simplicity and flexibility. It\u0026rsquo;s about making the most of your time and effort by focusing on what truly matters. Whether you\u0026rsquo;re aiming for academic excellence, professional growth, or personal development, these strategies can be tailored to fit your needs and help you achieve your goals.\nUntil next time, keep it productive, folks! 🔥\n","permalink":"https://bookofdaniel.in/posts/the-ultimate-productive-system/","summary":"\u003cp\u003eIn the high-speed hustle of modern professional life, we often find ourselves chasing time, trying to grasp every fleeting minute. Productivity isn\u0026rsquo;t just a buzzword; it\u0026rsquo;s a lifeline that keeps us afloat in a sea of deadlines, meetings, and endless tasks. If you\u0026rsquo;re like me, a busy professional always on the lookout for the best way to streamline your day, you know that most productivity systems seem to add more to our plates rather than taking off the load.\u003c/p\u003e","title":"The Ultimate Productivity System: Get Things Done Effortlessly! 🚀"},{"content":"The early days of the internet were marked by a sense of freedom and innovation. Platforms like Twitter, founded by Jack Dorsey and others, were initially open-source ecosystems where developers could contribute, leading to features like hashtags and trending topics. This era was characterized by decentralized protocols, like email, allowing for diverse applications and innovation.\nHowever, as platforms like Facebook, Twitter, and Google grew, they started organizing this vast information, turning social media into centralized repositories. This shift was efficient but led to the accumulation of power in the hands of a few companies, creating \u0026lsquo;walled gardens\u0026rsquo; where user data became a commodity for advertising revenue.\n\u0026ldquo;The digital revolution is not about technology; it\u0026rsquo;s about people.\u0026rdquo; - Howard Rheingold\nThe Problem: Addiction and Control 🕹️ The primary business model of social media companies pivoted to advertising, fostering designs that promote addiction - the infinite scroll, tailored feeds, etc. This change led to a significant shift in how content is delivered and consumed, prioritizing user retention over quality content. Moreover, this centralization led to concerns about censorship and data privacy, as these platforms gained control over what content is visible and who gets to see it.\nEnter NOSTR: A Glimpse of Hope ✨ Amidst these challenges, NOSTR emerged as a potential game-changer. Endorsed by influential figures like Edward Snowden and Twitter\u0026rsquo;s founder Jack Dorsey, NOSTR aims to decentralize social media. By creating an open protocol, it allows for a distribution of control, returning power to the users.\nNOSTR functions similarly to the email protocol, where data is decentralized, and various apps can access and display this data in unique ways. This system enables users to choose how they consume content, bypassing the limitations and biases of a centralized platform\u0026rsquo;s algorithm.\nThe Revolutionary Potential of NOSTR 💪🏻 NOSTR\u0026rsquo;s decentralized nature promises several benefits:\nUser Control and Innovation: Users can choose or even build applications that suit their preferences, fostering innovation. Data Ownership: Users have control over their data, reducing dependency on any single platform. Diverse Perspectives: Decentralization allows for a more diverse range of voices and content, reducing the echo chamber effect prevalent in current social media. The Broader Impact 🔥 NOSTR isn\u0026rsquo;t just about social media; it\u0026rsquo;s about redefining digital communication and interaction. From messaging to business services, its underlying protocol could revolutionize various aspects of our digital lives.\nConclusion: A Call to Action and Participation The journey of NOSTR is not just about technology; it\u0026rsquo;s about community participation and democratizing the digital space. As an open-source project, it invites contributions from everyone, echoing the early days of the internet where innovation and freedom were paramount.\nThe YouTube video, while informative, is just the tip of the iceberg. For those intrigued by NOSTR and its potential, the invitation is open to explore, contribute, and be part of this exciting new phase in the evolution of social media and digital communication.\n","permalink":"https://bookofdaniel.in/posts/the-rise-of-nostr-and-the-renaissance-of-social-media/","summary":"\u003cp\u003eThe early days of the internet were marked by a sense of freedom and innovation. Platforms like Twitter, founded by Jack Dorsey and others, were initially open-source ecosystems where developers could contribute, leading to features like hashtags and trending topics. This era was characterized by decentralized protocols, like email, allowing for diverse applications and innovation.\u003c/p\u003e\n\u003cp\u003eHowever, as platforms like Facebook, Twitter, and Google grew, they started organizing this vast information, turning social media into centralized repositories. This shift was efficient but led to the accumulation of power in the hands of a few companies, creating \u0026lsquo;walled gardens\u0026rsquo; where user data became a commodity for advertising revenue.\u003c/p\u003e","title":"Reclaiming Our Digital Space: The Rise of NOSTR and the Renaissance of Social Media 💪🏻"},{"content":"Hello there! If you\u0026rsquo;ve ever wondered how to make your computer tasks faster and more efficient, you\u0026rsquo;re in the right place. This blog post is all about Batch scripting - a simple yet powerful way to automate tasks in Windows.\nWhether you\u0026rsquo;re new to programming or have some experience, \u0026ldquo;The Batchology Handbook\u0026rdquo; is designed to be easy to follow and understand. We\u0026rsquo;ll start with the basics, like setting up your workspace, and gradually move into more exciting stuff, like automating file management and using cool commands you might not know about.\nBatch scripting might sound a bit technical, but I promise to keep things light and straightforward. You\u0026rsquo;ll find practical examples that you can try out yourself, and I\u0026rsquo;ll explain everything step by step. By the end of this guide, you\u0026rsquo;ll be amazed at how much you can do with just a few simple commands.\nSo, let\u0026rsquo;s get started and explore the world of Batch scripting together!\n1. Introduction Batch programming, also known as batch scripting, is a method of automating repetitive tasks in the Windows operating system. This is achieved through the creation of a batch file, a text file containing a series of commands to be executed by the command-line interpreter. Batch files are identified by their .bat or .cmd extension.\nPurpose The primary purpose of batch programming is to simplify the execution of multiple commands. It\u0026rsquo;s a powerful tool for system administrators, developers, and power users to automate routine tasks like file management, software installation, and system configuration.\nAdvantages Efficiency: Automates repetitive tasks, saving time and effort. Ease of Use: Requires no special programming skills, making it accessible for beginners. Flexibility: Can be combined with other scripts and programs for more complex operations. Resource-Friendly: Consumes minimal system resources. Basic Components Command Prompt: The environment where batch files are executed. Commands: Instructions executed sequentially in a batch file. Scripting Elements: Include loops, conditional statements, and variables. 2. Requirements Batch programming doesn\u0026rsquo;t require any additional software installations on Windows operating systems as it utilizes the built-in Command Prompt. However, to create and edit batch files efficiently, the following are recommended:\nWindows OS: Any modern version (Windows 7, 8, 10, or 11). Text Editor: Notepad (built-in), Notepad++, or any other preferred text editor, Recommended: Visual Studio Code. Configuring the Command Prompt While not strictly necessary, configuring the Command Prompt for better usability can enhance the batch scripting experience:\nAccessing Command Prompt: Press Win + R, type cmd, and press Enter. Alternatively, search for \u0026ldquo;Command Prompt\u0026rdquo; in the start menu. Customization (Optional): Right-click the title bar of the Command Prompt window. Choose \u0026lsquo;Properties\u0026rsquo; to adjust settings like font size, layout, and color scheme for better visibility. Creating Your First Batch File Open a Text Editor: Right-click on your desktop or in any folder, select \u0026ldquo;New\u0026rdquo; \u0026gt; \u0026ldquo;Text Document.\u0026rdquo; Enter a Simple Command: For example, type echo Hello, world!. Save the File with a .bat Extension: Click \u0026lsquo;File\u0026rsquo; \u0026gt; \u0026lsquo;Save As\u0026rsquo;, name your file (e.g., HelloWorld.bat), and change the \u0026lsquo;Save as type\u0026rsquo; to \u0026lsquo;All Files\u0026rsquo;. Ensure the filename ends with .bat. Run Your Batch File: Double-click the file to execute or run it from the Command Prompt. Setting File Associations (Optional) If double-clicking the .bat file doesn\u0026rsquo;t execute it, ensure that .bat files are associated with the Command Prompt: Right-click on a .bat file and select \u0026lsquo;Open with\u0026rsquo; \u0026gt; \u0026lsquo;Choose another app.\u0026rsquo; Select \u0026lsquo;More apps\u0026rsquo;, scroll down, and choose \u0026lsquo;Look for another app on this PC.\u0026rsquo; Navigate to C:\\Windows\\System32 and select cmd.exe. Tips File Locations: Store your batch files in a dedicated folder for easy management. Testing: Always test new scripts in a controlled environment to prevent unintended actions. 3. Basic Commands Batch files execute a series of Command Prompt commands. Here, we\u0026rsquo;ll explore some fundamental commands that form the building blocks of batch scripting.\nCommand Purpose Example Description Echo Displays messages or turns command echoing on/off. echo Hello, world! Displays \u0026ldquo;Hello, world!\u0026rdquo; in Command Prompt. REM Adds comments in the script for readability. REM This is a comment Ignored during execution, used for notes. SET Creates or changes environment variables. SET name=John Sets the value of name to \u0026ldquo;John.\u0026rdquo; GOTO Directs the script to another section. GOTO END Jumps to the label :END in the script. Labels Marks a section of the script. :END Used with GOTO to create script sections. PAUSE Pauses the script, waiting for a key press. PAUSE Used for testing or script pauses. EXIT Exits the Command Prompt or a script. EXIT Closes the Command Prompt window in a script. Sample Batch File 1 2 3 4 5 6 7 8 9 10 @ECHO OFF REM Sample batch script SET name=John echo Hello, %name%! PAUSE GOTO END :END echo Script is ending... EXIT This script introduces basic commands, including setting a variable, displaying messages, and controlling the script flow.\n4. Advanced Scripting Techniques After mastering basic commands, you can enhance your batch scripts with more sophisticated techniques. These advanced methods allow for greater flexibility and functionality in your scripts.\nUsing Variables Dynamic Variable Assignment:Variables can be set based on user input or other commands. Example: SET /P username=Enter your username: This command prompts the user to enter a username, which is stored in the username variable. Manipulating Strings: Batch scripts support basic string manipulation. Example: SET fullname=%firstname% %lastname% Concatenates two variables to create a full name. Conditional Statements IF Command: Used for making decisions in scripts. Syntax: IF [condition] [command] Example: IF %age% LEQ 18 echo You are a minor. Checks if the value of age is less than or equal to 18. Using ELSE: To specify an alternative action if the IF condition is false. Example: IF %number% EQU 10 (echo Number is 10) ELSE (echo Number is not 10) Loops FOR Command: Executes a command for each item in a set. Syntax: FOR %%parameter IN (set) DO command Example: FOR %%G IN (*.txt) DO echo %%G This will echo the name of each .txt file in the current directory. Subroutines CALL Command: Calls another batch file or a label within the same file. Example: CALL :subroutine Calls a label named :subroutine within the script. Creating a Subroutine: Example: 1 2 3 :subroutine echo This is a subroutine. GOTO :EOF Error Handling ErrorLevel: Used to check the status of the last executed command. Example: IF %ERRORLEVEL% NEQ 0 echo Error occurred. Using EXIT /B: Exits the script or subroutine without closing the Command Prompt. Example: EXIT /B 1 Exits the subroutine or script and sets the ErrorLevel to 1. Delay Execution TIMEOUT Command: Pauses the script for a specified number of seconds. Example: TIMEOUT /T 10 Pauses the script for 10 seconds. Sample Advanced Batch File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @ECHO OFF SET /P userinput=Enter a number: IF %userinput% EQU 10 ( echo The number is 10. ) ELSE ( echo The number is not 10. ) FOR %%G IN (*.docx) DO echo Found document: %%G CALL :subroutine EXIT :subroutine echo This is a subroutine. GOTO :EOF This script combines various advanced techniques, showcasing conditional logic, loops, subroutines, and user input.\nConclusion Advanced scripting techniques in batch programming significantly enhance the capabilities of your scripts, allowing you to automate more complex tasks and handle various scenarios efficiently.\n4. File Operations File operations are crucial in batch programming for managing files and directories. This section covers commands to perform various file operations such as creating, copying, moving, and deleting files and directories.\nCreating Files and Directories Creating a New File: Command: type nul \u0026gt; filename.ext Example: type nul \u0026gt; example.txt Creates an empty file named example.txt. Creating a Directory: Command: mkdir directoryname Example: mkdir MyFolder Creates a new directory named MyFolder. Copying Files and Directories Copying a File: Command: copy source destination Example: copy example.txt D:\\Backup\\example.txt Copies example.txt to the D:\\Backup directory. Copying a Directory: Command: xcopy source destination /E /H /C /I Example: xcopy MyFolder D:\\Backup\\MyFolder /E /H /C /I Recursively copies MyFolder to D:\\Backup, including hidden files and subdirectories. Moving and Renaming Files and Directories Moving a File: Command: move source destination Example: move example.txt D:\\Archive Moves example.txt to D:\\Archive. Renaming a File or Directory: Command: ren oldname newname Example: ren example.txt new_example.txt Renames example.txt to new_example.txt. Deleting Files and Directories Deleting a File: Command: del filename Example: del example.txt Deletes example.txt. Deleting a Directory: - Command: rmdir /S /Q directoryname - Example: rmdir /S /Q MyFolder Deletes MyFolder and all of its contents. Working with File Attributes Changing File Attributes: Command: attrib [+|-][R|A|S|H] filename Example: attrib +R example.txt Sets the read-only attribute to example.txt. Sample Script for File Operations 1 2 3 4 5 6 7 @ECHO OFF mkdir MyDocuments type nul \u0026gt; MyDocuments\\doc1.txt xcopy MyDocuments D:\\Backup\\MyDocuments /E /H /C /I move MyDocuments\\doc1.txt MyDocuments\\document.txt del MyDocuments\\doc1.txt rmdir /S /Q MyDocuments This script demonstrates creating a directory and a file, copying the directory, renaming a file, deleting the file, and finally deleting the directory.\nConclusion Understanding file operations in batch programming is essential for effective script writing, especially when handling bulk file management tasks. These operations lay the foundation for automating routine file management tasks in a Windows environment.\n5. Cool Commands In addition to basic file operations and scripting techniques, batch programming offers a plethora of \u0026ldquo;cool commands\u0026rdquo; that can be used for a wide range of purposes, from system administration to advanced file manipulation. Let\u0026rsquo;s explore some of these commands and their functionalities.\nCommand Purpose Usage Reference Link FC Compares two files and displays the differences. FC file1 file2 🔗 FORFILES Executes a command on each file in a set of files. FORFILES /P directory /M searchmask /C cmd 🔗 HELP Provides help information for batch commands. HELP command 🔗 MSG Sends a message to a user or session. MSG [username] 🔗 PROMPT Changes the command prompt. PROMPT [text] 🔗 QUERY Displays the status of a specified service/session. QUERY session 🔗 REG Manages registry keys and values. REG QUERY key 🔗 RUNAS Executes a program under a different user account. RUNAS /USER:user program 🔗 SC Manages Windows services. SC command service_name 🔗 SCHTASKS Schedules commands/programs to run periodically. SCHTASKS /CREATE /SC schedule /TN task /TR run 🔗 SHORTCUT Creates a Windows shortcut. SHORTCUT /F:filename /A:C /T:targetpath 🔗 SHUTDOWN Shuts down or restarts a computer. SHUTDOWN /s /t time 🔗 SUBST Associates a path with a drive letter. SUBST drive: path 🔗 SYSTEMINFO Displays detailed system information. SYSTEMINFO 🔗 TAKEOWN Takes ownership of a file or folder. TAKEOWN /F file_or_folder 🔗 TASKKILL Ends tasks or processes. TASKKILL /IM imagename 🔗 TASKLIST Displays a list of currently running tasks. TASKLIST 🔗 TELNET Communicates using the Telnet protocol. TELNET [host] [port] 🔗 TREE Graphically displays folder structure. TREE path 🔗 TSDISCON Disconnects a Terminal Services session. TSDISCON sessionid 🔗 WHERE Locates and displays file paths matching a pattern. WHERE pattern 🔗 WHOAMI Displays user, group, and privilege information. WHOAMI 🔗 References Windows CMD Command Syntax - SS64.com An A-Z Index of Windows CMD commands - SS64.com ","permalink":"https://bookofdaniel.in/posts/the-batchology-handbook/","summary":"\u003cp\u003eHello there! If you\u0026rsquo;ve ever wondered how to make your computer tasks faster and more efficient, you\u0026rsquo;re in the right place. This blog post is all about Batch scripting - a simple yet powerful way to automate tasks in Windows.\u003c/p\u003e\n\u003cp\u003eWhether you\u0026rsquo;re new to programming or have some experience, \u0026ldquo;The Batchology Handbook\u0026rdquo; is designed to be easy to follow and understand. We\u0026rsquo;ll start with the basics, like setting up your workspace, and gradually move into more exciting stuff, like automating file management and using cool commands you might not know about.\u003c/p\u003e","title":"The Batchology Handbook"},{"content":"In the hustle and bustle of our daily lives, it\u0026rsquo;s easy to let moments slip by without much thought. The idea of journaling, while appealing in theory, often feels daunting in practice. After a long day, the last thing many of us want to do is reflect on our experiences and plan for the next. But what if I told you there\u0026rsquo;s a way to reap the benefits of journaling in just five minutes a day? Enter the 5-Minute Death Journal.\nInspired by positive psychology research, this journaling method is designed to help you remember more, increase productivity, and foster happiness—all in just five minutes. It may sound too good to be true, but after incorporating it into my daily routine for the past month, I can attest to its transformative power.\nThe 5-Minute Death Journal method was introduced to me by SpoonFedStudy, whose insightful YouTube video served as the catalyst for my journey. In his video titled \u0026ldquo;the most important book in your entire life,\u0026rdquo; eloquently outlines the principles and exercises of the journal, offering practical tips for implementation.\nThe journal consists of five simple exercises, each taking just one minute to complete. The first exercise, \u0026ldquo;The Deathbed Time Machine Tour of Today,\u0026rdquo; prompts you to imagine yourself at the end of your life, looking back on the day. What are the top three moments you want your older self to cherish? This exercise encourages you to find meaning in the seemingly mundane and appreciate the small joys that make life rich.\nNext, \u0026ldquo;The Wise Old Man Who Teaches You Stuff\u0026rdquo; challenges you to identify the lessons learned throughout the day. Whether it\u0026rsquo;s a profound realization or a subtle insight, there\u0026rsquo;s always something to be gained from each experience.\nThen comes \u0026ldquo;The Evil Serial Kidnapper,\u0026rdquo; a whimsical yet effective exercise in productivity. Imagine a villain holding your beloved hamster hostage, demanding three concrete tasks for the following day. By setting clear goals, you not only prioritize your time but also gain clarity and focus.\nThe fourth exercise, \u0026ldquo;The Karmic Well of Universal Awesomeness,\u0026rdquo; prompts you to reflect on the positive contributions you\u0026rsquo;ve made to the world. From small acts of kindness to meaningful gestures, every action has the power to make a difference.\nFinally, \u0026ldquo;The Internal Springs of Never-Ending Gratitude\u0026rdquo; invites you to express gratitude for the day\u0026rsquo;s blessings. Research has shown that cultivating gratitude leads to increased happiness and overall well-being, making this exercise a crucial component of the journaling process.\nBy dedicating just five minutes to these exercises each day, you\u0026rsquo;ll develop a deeper appreciation for life\u0026rsquo;s moments and cultivate a mindset of gratitude and purpose. Each day becomes a cherished friend, worthy of celebration and remembrance.\nSo why wait? Start your own journey with the 5-Minute Death Journal today, inspired by SpoonFedStudy insightful guidance. After all, every moment is precious—let\u0026rsquo;s make the most of them together.\n","permalink":"https://bookofdaniel.in/posts/the-5-minute-death-journal-method/","summary":"\u003cp\u003eIn the hustle and bustle of our daily lives, it\u0026rsquo;s easy to let moments slip by without much thought. The idea of journaling, while appealing in theory, often feels daunting in practice. After a long day, the last thing many of us want to do is reflect on our experiences and plan for the next. But what if I told you there\u0026rsquo;s a way to reap the benefits of journaling in just five minutes a day? Enter the 5-Minute Death Journal.\u003c/p\u003e","title":"The 5 Minute Death Journal Method"},{"content":"Yes, you read that right. The port you’re trying to use is already occupied by another application. If you’re a system or network administrator, you’ve likely encountered this frustrating message more times than you care to remember. It’s challenging to pinpoint which application is hogging the port (though it’s not as hard as it seems, as I’ll show you 😉). These ghost messages can leave you in quite a pickle.\nJust yesterday, we faced this exact issue and wasted two hours troubleshooting so that you don’t have to!\nOur situation was this: We had installed SQL Server 2019 Reporting Services on one of our development servers, and the default port it selected for installation was 8082. Unfortunately, this port was already in use by our front-facing application, so we needed to free it up.\nAfter some frantic Googling, we found a solution: we updated the port from 8082 to 8083 in the config.json file located in the Reporting Services folder. (You can usually find the file here: C:\\Program Files\\Microsoft SQL Server Reporting Services\\SSRS\\RSHostingService\\RSHostingService.exe.) We then restarted the SQL Server Reporting Services Windows service, and voilà—RSHostingService.exe started running on port 8083.\nWe thought everything was resolved and that port 8082 was free. However, when we configured our front-end application and tried to access it, we were greeted with a 503 Server Unavailable error. 😢\nTroubleshooting Port Availability To narrow down the issue, it’s helpful to identify which application is using the port.\nFinding the Process Using netstat The netstat command, short for \u0026ldquo;network statistics,\u0026rdquo; is a command-line tool that displays active network connections, routing tables, and various network interface statistics. By running the netstat command with the noa parameters, we can list all the listening ports with process ID information and pipe that command to search for the specific port.\n1 netstat -noa | find \u0026#34;8082\u0026#34; Output:\n1 2 TCP 0.0.0.0:8082 0.0.0.0:0 LISTENING 4 TCP [::]:8082 [::]:0 LISTENING 4 The process ID 4 indicates that it’s a system process. Unfortunately, this doesn’t tell us which application is using the port, so we need another method.\nFinding the Process Using netsh Netsh, short for Network Shell, is a command-line utility that allows users to configure and manage network devices both locally and remotely. It’s particularly useful for tasks like changing IP addresses, resetting the TCP/IP stack, and managing wireless settings.\nWe can use netsh to query the HTTP request and find out which process is using port 8082:\n1 netsh http show servicestate view=requestq Save the output of the above command to a file, open it using Notepad, and search for the port details:\n1 2 netsh http show servicestate view=requestq \u0026gt; results.log notepad results.log Output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Request queue name: Request queue is unnamed. Version: 2.0 State: Active Request queue 503 verbosity level: Basic Max requests: 1000 Number of active processes attached: 1 Process IDs: 2312 URL groups: URL group ID: BE00000340000001 State: Active Request queue name: Request queue is unnamed. Properties: Max bandwidth: inherited Max connections: inherited Timeouts: Timeout values inherited Number of registered URLs: 1 Registered URLs: HTTP://LOCALHOST:8082/ Server session ID: C000000320000001 Version: 2.0 State: Active Properties: Max bandwidth: 4294967295 Timeouts: Entity body timeout (secs): 120 Drain entity body timeout (secs): 120 Request queue timeout (secs): 120 Idle connection timeout (secs): 120 Header wait timeout (secs): 120 Minimum send rate (bytes/sec): 150 Now you can see that port 8082 is being used by process 2312. You can check Task Manager to see which process is associated with this PID, or use the following command:\n1 tasklist /FI \u0026#34;PID eq 2312\u0026#34; Output:\n1 2 3 Image Name PID Session Name Session# Mem Usage ========================= ======== ================ =========== ============ SQLServerReportingServices 2312 Services 0 42,232 K What the Heck?! 😮 How is this possible? We changed the reporting tool config file, yet netsh still shows that port 8082 is in use. Upon rechecking the reporting service logs, we confirmed that the application is indeed running on port 8083. 🤔\nIt turns out that the URL with port 8082 was reserved by SQL Reporting Services with an Access Control List (ACL) during installation. Changing the config file alone wasn’t enough. To see the list of URL reservations for HTTP services, use this command:\n1 netsh http show urlacl Output:\n1 2 3 4 5 Reserved URL : https://*:8082/ User: NT SERVICE\\SQLServerReportingServices Listen: Yes Delegate: No SDDL: D:(A;;GX;;;BU)(A;;GX;;;LS) To free up port 8082 for our front-end application, we needed to delete this obsolete HTTP reservation:\n1 netsh http delete urlacl url=http://+:8082/ Output:\n1 Deleted Successfully. To confirm that the port is free and available for use, run:\n1 netstat -noa | find \u0026#34;8082\u0026#34; The expected output should be blank! 😉\nFinally, the port is free!\n","permalink":"https://bookofdaniel.in/posts/port-not-available/","summary":"\u003cp\u003e\u003cstrong\u003eYes, you read that right.\u003c/strong\u003e The port you’re trying to use is already occupied by another application. If you’re a system or network administrator, you’ve likely encountered this frustrating message more times than you care to remember. It’s challenging to pinpoint which application is hogging the port (though it’s not as hard as it seems, as I’ll show you 😉). These ghost messages can leave you in quite a pickle.\u003c/p\u003e","title":"Port Not Available"},{"content":"Picture it: your bed, a chaotic landscape of tangled limbs and rogue pillows, mirroring the state of your life. Enter Admiral William McRaven\u0026rsquo;s \u0026ldquo;Make Your Bed,\u0026rdquo; a book that\u0026rsquo;s anything but your average fluff-and-fold self-help manual. Think less feng shui, more forging a battle cry in the crucible of Navy SEAL training. This ain\u0026rsquo;t about smoothing out your duvet; it\u0026rsquo;s about scaling metaphorical Mount Everest, one messy pillowcase at a time.\nMcRaven throws down ten life lessons that sting with truth and resonate with anyone who\u0026rsquo;s ever wrestled with life\u0026rsquo;s tangled sheets.\nLesson 1: Start small, conquer big. Making your bed is a tiny triumph, but it sends a ripple of accomplishment through your day. It whispers, \u0026ldquo;I can do this,\u0026rdquo; a mantra that empowers you to tackle seemingly insurmountable obstacles. You know, like that presentation that feels like climbing Everest in stilettos. One sheet at a time, you\u0026rsquo;ll conquer it.\nIf you want to change the world, start off by making your bed.\nLesson 2: Teamwork makes the dream (and boat) work. No SEAL goes solo. They rely on their squad, their cheerleaders with flippers, their \u0026ldquo;band of brothers\u0026rdquo; who navigate the choppy waters together. This translates to life as well. Find your tribe, those who\u0026rsquo;ll high-five your victories and hold your paddle when the current gets rough. Remember, even the mightiest battleship needs an anchor.\nIf you want to change the world, find someone to help you paddle.\nLesson 3: Size doesn\u0026rsquo;t matter, heart does. Forget judging people by their bank accounts or biceps. McRaven reminds us that true strength lies in character, resilience, and the size of your (figurative) heart. A kind word from the smallest person can change the world, so spread kindness like confetti. Remember, even the tiniest pebble can create the biggest ripples.\nIf you want to change the world, find someone to help you paddle. If you want to change the world, measure a person by the size of their heart, not the size of their flippers.\nLesson 4: Life is a sugar cookie crumble (and that\u0026rsquo;s okay). Sometimes, no matter how hard you bake, your life cookie crumbles. You bomb a presentation, trip in public, or stub your toe on the existential dread of the universe. It happens. The key is to dust yourself off, lick the icing sugar tears, and keep baking. Remember, even the most perfectly decorated cookies sometimes fall apart.\nIf you want to change the world, get over being a sugar cookie and keep moving.\nLesson 5: Embrace the circus (of failure). Life is a three-ring spectacle of awkward moments, missed opportunities, and flat tires. Don\u0026rsquo;t let fear of failure paralyze you. Embrace the stumbles, learn from the clowns, and remember, even the most graceful tightrope walkers wobble sometimes. So stumble gracefully, laugh at your mishaps, and remember, falling down is just another way of flying (sort of).\nIf you want to change the world, don’t be afraid of the Circuses.\nLesson 6: Sometimes, headfirst is best. Take risks, challenge the status quo, and don\u0026rsquo;t be afraid to slide headfirst down the metaphorical rope of life. You might surprise yourself, break a record, and land with a grin on your face (and maybe a few scrapes). Remember, playing it safe won\u0026rsquo;t get you to the trapeze, so swing boldly, even if it means a few bumps on the way.\nIf you want to change the world, sometimes you have to slide down the obstacle headfirst.\nLesson 7: Sharks are inevitable (but you can swim with them). The world is full of metaphorical sharks: bullies, doubters, and negativity. You can\u0026rsquo;t avoid them, but you can learn to navigate their waters. Stand your ground, be your best self, and remember, sometimes the best way to disarm a shark is with a confident smile and a well-placed tuna sandwich. Just like dolphins, swim with the sharks, but don\u0026rsquo;t become one.\nIf you want to change the world, don’t back down from the sharks.\nLesson 8: Be your lighthouse in the storm. When darkness descends, don\u0026rsquo;t let it extinguish your inner light. Stay calm, draw on your inner strength, and remember, even the faintest glimmer can guide others through the roughest seas. Be the beacon in the storm, the lighthouse in the fog, the reminder that even when the waves crash, hope keeps us afloat.\nIf you want to change the world, you must be your very best in the darkest moment.\nLesson 9: Sing in the mud (it helps). Hope is a powerful weapon. When you\u0026rsquo;re neck-deep in life\u0026rsquo;s muck, find a reason to sing. Share your light, inspire others, and remember, even a muddy chorus can change the world. So hum a tune, even if it\u0026rsquo;s off-key, because sometimes the only way to rise above the mud is to sing through it.\nIf you want to change the world, start singing when you’re up to your neck in mud.\nLesson 10: Never ring the bell (unless it\u0026rsquo;s for tea). Life throws challenges, but quitting is not an option. Push through the discomfort, persevere through the pain, and remember, the bell is always there, but the satisfaction of overcoming your fears is far sweeter than any escape. So keep climbing, keep fighting, keep singing in the mud, because the view from the top of the mountain is worth\nIf you want to change the world, don’t ever, ever ring the bell.\n","permalink":"https://bookofdaniel.in/posts/make-your-bed-summary/","summary":"\u003cp\u003ePicture it: your bed, a chaotic landscape of tangled limbs and rogue pillows, mirroring the state of your life. Enter \u003ca href=\"https://en.wikipedia.org/wiki/William_H._McRaven\"\u003eAdmiral William McRaven\u0026rsquo;s\u003c/a\u003e \u0026ldquo;\u003ca href=\"https://www.amazon.in/Make-Your-Bed-William-McRaven/dp/0718188861\"\u003eMake Your Bed\u003c/a\u003e,\u0026rdquo; a book that\u0026rsquo;s anything but your average fluff-and-fold self-help manual. Think less feng shui, more forging a battle cry in the crucible of Navy SEAL training. This ain\u0026rsquo;t about smoothing out your duvet; it\u0026rsquo;s about scaling metaphorical Mount Everest, one messy pillowcase at a time.\u003c/p\u003e","title":"Make Your Bed Summary"},{"content":"In a recent YouTube video, a former WhatsApp engineer shared his insights on success in the tech industry and the key lessons he learned during his time at the company. Here are the main takeaways from his experience, formatted with lists and paragraphs:\nKey Lessons Learned Influence and Likability: Influence in the tech industry is about being likable and credible. People who are dependable and pleasant to work with are more likely to be sought after for new opportunities and promotions. Focus on building a strong reputation by being a good team player and delivering results. 👍\nChoose Your Passion: The engineer emphasized the importance of choosing one thing to be known for and following your passions. Blindly following trends or chasing popularity can lead to failure. Instead, focus on your strengths and interests to drive your success. 🔥\nSimplicity and Prioritization: WhatsApp\u0026rsquo;s engineering culture was centered around simplicity and prioritization. The company focused on a few core features and offerings, ensuring that everything worked seamlessly and efficiently. This approach helped WhatsApp grow from 20 million to 1.2 billion active monthly users. 🚀\nUnderstanding Your Audience: WhatsApp\u0026rsquo;s success was also attributed to its ability to prioritize user experience. The company designed its app to work well even for users with poor internet connections and old devices, ensuring that everyone could use the platform. 📱\nClear Communication and Goal Setting: WhatsApp\u0026rsquo;s engineering culture emphasized clear communication and goal setting. The company\u0026rsquo;s leadership provided clear messaging about which features to prioritize, and engineers were encouraged to understand and align with the company\u0026rsquo;s long-term goals. 📈\nAdapting to Change: The engineer\u0026rsquo;s experience with WhatsApp\u0026rsquo;s acquisition by Facebook demonstrated the importance of adapting to change in the tech industry. Embrace new opportunities and learn from them, as they can lead to personal and professional growth. 🌟\nQuote of the Video \u0026ldquo;I do recommend, even if you are really good at all of them, please choose one thing what do you want to be known for as.\u0026rdquo;\nChannel Credit Don\u0026rsquo;t forget to subscribe this channel that shared this valuable content: Entrepreneurship Opportunities. 👏\nIn conclusion, success in the tech industry requires a combination of influence, passion, simplicity, and prioritization. By focusing on these principles and learning from the experiences of others, you can increase your chances of success and create a meaningful career for yourself in the technology sector.\n","permalink":"https://bookofdaniel.in/posts/lessons-learned-from-a-former-whatsapp-engineer/","summary":"\u003cp\u003eIn a recent YouTube video, a former WhatsApp engineer shared his insights on success in the tech industry and the key lessons he learned during his time at the company. Here are the main takeaways from his experience, formatted with lists and paragraphs:\u003c/p\u003e\n\u003ch3 id=\"key-lessons-learned\"\u003eKey Lessons Learned\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInfluence and Likability\u003c/strong\u003e: Influence in the tech industry is about being likable and credible. People who are dependable and pleasant to work with are more likely to be sought after for new opportunities and promotions. Focus on building a strong reputation by being a good team player and delivering results. 👍\u003c/p\u003e","title":"Lessons Learned From a Former WhatsApp Engineer"},{"content":"Welcome to our exploration of Kubernetes architecture! Kubernetes, often abbreviated as K8s, is a powerful tool designed to manage containerized applications in a scalable and automated fashion. This blog post aims to provide a high-level understanding of Kubernetes architecture using an analogy of ships, which simplifies the complex interplay between its components. Special thanks to KodeKloud for their insightful lecture, which inspired this overview.\nThe Kubernetes Cluster: Ships at Sea At its core, a Kubernetes cluster comprises two types of nodes: worker nodes and master nodes. To understand their roles, let\u0026rsquo;s imagine a fleet of ships:\nWorker Nodes (Cargo Ships): These ships carry the actual load—in Kubernetes, this translates to running your containerized applications. Master Nodes (Control Ships): These ships oversee and manage the cargo ships, ensuring everything runs smoothly. Master Nodes: The Control Ships Master nodes are the brain of the Kubernetes cluster, managing the state and operations of the entire system. They contain several key components, collectively known as the control plane:\netcd: A highly available key-value store that keeps all cluster data. Think of it as the central database that logs the details of every ship and container.\nKube Scheduler: Similar to cranes that load containers onto ships, the Kube Scheduler assigns containers to nodes based on resource requirements, node capacity, and various constraints like node affinity and tolerations.\nControllers: These are specialized offices on the control ships handling specific tasks:\nNode Controller: Manages the state of nodes, ensuring they are operational and adding new ones to the cluster as needed. Replication Controller: Ensures the desired number of container replicas are running at all times. Kube API Server: The central management component that orchestrates all operations within the cluster. It exposes the Kubernetes API, allowing users and controllers to interact with the cluster, monitor its state, and make necessary changes.\nWorker Nodes: The Cargo Ships Worker nodes are where your applications run, managed by two key components:\nKubelet: The captain of the ship, responsible for communicating with the Kube API Server, receiving instructions, and managing containers on the node. The Kubelet also sends status reports back to the master nodes.\nKube-proxy: Ensures smooth communication between containers across different nodes by maintaining network rules on each node. This is essential for services to interact seamlessly, like a web server on one node communicating with a database server on another.\nContainer Runtime All nodes, whether master or worker, require a container runtime engine to run containers. While Docker is a popular choice, Kubernetes also supports other runtimes like containerd and CRI-O. This runtime environment is crucial for deploying containerized applications and components.\nConclusion Kubernetes architecture, with its intricate components and interactions, ensures efficient and scalable management of containerized applications. By likening it to a fleet of ships with dedicated roles and responsibilities, we can better understand the complex but elegant orchestration that Kubernetes provides.\nIn upcoming posts, we will delve deeper into each component, exploring their configurations and roles in greater detail. Stay tuned as we continue to navigate the fascinating world of Kubernetes!\nCredit: This content is inspired by a lecture from KodeKloud.\n","permalink":"https://bookofdaniel.in/posts/kubernetes-architecture-explained-the-ship-analogy-of-kodekloud/","summary":"\u003cp\u003eWelcome to our exploration of Kubernetes architecture! Kubernetes, often abbreviated as K8s, is a powerful tool designed to manage containerized applications in a scalable and automated fashion. This blog post aims to provide a high-level understanding of Kubernetes architecture using an analogy of ships, which simplifies the complex interplay between its components. Special thanks to KodeKloud for their insightful lecture, which inspired this overview.\u003c/p\u003e\n\u003ch3 id=\"the-kubernetes-cluster-ships-at-sea\"\u003eThe Kubernetes Cluster: Ships at Sea\u003c/h3\u003e\n\u003cp\u003eAt its core, a Kubernetes cluster comprises two types of nodes: worker nodes and master nodes. To understand their roles, let\u0026rsquo;s imagine a fleet of ships:\u003c/p\u003e","title":"Kubernetes Architecture Explained the Ship Analogy of KodeKloud"},{"content":"Original Post : https://cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it\nIn the dynamic world of cloud computing, the term \u0026lsquo;cloud-native architecture\u0026rsquo; is often synonymous with innovation and efficiency. This article, inspired by a Google Cloud blog post, expands the concept to encompass universal principles applicable across all cloud platforms, be it Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). 🌦️🏗️\nUnderstanding Cloud-Native Architecture Cloud-native architecture is about leveraging the unique capabilities of cloud environments. It goes beyond mere migration to the cloud; it\u0026rsquo;s about optimizing applications to thrive in this dynamic ecosystem. 🌩️💡\nKey aspects include:\nFunctional Requirements: What the system needs to do. (e.g., processing orders) Non-Functional Requirements: Performance metrics like processing speed and efficiency. Constraints: Limitations or fixed aspects within which the system must operate. In a cloud-native context, these elements are approached with flexibility and innovation, ensuring systems are resilient, cost-effective, and easily maintainable. 🛠️⚖️\nFive Universal Principles of Cloud-Native Architecture Design for Automation: Automation is key in cloud-native systems. Embrace tools and practices that facilitate automated infrastructure management, deployment, and scaling. Think Jenkins, Terraform, or GitLab CI for streamlined operations. 🤖🔄\nBe Smart with State: Prioritize stateless components. They are easier to scale, repair, roll back, and balance. Stateless design simplifies complex cloud environments. 📊🔁\nFavor Managed Services: Opt for cloud-managed services when possible. They offer convenience and efficiency, reducing the headache of backend management. The choice between open-source compatible services, highly beneficial managed services, and others should be weighed against organizational needs. ☁️🛠️\nPractice Defense in Depth: In the cloud, security is multi-layered. Implement robust authentication, rate limiting, and protective measures across all components. This reduces vulnerability and strengthens the overall security posture. 🛡️🔒\nAlways Be Architecting: Cloud architecture is an evolving journey. Adapt and refine your architecture in response to changing needs and technologies. This mindset ensures your systems remain relevant and efficient. 🔄🌟\nAdapting to Change In the cloud, adaptation is not just a strategy; it\u0026rsquo;s a necessity for survival. Like evolving species, cloud architectures must continuously adapt to their environment. This evolution isn\u0026rsquo;t a linear journey but a constant, dynamic process. 🌿📈\nConclusion Adopting cloud-native architecture principles isn\u0026rsquo;t just about leveraging new technologies; it\u0026rsquo;s about embracing a mindset of continual adaptation and optimization. These universal principles guide you to make the most of any cloud environment, ensuring your systems are resilient, agile, and future-ready. Change might be challenging, but it\u0026rsquo;s the key to thriving in the ever-evolving cloud landscape. 💪☁️\n","permalink":"https://bookofdaniel.in/posts/key-principles-of-any-cloud-environments/","summary":"\u003cp\u003eOriginal Post : \u003ca href=\"https://cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it\"\u003ehttps://cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIn the dynamic world of cloud computing, the term \u0026lsquo;cloud-native architecture\u0026rsquo; is often synonymous with innovation and efficiency. This article, inspired by a Google Cloud blog post, expands the concept to encompass universal principles applicable across all cloud platforms, be it Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). 🌦️🏗️\u003c/p\u003e\n\u003ch3 id=\"understanding-cloud-native-architecture\"\u003eUnderstanding Cloud-Native Architecture\u003c/h3\u003e\n\u003cp\u003eCloud-native architecture is about leveraging the unique capabilities of cloud environments. It goes beyond mere migration to the cloud; it\u0026rsquo;s about optimizing applications to thrive in this dynamic ecosystem. 🌩️💡\u003c/p\u003e","title":"Key Principles of Any Cloud Environments"},{"content":"I\u0026rsquo;ve had the privilege of navigating through a wide array of technical landscapes. Yet, the recent endeavor of configuring a Windows Server Failover Cluster (WSFC) for SQL Server AlwaysOn presented a learning curve that was both steep and enriching. In this piece, I hope to share my experiences, the challenges encountered, and the lessons learned, with a spirit of humility and the intention of contributing to our collective knowledge base.\nEncountering the First Major Challenge: Adding a Cloud Witness The journey began with an attempt to integrate a cloud witness into our WSFC configuration, a task that quickly unfolded into a series of troubleshooting steps. The primary node\u0026rsquo;s refusal to fail over automatically, sticking instead in a \u0026ldquo;resolving\u0026rdquo; state, was our first major roadblock. The error message pointed towards an authentication and network recognition issue:\n\u0026ldquo;An error was encountered while modifying the quorum settings. ERROR CODE: 0x80131500; NATIVE ERROR CODE: 1. WinRM cannot process the request\u0026hellip; Cannot find the computer proddbcluster.test.com.\u0026rdquo;\nNavigating Through the Storm The resolution required a combination of technical know-how and a willingness to delve deep into the problem. Here\u0026rsquo;s how we approached it:\nStep-by-Step Cloud Witness Configuration: I leaned on a comprehensive guide to configure a cloud witness via PowerShell, a testament to the importance of following detailed instructions meticulously 🔗. Ensuring Secure Communications: Verifying SSL TLS 1.2 implementation was crucial, highlighting the necessity of adhering to secure communication protocols 🔗. Centralizing Control from the Owner Node: Initiating configuration changes from the owner node underscored the importance of a centralized command approach in managing cluster configurations effectively. Clearing the Slate: Prior to setting up anew, I used PowerShell scripts to clear any existing cloud witness configurations, ensuring we started from a clean slate 🔗. WinRM Configuration Check: The winrm quickconfig command was a crucial step in confirming the operational status of Windows Remote Management across all nodes, a critical component for remote management 🔗. Testing Node-to-Storage Communication: It was essential to confirm that there were no communication barriers between the nodes and the cloud witness storage account, foundational for the health of the cluster. Utilizing Advanced Options for Cloud Witness Addition: During the cloud witness addition, selecting advanced options and ensuring all nodes were included was a critical step for a comprehensive setup 🔗. Firewall Configuration Adjustments: Adjusting the firewall settings to facilitate WinRM communication and ensuring cloud witness storage accessibility from all networks were the breakthrough moments in this journey 🔗. The Challenge of Failover Resolution Another significant challenge was the primary node\u0026rsquo;s failure to automatically fail over to the secondary node during testing. This was an unexpected issue that required further investigation and adjustment.\nFinding a Path Forward The solution involved adjusting the Maximum Failover Period within the SQL Server configurations, a reminder of the importance of reviewing and fine-tuning these settings to accommodate our testing needs 🔗.\nEmbracing Humility and the Joy of Learning This experience has been a humbling reminder that no matter the length of time one spends in the field, there\u0026rsquo;s always more to learn. The challenges faced and the solutions found have reinforced a few core principles:\nPrecision Matters: The importance of attention to detail in every step of the configuration process cannot be overstated. Persistence Pays Off: Facing down errors and persisting through troubleshooting is part of the journey. Each challenge is an opportunity to learn. The Value of Community: The guidance and solutions shared by the broader community have been invaluable. It\u0026rsquo;s a reminder of the strength found in collective knowledge. Final Thoughts The process of configuring WSFC for SQL Server AlwaysOn is a complex one, filled with potential pitfalls but also opportunities for growth and learning. My hope is that by sharing my journey, I can help others navigate similar challenges more smoothly. In the ever-evolving field of cloud and database architecture, each new project is a chance to expand our horizons and deepen our understanding.\n","permalink":"https://bookofdaniel.in/posts/issue-faced-on-wsfc-configuration-for-sql-server-alwayson-setup/","summary":"\u003cp\u003eI\u0026rsquo;ve had the privilege of navigating through a wide array of technical landscapes. Yet, the recent endeavor of configuring a Windows Server Failover Cluster (WSFC) for SQL Server AlwaysOn presented a learning curve that was both steep and enriching. In this piece, I hope to share my experiences, the challenges encountered, and the lessons learned, with a spirit of humility and the intention of contributing to our collective knowledge base.\u003c/p\u003e","title":"Issue Faced on WSFC Configuration for SQL Server AlwaysOn Setup"},{"content":"Have you ever been a part of a hardworking team that, despite their efforts, ends up shouldering blame? If yes, our situation in operations might resonate with you. On any given day, our Zoho Desk is inundated with approximately 300 tickets, each falling under various classifications and priorities. Many of these tickets are complex and time-consuming to resolve, causing them to carry over to the next day and become overdue.\nThe challenge we face is the lack of visibility into critical metrics. Questions like the total number of tickets created, the various classifications in use, the resource who close the most tickets, and pending tickets are difficult to answer definitively. Convincing others about the work overload has proven to be quite a task.\nWhile Zoho Desk does offer predefined dashboards and customization options, our workflow relies heavily on custom fields. Additionally, the platform lacks real-time monitoring capabilities.\nIf you\u0026rsquo;ve followed along this far, you may find yourself in a similar scenario or be seeking solutions for building real-time monitoring functionality within Zoho Desk. Keep reading!\nChallenges As a team, we set out to tackle these challenges head-on, with the ultimate goal of maximizing throughput and efficiency to ensure customer satisfaction. Here are the key challenges we identified on our team board:\n1. Large Team Management: With a growing client base, our team size has expanded proportionally. Managing a larger team is becoming increasingly challenging without access to key metrics.\n2. Ticket Assignment Automation: Currently, our ticket assignment process involves agents or team members manually assigning tickets from a common email account. Some agents assign tickets to themselves, while others choose to assign them to the next team member. This variation in assignment might be due to differences in ticket complexity, with some opting for the low-hanging fruit.\n3. Timely Escalation: To maintain SLA compliance, we required a solution which will escalate the tickets to a manager when there is breach in a predefined time threshold (e.g., x minutes/hours).\n4. Email Tickets with Mandatory Fields: Zoho Desk offers a convenient feature that allows users to raise tickets via email. However, this feature creates tickets without mandatory fields, such as client or environment name. This calls for communicating the team over and over again resulting in time consumption.\n5. Real-Time Monitoring \u0026amp; Scoreboard: The absence of a real-time dashboard has caused inefficiencies in tracking productivity.\nImproving Daily work is even more important than doing daily work. - The Phoenix Project\nSolutions I have slept on these challenges for a day and explored the Zoho documentation along with some Google search. It seemed like I could solve some of these issues using zoho\u0026rsquo;s features, while others might require development.\nChallenge Solution Large Team Management Proposed to split the team into multiple pods or pools. So that it can be managed by a senior / lead with minimal members and work can be monitored efficiently. Pool segregation will be based on the number of clients. Ticket Assignment Automation Since the team is arranged into multiple pools, based on the client hierarchy, whenever a ticket is raised for the specific client it should be allocated directly to the respecitve pool. Then the pool lead / member can assign and work accordingly. To Automate this process Zoho Desk provides a feature called Direct Assignment. Create an assignment rule with client name and select respective pool account. Viola! The ticket will be assigned automatically on creation. Timely Escalation Zoho provides a feature called SLAs escalation. If an escalation occurs within x minutes it will be escalated to 2 levels. We chose to adopt it, so that the breached ticket will be esclated to the respective lead / manager of the pool within 30 mins. Email Ticket with mandatory fields We created a Zoho Task to close the email ticket with an automated reply to fill the mandatory fields. So the tickets in which the mandatory fields are not filled will be closed automatically. Real-Time Monitoring \u0026amp; Scoreboard Zoho analytics with additional license offers Dashboard experience but I needed something more interesting like a real-time dashboard to proactively act on tickets which does not require additional license.\nSo I came up with a solution which is powered by Grafana Dashboard and projected it on a monitor. For that, I used zoho web hook, instead of polling ticket meta details from Zoho desk.\nThe Webhook listener is developed using a simple java script. The json payload recieved will be deserialized and stored in a relational database like SQL Server.\nThen I created a Grafana Dashboard to pull the details from the database.\nArchitecture Diagram Final Results By incorporating real-time monitoring for Zoho tickets, my team\u0026rsquo;s productivity and efficiency increased tenfold. This enhancement not only bolstered accountability but also motivated everyone to aim higher and soar!\nA great team doesn’t mean that they had the smartest people. What made those teams great is that everyone trusted one another. It can be a powerful thing when that magic dynamic exists. - The Phoenix Project\n","permalink":"https://bookofdaniel.in/posts/integration-of-zoho-desk-and-grafana/","summary":"\u003cp\u003eHave you ever been a part of a hardworking team that, despite their efforts, ends up shouldering blame? If yes, our situation in operations might resonate with you. On any given day, our Zoho Desk is inundated with approximately 300 tickets, each falling under various classifications and priorities. Many of these tickets are complex and time-consuming to resolve, causing them to carry over to the next day and become overdue.\u003c/p\u003e","title":"Integration of Zoho Desk and Grafana"},{"content":"Installing Kafka and Kafka Connect on separate servers allows for better resource management, especially in production environments where Kafka brokers and Connectors may need dedicated hardware. This guide will walk you through the steps to set up Kafka and Kafka Connect on separate Linux servers, using Ubuntu 24.04.\nPrerequisites Two Linux machines (Ubuntu 24.04) Inbound ports opened on both servers: Kafka broker ports: 9092, 9093 Kafka Connect REST port: 8083 Java 11 (OpenJDK) installed on both servers Step 1: Set Up Kafka on the First Server Set the hostname for the Kafka server:\n1 2 sudo hostnamectl set-hostname kafka-server sudo nano /etc/hosts Replace 127.0.1.1 with your new hostname.\nInstall necessary updates: Update and upgrade your server with:\n1 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade Reboot the server to apply changes:\n1 sudo reboot Install OpenJDK: Kafka requires Java to run. Install OpenJDK 11:\n1 sudo apt install openjdk-11-jdk -y Verify the installation:\n1 java -version Download and Extract Kafka: Download Kafka from the official site:\n1 2 3 wget https://dlcdn.apache.org/kafka/3.8.0/kafka_2.13-3.8.0.tgz tar -xzf kafka_2.13-3.8.0.tgz sudo mv kafka_2.13-3.8.0 /opt/kafka Create a Kafka User and Group: For better management, create a dedicated service account:\n1 2 3 sudo groupadd kafka sudo useradd -r -g kafka -d /opt/kafka -s /bin/false kafka sudo chown -R kafka:kafka /opt/kafka Configure Kafka: Kafka stores logs in /tmp by default. To make management easier, move the logs to /var/log/kafka:\n1 2 3 sudo mkdir -p /var/log/kafka sudo chown -R kafka:kafka /var/log/kafka sudo chmod -R 755 /var/log/kafka Edit the Kafka Configuration: Update the server.properties file to listen on all interfaces:\n1 sudo nano /opt/kafka/config/kraft/server.properties Add the following lines:\n1 2 3 listeners=PLAINTEXT://0.0.0.0:9092 advertised.listeners=PLAINTEXT://\u0026lt;Kafka_Server_IP\u0026gt;:9092 log.dirs=/var/log/kafka Format the Log Directory: Change directory to Kafka folder and format the storage with a unique cluster ID:\n1 2 3 cd /opt/kafka KAFKA_CLUSTER_ID=\u0026#34;$(bin/kafka-storage.sh random-uuid)\u0026#34; sudo -u kafka bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties Set Up Kafka as a Systemd Service: Create a service file:\n1 sudo nano /etc/systemd/system/kafka.service Add the following content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description=Apache Kafka Server (KRaft Mode) After=network.target [Service] User=kafka Group=kafka ExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/kraft/server.properties ExecStop=/opt/kafka/bin/kafka-server-stop.sh Restart=on-failure RestartSec=10 [Install] WantedBy=multi-user.target Enable and start the Kafka service:\n1 2 3 4 sudo systemctl daemon-reload sudo systemctl start kafka sudo systemctl enable kafka sudo systemctl status kafka Step 2: Set Up Kafka Connect on the Second Server Install necessary updates: Update and upgrade your server with:\n1 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade Reboot the server to apply changes:\n1 sudo reboot Install Java on Kafka Connect Server: Just like with the Kafka server, install Java:\n1 2 sudo apt install openjdk-11-jdk -y java -version Download and Extract Kafka Connect: Kafka Connect is part of the Kafka package, so download Kafka here too:\n1 2 3 wget https://dlcdn.apache.org/kafka/3.8.0/kafka_2.13-3.8.0.tgz tar -xzf kafka_2.13-3.8.0.tgz sudo mv kafka_2.13-3.8.0 /opt/kafka Configure Kafka Connect: Edit the connect-distributed.properties file to configure Kafka Connect:\n1 sudo nano /opt/kafka/config/connect-distributed.properties Update the plugin path:\n1 2 plugin.path=/opt/kafka/connectors bootstrap.servers=\u0026lt;Kafka_Server_IP\u0026gt;:9092 Create a Connectors Directory: Create a directory for Kafka Connect plugins:\n1 2 sudo mkdir /opt/kafka/connectors sudo chown kafka:kafka /opt/kafka/connectors Set Up Kafka Connect as a Systemd Service: Create a service file:\n1 sudo nano /etc/systemd/system/kafka-connect.service Add the following content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description=Kafka Connect Distributed Mode Service After=network.target [Service] User=kafka Group=kafka ExecStart=/opt/kafka/bin/connect-distributed.sh /opt/kafka/config/connect-distributed.properties Restart=on-failure RestartSec=10 Environment=\u0026#34;KAFKA_HEAP_OPTS=-Xmx1G -Xms1G\u0026#34; [Install] WantedBy=multi-user.target Enable and start the Kafka Connect service:\n1 2 3 4 sudo systemctl daemon-reload sudo systemctl start kafka-connect sudo systemctl enable kafka-connect sudo systemctl status kafka-connect Step 3: Verify the Installation On the Kafka server, check the Kafka logs:\n1 sudo journalctl -u kafka On the Kafka Connect server, check the Kafka Connect logs:\n1 sudo journalctl -u kafka-connect Step 4: Install and Configure Debezium (Optional) If you want to set up Debezium for change data capture (CDC), follow these steps:\nDownload Debezium SQL Server Connector:\n1 2 3 wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/2.7.1.Final/debezium-connector-sqlserver-2.7.1.Final-plugin.tar.gz sudo tar -xzf debezium-connector-sqlserver-2.7.1.Final-plugin.tar.gz -C /opt/kafka/connectors/ sudo chown -R kafka:kafka /opt/kafka/connectors/debezium-connector-sqlserver Restart Kafka Connect:\n1 sudo systemctl restart kafka-connect Verify the Plugin:\n1 curl -s localhost:8083/connector-plugins | jq Commands for TroubleShooting Kafka 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # List all the topics $KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list # Creating new topic $KAFKA_HOME/bin/kafka-topics.sh --create --topic first-topic --bootstrap-server localhost:9092 # Get information about the topic $KAFKA_HOME/bin/kafka-topics.sh --describe --topic first-topic --bootstrap-server localhost:9092 # Read the messages from the topics $KAFKA_HOME/bin/kafka-console-consumer.sh --topic first-topic --bootstrap-server localhost:9092 # Read the message before the consumer $KAFKA_HOME/bin/kafka-console-consumer.sh --topic first-topic --from-beginning --bootstrap-server localhost:9092 # Delete topic bin/kafka-topics.sh --delete --topic first-topic --bootstrap-server localhost:9092 # Example codes /opt/kafka_2.13-3.8.0/bin/kafka-console-consumer.sh --bootstrap-server \u0026lt;SQL SERVER\u0026gt;:9092 --topic crewing.vessel.fake.crew --from-beginning | jq \u0026#39;.payload | { operation: .op, before: .before.crew_id , after: .after.crew_id ,first_name: .after.first_name, last_name: .after.last_name, change_lsn: .source.change_lsn, commit_lsn: .source.commit_lsn, transaction: .transaction.id }\u0026#39; Kafka Connect 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # List all connectors curl -X GET http://localhost:8083/connectors # Get status of the connectors curl -X GET http://localhost:8083/connectors/my-connector/status # Delete a connector curl -X DELETE http://localhost:8083/connectors/my-connector # Update a connector curl -X PUT -H \u0026#34;Content-Type: application/json\u0026#34; -d @updated-connector-config.json http://localhost:8083/connectors/connector_name/config # Verifying the update of the connector curl -X GET http://localhost:8083/connectors/connector_name/config Conclusion You have successfully installed Kafka and Kafka Connect on separate servers, ensuring that both services are set up for distributed, scalable processing. This setup is optimal for large-scale deployments and is ready for further configuration, such as integrating Debezium or other Kafka Connect plugins.\n","permalink":"https://bookofdaniel.in/posts/installing-kafka-and-kafka-connect-on-seperate-servers/","summary":"\u003cp\u003eInstalling Kafka and Kafka Connect on separate servers allows for better resource management, especially in production environments where Kafka brokers and Connectors may need dedicated hardware. This guide will walk you through the steps to set up Kafka and Kafka Connect on separate Linux servers, using Ubuntu 24.04.\u003c/p\u003e\n\u003ch3 id=\"prerequisites\"\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTwo Linux machines\u003c/strong\u003e (Ubuntu 24.04)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInbound ports opened\u003c/strong\u003e on both servers:\n\u003cul\u003e\n\u003cli\u003eKafka broker ports: 9092, 9093\u003c/li\u003e\n\u003cli\u003eKafka Connect REST port: 8083\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eJava 11 (OpenJDK)\u003c/strong\u003e installed on both servers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-1-set-up-kafka-on-the-first-server\"\u003eStep 1: Set Up Kafka on the First Server\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSet the hostname\u003c/strong\u003e for the Kafka server:\u003c/p\u003e","title":"Installing Kafka and Kafka Connect on Seperate Servers"},{"content":"Hey there, fellow knowledge seekers! 🚀 Recently, I stumbled upon an incredibly inspiring YouTube video that reshaped my entire approach to focus and productivity. In this blog post, I\u0026rsquo;ll distill the essence of this video into a cool, easily digestible guide. Trust me, it\u0026rsquo;s not your typical \u0026ldquo;how-to\u0026rdquo; list; it\u0026rsquo;s a game-changer! (P.S. Check out the original video here for the full experience!)\n1. Create Your Sacred Space 🌌 The first step is to designate a \u0026ldquo;Sacred Space\u0026rdquo; - a special area used solely for studying or working. Imagine it as a holy box where productivity reigns supreme. Once you enter this space, work begins - no exceptions! This approach mirrors insomnia research, teaching you to associate certain places with specific activities.\nPro Tip: Keep your Sacred Space clean and clutter-free. Studies from Princeton University and Harvard Business Review highlight how clutter-free spaces boost focus and productivity.\n2. You Are Sacred Too! 👑 Yes, you read that right! Treat your time, energy, and focus as sacred. You\u0026rsquo;re the CEO of your life, and your time is as precious as that of any billionaire. So, guard it fiercely! No random emails, messages, or distractions. You\u0026rsquo;re on a mission to become the best version of yourself.\nRemember: Avoid micro distractions. That annoying shirt tag or uncomfortable posture? They\u0026rsquo;re robbing you of potential \u0026lsquo;Eureka!\u0026rsquo; moments.\n3. The Power of a Hard Reset 🔄 Feeling overwhelmed? Try this: intense exercise followed by belly breathing. This combo shocks you into the present and primes your brain for peak performance. Thanks to BDNF production (a brain-loving protein) and the relaxation response from belly breathing, you\u0026rsquo;re setting yourself up for cognitive success.\n4. Follow Your Flow, Not the Clock ⏳ Forget timed study sessions. If you\u0026rsquo;re in the zone, keep going. If not, take a break. Your focus and efficiency are the true timers. Every day is different, so be flexible!\n5. Change Your Scenery 🌍 If a study space feels stale, move! Research shows that studying in various environments helps cement knowledge in your brain. New surroundings, new connections!\n6. High-Intensity Focus Cycle 🚴 Sometimes, you just can\u0026rsquo;t focus, and that\u0026rsquo;s okay. Try shifting to a less demanding task or just relax and recharge. High-intensity focus, exercise, and rest - this trio is the key to sustaining long-term productivity.\nIn conclusion, these strategies aren\u0026rsquo;t just tips; they\u0026rsquo;re transformative practices that can elevate your focus and productivity to new heights. Give them a try, and who knows, you might just unlock a level of efficiency you never thought possible!\n","permalink":"https://bookofdaniel.in/posts/how-to-supercharge-your-focus-a-revolutionar-approach/","summary":"\u003cp\u003eHey there, fellow knowledge seekers! 🚀 Recently, I stumbled upon an incredibly inspiring YouTube video that reshaped my entire approach to focus and productivity. In this blog post, I\u0026rsquo;ll distill the essence of this video into a cool, easily digestible guide. Trust me, it\u0026rsquo;s not your typical \u0026ldquo;how-to\u0026rdquo; list; it\u0026rsquo;s a game-changer! (P.S. Check out the \u003ca href=\"https://youtu.be/Pe-iBysIb7o\"\u003eoriginal video\u003c/a\u003e here for the full experience!)\u003c/p\u003e\n\u003ch2 id=\"1-create-your-sacred-space-\"\u003e1. \u003cstrong\u003eCreate Your Sacred Space\u003c/strong\u003e 🌌\u003c/h2\u003e\n\u003cp\u003eThe first step is to designate a \u0026ldquo;Sacred Space\u0026rdquo; - a special area used solely for studying or working. Imagine it as a holy box where productivity reigns supreme. Once you enter this space, work begins - no exceptions! This approach mirrors insomnia research, teaching you to associate certain places with specific activities.\u003c/p\u003e","title":"How to Supercharge Your Focus a Revolutionary Approach"},{"content":"Welcome to the hard way of installing Kubernetes in Azure Virtual Machines. The instructions will be moreover same for On-prem. For many developers, using managed Kubernetes services like GKE, EKS, or AKS can be convenient, but they often abstract away the intricate details of how a cluster operates under the hood.\nRequirements NODES IP HOSTNAME MACHINE TYPE OPERATING SYSTEM master 172.16.39.14 k8s-master.local Standard B2ms Ubuntu 24.04 k8s-worker1 172.16.39.23 k8s-worker1.local Standard B4ms Ubuntu 24.04 Provisioning the Servers in Azure To provision two Linux virtual machines (VMs) in Azure with the specified details, you can use the Azure CLI (az) to achieve this. Here’s how you can provision both machines using az vm create commands.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # 1. Set common variables RESOURCE_GROUP=\u0026#34;k8s-cluster\u0026#34; LOCATION=\u0026#34;eastus\u0026#34; MASTER_VM_NAME=\u0026#34;k8s-master\u0026#34; WORKER_VM_NAME=\u0026#34;k8s-worker1\u0026#34; MASTER_IP=\u0026#34;172.16.39.14\u0026#34; WORKER_IP=\u0026#34;172.16.39.23\u0026#34; VNET_NAME=\u0026#34;k8s-vnet\u0026#34; SUBNET_NAME=\u0026#34;k8s-subnet\u0026#34; MASTER_HOSTNAME=\u0026#34;k8s-master.local\u0026#34; WORKER_HOSTNAME=\u0026#34;k8s-worker1.local\u0026#34; # 2. Resource group creation az group create --name $RESOURCE_GROUP --location $LOCATION # 3. Create a virtual network (VNet) az network vnet create \\ --resource-group $RESOURCE_GROUP \\ --name $VNET_NAME \\ --address-prefix 172.16.0.0/16 \\ --subnet-name $SUBNET_NAME \\ --subnet-prefix 172.16.39.0/24 # 4. Create static IP addresses # Create public IPs az network public-ip create --resource-group $RESOURCE_GROUP --name masterPublicIP --allocation-method Static --sku Standard az network public-ip create --resource-group $RESOURCE_GROUP --name worker1PublicIP --allocation-method Static --sku Standard # Create NIC for the master node az network nic create \\ --resource-group $RESOURCE_GROUP \\ --name masterNIC \\ --vnet-name $VNET_NAME \\ --subnet $SUBNET_NAME \\ --private-ip-address $MASTER_IP \\ --public-ip-address masterPublicIP # Create NIC for the worker node az network nic create \\ --resource-group $RESOURCE_GROUP \\ --name worker1NIC \\ --vnet-name $VNET_NAME \\ --subnet $SUBNET_NAME \\ --private-ip-address $WORKER_IP \\ --public-ip-address worker1PublicIP # 5. Provision the master node az vm create \\ --resource-group $RESOURCE_GROUP \\ --name $MASTER_VM_NAME \\ --size Standard_B2ms \\ --nics masterNIC \\ --image Canonical:0001-com-ubuntu-server-jammy:24_04-lts:latest \\ --admin-username azureuser \\ --generate-ssh-keys \\ --custom-data cloud-init.yaml \\ --host-name $MASTER_HOSTNAME # 6. Provision the worker node az vm create \\ --resource-group $RESOURCE_GROUP \\ --name $WORKER_VM_NAME \\ --size Standard_B4ms \\ --nics worker1NIC \\ --image Canonical:0001-com-ubuntu-server-jammy:24_04-lts:latest \\ --admin-username azureuser \\ --generate-ssh-keys \\ --custom-data cloud-init.yaml \\ --host-name $WORKER_HOSTNAME # 7. Verification az vm list --resource-group $RESOURCE_GROUP -o table This will show you the VMs with their details. You should see k8s-master and k8s-worker1 with the correct IP addresses and machine types.\nSystem Preparation for Kubernetes Installation Before diving into the installation of Kubernetes, it\u0026rsquo;s essential to prepare your system for optimal performance and stability. This section outlines the necessary steps to get both the master and worker nodes ready for Kubernetes.\nℹ️ Execute the following commands on both master and worker nodes.\n1. Update the OS First, ensure your system is up-to-date by running the following commands to update and upgrade all installed packages:\n1 2 sudo apt update sudo apt upgrade -y After the upgrade completes, reboot the system to apply all changes:\n1 sudo reboot 2. Set Hostname Assign meaningful hostnames to both your master and worker nodes. This makes it easier to identify and manage the nodes in your cluster.\nMaster Node: 1 sudo hostnamectl set-hostname \u0026#34;k8s-master.local\u0026#34; Worker Node: 1 sudo hostnamectl set-hostname \u0026#34;k8s-worker1.local\u0026#34; Next, update the /etc/hosts file on both nodes to map the hostnames to their corresponding IP addresses. Add the following lines to the file:\n1 2 172.16.39.14 k8s-master.local 172.16.39.23 k8s-worker1.local 3. Disable Swap Kubernetes requires swap to be disabled to function properly. This is crucial because the Kubelet, the primary Kubernetes agent running on each node, does not handle memory swapping well. Enabling swap can lead to performance degradation and unpredictable behavior in your cluster.\nDisable swap immediately with the following commands:\n1 2 3 sudo swapoff -a sudo sed -i \u0026#39;/ swap / s/^/#/\u0026#39; /etc/fstab sudo mount -a By commenting out the swap entry in /etc/fstab, this ensures swap remains disabled after a reboot. Verify swap is disabled with:\n1 free -h The output should indicate that swap is set to 0:\n1 2 3 total used free shared buff/cache available Mem: 7.8Gi 1.4Gi 4.9Gi 5.0Mi 1.7Gi 6.3Gi Swap: 0B 0B 0B 4. Update Kernel and Configure Modules For Kubernetes to run efficiently, specific kernel modules and network parameters need to be configured.\nFirst, create a configuration file for kernel modules:\n1 2 3 4 sudo tee /etc/modules-load.d/containerd.conf \u0026lt;\u0026lt;EOF overlay br_netfilter EOF overlay: Used for overlay filesystems, which are essential for container storage. br_netfilter: Enables Kubernetes to manage network traffic between containers. Next, load the required kernel modules:\n1 2 sudo modprobe overlay sudo modprobe br_netfilter Set the necessary kernel parameters for Kubernetes by creating a configuration file:\n1 2 3 4 5 sudo tee /etc/sysctl.d/kubernetes.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF Finally, reload the sysctl configuration to apply the new parameters:\n1 sudo sysctl --system These steps ensure your system is properly prepared for the Kubernetes installation, providing a stable foundation for the cluster to run efficiently.\nInstalling Containerd Runtime on All Nodes A critical component of any Kubernetes cluster is the container runtime, which is responsible for running containers on each node. Containerd is a lightweight and powerful runtime that provides essential container lifecycle management, including image transfer, storage, and execution. Originally developed as part of Docker, it is now a key part of the Cloud Native Computing Foundation (CNCF) and is favored for Kubernetes environments due to its simplicity and performance.\nTo install containerd on all nodes in your Kubernetes cluster, follow these steps:\nFirst, ensure that the necessary packages and dependencies are installed:\n1 sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates Add Docker’s official GPG key and Docker’s repository to your system:\n1 2 sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; Update your package lists to include the newly added Docker repository, and install containerd:\n1 2 sudo apt update sudo apt install -y containerd.io Once containerd is installed, you need to configure it to work with Kubernetes. Generate the default configuration file and enable SystemdCgroup, which ensures that containerd integrates smoothly with Kubernetes, particularly when using systemd for process management:\n1 2 containerd config default | sudo tee /etc/containerd/config.toml \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 sudo sed -i \u0026#39;s/SystemdCgroup = false/SystemdCgroup = true/g\u0026#39; /etc/containerd/config.toml Finally, restart and enable the containerd service so that it starts automatically on system boot:\n1 2 sudo systemctl restart containerd sudo systemctl enable containerd By following these steps, you’ll have a robust and efficient container runtime in place, ready for Kubernetes. Repeat this process on each node to ensure consistency across the cluster.\nInstalling Kubeadm, Kubelet, and Kubectl Now that your system is prepared, it\u0026rsquo;s time to install the essential Kubernetes components on all of your machines: kubeadm, kubelet, and kubectl.\nkubeadm: This tool helps bootstrap the Kubernetes cluster. kubelet: The agent that runs on all nodes in the cluster, responsible for running pods and containers. kubectl: A command-line utility that lets you interact with the Kubernetes cluster. It\u0026rsquo;s important to note that kubeadm will not manage or install kubelet or kubectl for you, so you must ensure that all these tools are on the correct version. Mismatches between kubeadm, kubelet, and kubectl versions can result in instability. Kubernetes does allow a one-minor-version difference between the kubelet and the control plane, but the kubelet version should never exceed the API server version. For example, kubelet v1.7.0 can work with an API server running v1.8.0, but not the other way around.\nAdditionally, as of September 13, 2023, Kubernetes has moved to a new package repository hosted at pkgs.k8s.io, which you must use to install any Kubernetes versions after v1.24. The legacy repositories (apt.kubernetes.io) are deprecated and may be removed without notice.\nFollow the steps below to install these packages for Kubernetes v1.31:\n1. Update Package Index and Install Dependencies Start by updating the system’s package index and installing the necessary dependencies:\n1 2 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gpg 2. Add the Kubernetes Signing Key Download the Kubernetes signing key for the package repositories. If the directory /etc/apt/keyrings doesn\u0026rsquo;t exist, create it before running the following command:\n1 curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg This ensures that you are installing authentic Kubernetes packages.\n3. Add the Kubernetes v1.31 Repository Add the Kubernetes v1.31 repository to your system’s sources list. If you need a different version, modify the version number in the URL accordingly:\n1 echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list 4. Install Kubelet, Kubeadm, and Kubectl Once the repository is added, update your package list and install kubeadm, kubelet, and kubectl:\n1 2 3 sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl Marking the packages with apt-mark hold ensures they won\u0026rsquo;t be accidentally updated during system upgrades, which is important for maintaining version stability across your cluster.\n5. Enable the Kubelet Service (Optional) Before you bootstrap the Kubernetes cluster with kubeadm, you can enable the kubelet service to start automatically on boot:\n1 sudo systemctl enable --now kubelet These steps install the core tools required for setting up and managing your Kubernetes cluster. Make sure to follow them carefully on both your master and worker nodes.\nHere’s the blog paragraph based on your snippet:\nInitializing the Kubernetes Cluster with Kubeadm With kubeadm, kubelet, and kubectl installed on your master and worker nodes, it’s time to initialize the Kubernetes cluster.\nℹ️ This step should only be executed on the master node, as it sets up the control plane that will manage the cluster.\nTo begin, use the following command on your master node to initialize the cluster:\n1 2 3 sudo kubeadm init \\ --pod-network-cidr=10.10.0.0/16 \\ --control-plane-endpoint=k8s-master.local \u0026ndash;pod-network-cidr=10.10.0.0/16: This specifies the CIDR range for the pod network. You can modify this value based on your network architecture. \u0026ndash;control-plane-endpoint=k8s-master.local: This is the DNS or IP address of your control plane (master node). Ensure that the DNS or IP is resolvable by all worker nodes in your cluster. After running this command, kubeadm will perform the following tasks:\nDownload and install the necessary control plane components such as etcd, kube-apiserver, kube-scheduler, and kube-controller-manager. Set up your cluster according to the parameters provided. Generate a join token that worker nodes can use to join the cluster. Once the initialization is complete, kubeadm will output instructions to finish setting up kubectl on the master node and provide the join command for your worker nodes.\nNotes:\nThe --pod-network-cidr value must align with the configuration of the pod network solution (e.g., Calico, Flannel) you plan to deploy. Make sure that the control plane endpoint (k8s-master.local) is properly configured in your DNS or /etc/hosts file so that all nodes can resolve it. At this point, the control plane will be ready, and the next step will be to install a network add-on to allow pod-to-pod communication within the cluster.\nOutput:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 azureuser@k8s-master:~$ sudo kubeadm init \\ --pod-network-cidr=10.10.0.0/16 \\ --control-plane-endpoint=k8s-master.local [init] Using Kubernetes version: v1.26.1 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.10] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master.local localhost] and IPs [192.168.1.10 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master.local localhost] and IPs [192.168.1.10 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 7.503422 seconds [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-master.local as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-master.local as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: daii9y.g4dq24u6irkz4pt0 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap,RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regularuser: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config== Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listedat: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following asroot: kubeadm join k8s-master.local:6443 --token daii9y.g4dq24u6irkz4pt0 \\ --discovery-token-ca-cert-hash sha256:58b9cc96ed57a5797fddea653756dbda830efbff55b720a10cffb3948d489148 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join k8s-master.local:6443 --token daii9y.g4dq24u6irkz4pt0 \\ --discovery-token-ca-cert-hash sha256:58b9cc96ed57a5797fddea653756dbda830efbff55b720a10cffb3948d489148 ℹ️ Now, As shown in the output execute below command in master node.\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Verify the cluster status:\n1 2 kubectl cluster-info kubectl get nodes Output:\n1 2 3 4 5 6 7 8 azureuser@k8s-master:~$ kubectl cluster-info Kubernetes control plane is running at https://k8s-master.local:6443 CoreDNS is running at https://k8s-master.local:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. azureuser@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master.local Ready control-plane 3d3h v1.30.4 It seems the control plane is running, we will proceed to add worker nodes to this cluster.\nAdding Worker Nodes to the Kubernetes Cluster After initializing the Kubernetes cluster on the master node, it\u0026rsquo;s time to add your worker nodes to the cluster. This will allow the control plane to distribute workloads across the nodes and manage them.\nℹ️ To add a worker node, you need to execute the kubeadm join command in worker nodes.\nThis command securely connects the worker node to the control plane. the command typically looks something like this:\n1 2 kubeadm join k8s-master.local:6443 --token daii9y.g4dq24u6irkz4pt0 \\ --discovery-token-ca-cert-hash sha256:58b9cc96ed57a5797fddea653756dbda830efbff55b720a10cffb3948d489148 k8s-master.local:6443: This is the control plane endpoint (master node\u0026rsquo;s address). \u0026ndash;token: The token generated during the kubeadm init process, which allows the worker node to authenticate with the control plane. \u0026ndash;discovery-token-ca-cert-hash: A hash that ensures the worker node securely discovers the control plane’s certificate authority. Once this command completes successfully, the worker node will be part of the Kubernetes cluster, ready to run workloads distributed by the control plane. You can verify that the node has joined the cluster by running the following command on the master node:\n1 kubectl get nodes Output:\n1 2 3 4 azureuser@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master.local Ready control-plane 3d3h v1.30.4 k8s-worker1.local Ready \u0026lt;none\u0026gt; 3d3h v1.30.4 This will list all nodes, including the newly added workers, along with their status in the cluster.\nRepeat the process for each worker node to ensure that all machines are part of the cluster.\nHere’s the updated blog paragraph with the sed command for editing the Calico manifest:\nInstalling Calico (v3.28.1) Pod Network for the Kubernetes Cluster In order to allow communication between the pods in your cluster, you\u0026rsquo;ll need to install a network add-on. One of the most popular options is Calico, which provides networking and network security capabilities for Kubernetes. Below, we\u0026rsquo;ll walk through how to install Calico on your Kubernetes cluster.\nℹ️ These commands should be run only on the master node.\n1. Download the Calico Manifest File To begin, download the Calico manifest file, which is pre-configured for clusters with fewer than 50 nodes:\n1 curl https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml -O This file contains all the necessary configuration to deploy Calico on your Kubernetes cluster.\n2. Edit the Calico Manifest Using sed To streamline the process of modifying the CALICO_IPV4POOL_CIDR in the calico.yaml file, you can use the following sed command. This automatically updates the pod network CIDR without manually opening the file:\n1 sed -i \u0026#39;s/value: \u0026#34;192.168.0.0\\/16\u0026#34;/value: \u0026#34;10.10.0.0\\/16\u0026#34;/\u0026#39; calico.yaml This command ensures that the pod network CIDR matches the one you specified during cluster initialization (10.10.0.0/16).\n3. Apply the Calico Manifest Once the manifest is updated, install Calico by applying the configuration using kubectl:\n1 kubectl apply -f calico.yaml Calico will be deployed on your cluster, enabling pod-to-pod communication and enforcing network policies.\nOutput:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 azureuser@k8s-master:~$ kubectl apply -f calico.yaml poddisruptionbudget.policy/calico-kube-controllers created serviceaccount/calico-kube-controllers created serviceaccount/calico-node created configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created deployment.apps/calico-kube-controllers created Verifying the K8s Installation You can verify that Calico is running correctly by checking the status of the pods in the kube-system namespace:\n1 kubectl get pods -n kube-system Output:\n1 2 3 4 5 6 7 azureuser@k8s-master:~$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-77d59654f4-25crd 1/1 Running 5 (3h59m ago) 3d3h calico-node-hqf82 1/1 Running 2 (3h59m ago) 3d3h calico-node-jxwbm 1/1 Running 4 (3h55m ago) 3d3h coredns-7db6d8ff4d-6f9cn 1/1 Running 2 (3h59m ago) 3d4h coredns-7db6d8ff4d-dnzq2 1/1 Running 2 (3h59m ago) 3d4h You should see Calico components such as calico-node and calico-kube-controllers running successfully.\nWith Calico installed, your Kubernetes cluster is now fully networked, allowing pods to communicate across nodes as necessary. You can also configure Calico for advanced network security features if needed.\nNow if we check the status of the nodes, the status will be Ready.\n1 kubectl get nodes Output:\n1 2 3 4 azureuser@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master.local Ready control-plane 3d4h v1.30.4 k8s-worker1.local Ready \u0026lt;none\u0026gt; 3d4h v1.30.4 Congrats, if you reach till the end 😊. You are a soldier 🪖.\n","permalink":"https://bookofdaniel.in/posts/how-to-setup-kubernetes-in-azure-virtual-machines/","summary":"\u003cp\u003eWelcome to the hard way of installing Kubernetes in Azure Virtual Machines. The instructions will be moreover same for On-prem. For many developers, using managed Kubernetes services like GKE, EKS, or AKS can be convenient, but they often abstract away the intricate details of how a cluster operates under the hood.\u003c/p\u003e\n\u003ch1 id=\"requirements\"\u003eRequirements\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003eNODES\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eIP\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eHOSTNAME\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eMACHINE TYPE\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eOPERATING SYSTEM\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003emaster\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e172.16.39.14\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ek8s-master.local\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eStandard B2ms\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eUbuntu 24.04\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003ek8s-worker1\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e172.16.39.23\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ek8s-worker1.local\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eStandard B4ms\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eUbuntu 24.04\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"provisioning-the-servers-in-azure\"\u003eProvisioning the Servers in Azure\u003c/h1\u003e\n\u003cp\u003eTo provision two Linux virtual machines (VMs) in Azure with the specified details, you can use the Azure CLI (az) to achieve this. Here’s how you can provision both machines using az vm create commands.\u003c/p\u003e","title":"How to Setup Kubernetes in Azure Virtual Machines"},{"content":"In a world where technology is ever-evolving, programmers are constantly seeking ways to enhance their skills and productivity. A treasure trove of wisdom that is often overlooked in the tech community is Stephen Covey\u0026rsquo;s iconic book, \u0026ldquo;7 Habits of Highly Effective People.\u0026rdquo; Sahil, a YouTuber, sheds light on how this book can be a game-changer for programmers aspiring to amplify their effectiveness tenfold.\n1. The Essence of Proactivity in Programming Covey distinguishes between reactive and proactive individuals, drawing a parallel with programmers. Reactive programmers might wait for a life-changing tutorial or a list of interview questions to boost their career. In contrast, proactive programmers take ownership of their growth. They invest time in understanding the fundamentals, practicing coding, and building projects. This mindset shift from dependency to self-reliance is crucial for any programmer aiming for excellence.\n2. Start with the End in Mind Strategizing is vital, and Covey\u0026rsquo;s advice to \u0026ldquo;begin with the end in mind\u0026rdquo; resonates well with software development. Before embarking on a project, understanding the desired outcome and the project\u0026rsquo;s requirements can save countless hours. Planning not only streamlines the development process but also ensures that the project\u0026rsquo;s goals are clear and achievable.\n3. Prioritization with the Eisenhower Matrix The Eisenhower Matrix, as recommended by Covey, is a powerful tool for managing tasks based on urgency and importance. Programmers can use this matrix to navigate the often overwhelming demands of software development, focusing on tasks that are crucial for long-term success. This method encourages developers to allocate time for learning and improving, which might otherwise be neglected amid pressing deadlines.\n4. Cultivating an Abundance Mindset Covey\u0026rsquo;s emphasis on developing an abundance mentality is particularly relevant in the competitive field of software engineering. Recognizing that success is not a zero-sum game enables programmers to collaborate more effectively and create win-win situations. This mindset fosters a healthy work environment and encourages collective success.\n5. Empathetic Communication Effective communication is paramount in programming. Covey\u0026rsquo;s principle of seeking first to understand then to be understood applies to code readability and user experience. Programmers who prioritize empathy in their coding and design processes contribute to more intuitive and user-friendly applications.\n6. Leveraging Synergy The concept of synergy, where the whole is greater than the sum of its parts, is critical in software development. By valuing team members\u0026rsquo; diverse strengths and working collaboratively, programmers can achieve remarkable results that surpass what could be accomplished individually.\n7. Continuous Learning: Sharpening the Saw Lastly, Covey\u0026rsquo;s habit of \u0026ldquo;sharpening the saw\u0026rdquo; underscores the importance of continuous learning in programming. With the rapid pace of technological advancements, staying updated and acquiring new skills is essential for a programmer\u0026rsquo;s growth and relevance in the field.\nConclusion Sahil\u0026rsquo;s insightful application of \u0026ldquo;7 Habits of Highly Effective People\u0026rdquo; to programming is a testament to the book\u0026rsquo;s universal relevance. By embracing these habits, programmers can enhance their effectiveness, foster meaningful collaborations, and navigate their careers with confidence and agility. For those looking to delve deeper into building consistent programming habits, Sahil also recommends \u0026ldquo;Atomic Habits\u0026rdquo; by James Clear, further enriching the toolkit for personal and professional development.\nWatch Sahil\u0026rsquo;s video to explore these concepts in detail and join him in the journey toward becoming a 10X programmer.\n","permalink":"https://bookofdaniel.in/posts/how-programmers-can-become-10x-more-effective/","summary":"\u003cp\u003eIn a world where technology is ever-evolving, programmers are constantly seeking ways to enhance their skills and productivity. A treasure trove of wisdom that is often overlooked in the tech community is Stephen Covey\u0026rsquo;s iconic book, \u0026ldquo;7 Habits of Highly Effective People.\u0026rdquo; Sahil, a YouTuber, sheds light on how this book can be a game-changer for programmers aspiring to amplify their effectiveness tenfold.\u003c/p\u003e\n\u003ch4 id=\"1-the-essence-of-proactivity-in-programming\"\u003e1. The Essence of Proactivity in Programming\u003c/h4\u003e\n\u003cp\u003eCovey distinguishes between reactive and proactive individuals, drawing a parallel with programmers. Reactive programmers might wait for a life-changing tutorial or a list of interview questions to boost their career. In contrast, proactive programmers take ownership of their growth. They invest time in understanding the fundamentals, practicing coding, and building projects. This mindset shift from dependency to self-reliance is crucial for any programmer aiming for excellence.\u003c/p\u003e","title":"How Programmers Can Become 10X More Effective"},{"content":"During the COVID-19 lockdown, people had differing views: some felt it could be the end, while others saw it as an opportunity to start something new. One common thread was that everyone was glued to their smartphones. YouTube trends during the pandemic showed skyrocketing watch times since everyone was either watching videos or sleeping.\nYouTube Shorts started gaining more traction over time, particularly those related to motivation. Everyone needed motivation because those were the tough times.\nSeizing the Moment ⏱️ I was late to join the motivation genre. In June 2022, I realized the potential of starting a motivational channel. Compared to the other channels, like \u0026lsquo;Day in the Life\u0026rsquo;, educational, or travel channels, it\u0026rsquo;s relatively simple. You just cut clips from long-form videos of other channels into shorts of less than 30 or 60 seconds with copyright-free music.\nI started multiple motivational channels like learnwithniel and musetales. Some garnered a few views in the first month due to the infamous YouTube algorithm push. After that, the views dwindled.\nI\u0026rsquo;ve long been a fan of Jay Shetty and a regular viewer of his Jay Shetty Podcast channel. In December 2022, I noticed there were no fan channels for Jay Shetty, so I started one called Jay Shetty Shorts 😉.\nSince Jay Shetty is a popular motivational speaker, the channel was a hit. It attracted a lot of attention from both haters and supporters, which helped increase views and comments 😊.\nThe Grind of Consistency 💪🏻 Before starting the YouTube channel, I thought it was easy money—people post videos and earn millions, like a money-printing machine.\nThen I realized that being a YouTuber is not easy! It\u0026rsquo;s a commitment to subscribers. If you\u0026rsquo;re consistent, the YouTube Algorithm will promote your videos, leading to more views. I committed to posting 2 clips + 2 shorts daily.\nIt was difficult because I was not naturally consistent. After starting this channel, I committed myself to editing videos daily. There were many sleepless nights, as I had to balance this with my day job. But let me tell you, it was worth it, and I experienced the power of consistency in my life.\n\u0026ldquo;Success isn\u0026rsquo;t always about greatness. It\u0026rsquo;s about consistency. Consistent hard work leads to success. Greatness will come.\u0026rdquo; - Dwayne Johnson\nBeyond Subscribers and Likes 👍🏻 On YouTube, having a large number of subscribers doesn\u0026rsquo;t necessarily mean higher earnings; it\u0026rsquo;s all about the views. If you have more views, you\u0026rsquo;ll earn more and gain subscribers as a by-product.\nThe comments from Jay Shetty\u0026rsquo;s fans were one of my motivations for this channel. It was inspiring to see how supporters took the content to heart and how it inspired them to express love and inspire others.\nBut remember, if you\u0026rsquo;re solely focused on subscriber count, comments, and likes, you won\u0026rsquo;t be able to continue the journey of being a YouTuber. Do good work and commit to the cause.\nEarning for your hard work is also important since you\u0026rsquo;re investing a lot of effort and precious time.\n\u0026ldquo;Time is what we want most, but what we use worst.\u0026rdquo; - William Penn\nMonetization and Google AdSense 💰 YouTube isn\u0026rsquo;t the money printing machine I once thought it was. Your channel only starts earning after reaching certain thresholds: 1000 Subscribers (now 500), 4000 watch hours (now 3000), and for Shorts, 3 million valid public shorts views in 90 days. These thresholds are there to ensure you\u0026rsquo;re consistent and to see if it\u0026rsquo;s worth betting on you for Google AdSense revenue.\nBy the grace of God, I started earning within 1.5 months. The revenue isn\u0026rsquo;t the same for every channel; it depends on the content and the ads.\nOne Shorts video featuring Jay Shetty and Drew Barrymore helped me surpass the YouTube revenue threshold.\nSometimes all you need is a single shot to turn your hard work into a dream ✨; keep digging ⛏️.\nThe Milestone: 100K Subscribers \u0026amp; The Silver Play Button 🏆 After six months of sleepless nights and consistent hard work, the channel reached 100K subscribers ✨.\nDoes this number of subscribers mean something? Yes, in terms of hard work and consistency it\u0026rsquo;s a reward, but it doesn\u0026rsquo;t mean you\u0026rsquo;re superior to anyone else; it humbles you to thank every subscriber, hater, and supporter for engaging with the content.\nIt took about three weeks to receive the YouTube Silver Play Button after reaching 100K subscribers.\nHolding that award for the first time felt good. I thanked Almighty God for it and, like a typical human being, shared the news on social media.\nMost of my colleagues and friends were surprised because only a few knew that I owned a YouTube channel.\nAfter posting on social media, I put the award back in its box and stored it away. I haven\u0026rsquo;t had the time to frame it and hang it on the wall.\nCopyright Strike ⚡️ By the time I had posted over 1000 videos and amassed 100K+ subscribers, I began receiving copyright strikes from SuperBam.\nI had anticipated this for a long time. 😊 Although it was a fan channel, the content was still from the Jay Shetty Podcast Channel. The copyright strike was legitimate.\nThe nature of the strike was such that I could continue posting videos but was prohibited from earning any revenue from them.\nThe End Now, I have stopped posting videos as we are receiving copyright strikes ⚡️. I\u0026rsquo;ve learned so many things over these months and am thankful to Jay Shetty for pushing me to exceed my own limits regarding consistency. Editing his 1000+ videos has taught me about his commitment to his purpose and his consistency in posting weekly without fail.\nIf you are planning to start a YouTube channel, I hope this will serve as motivation for you. Consistency is the key in every trade.\n\u0026ldquo;The best time to start a YouTube channel was yesterday.\u0026rdquo; - Unknown\n","permalink":"https://bookofdaniel.in/posts/how-i-started-a-youtube-channel/","summary":"\u003cp\u003eDuring the COVID-19 lockdown, people had differing views: some felt it could be the end, while others saw it as an opportunity to start something new. One common thread was that everyone was glued to their smartphones. YouTube \u003ca href=\"https://www.youtube.com/trends/articles/covid-impact/\"\u003etrends during the pandemic\u003c/a\u003e showed skyrocketing watch times since everyone was either watching videos or sleeping.\u003c/p\u003e\n\u003cp\u003eYouTube Shorts started gaining more traction over time, particularly those related to motivation. Everyone needed motivation because those were the tough times.\u003c/p\u003e","title":"How I Started a YouTube Channel and Got 100K Subscribers in 6 Months (Even Though I Got Copyright Strikes)"},{"content":"In the cold winter of 1944, Germany’s once-unbreakable Enigma code had been compromised by the Allies. With every intercepted message, their plans were at risk of exposure, and entire operations could be jeopardized. In response, German High Command devised a new, unbreakable system of communication. They called it Operation Valkyrie.\nThis wasn’t just another machine like the Enigma; this was a new system that couldn’t be cracked, even if the messages were intercepted. Colonel Franz Richter, a seasoned officer in charge of communications, knew that their most sensitive military orders would need to be protected more securely than ever. He gathered his top commanders, including Major Klaus Weber, to unveil the system that could change the course of the war.\nThe Commander’s Key and the Small Vault As Colonel Richter spoke, the tension in the room was palpable. “Our enemies have learned how to read our messages,” he began, “but what I’m about to give you is unbreakable.”\nHe handed each commander a Commander’s Key, a tool that would allow them to lock their orders inside a small vault. These vaults, forged from rare metals, couldn’t be opened by anyone once sealed—except by those with the matching General’s Cipher, a key held only by German High Command.\n“You will use this key to lock your messages in the vault,” Richter explained, “and no one, not even our couriers, will be able to open them. Only we at High Command have the cipher to unlock it.”\nThis was no ordinary encryption. It was based on a cutting-edge cryptographic technique called asymmetric encryption. The Commander’s Key (known today as the public key) could lock the message, but only the private key—the General’s Cipher—could unlock it. If the vault was intercepted, it would remain sealed and secure, its contents unreadable.\nThe Symmetric Key: Securing the Message Inside Colonel Richter wasn’t finished. “Locking the vault is only part of the plan,” he said. “The contents inside must also be encrypted.”\nThe commanders were introduced to another tool: the Symmetric Key. This key would allow both commanders and High Command to encode and decode the actual message. Even if an enemy opened the vault, they wouldn’t understand the orders unless they also had the symmetric key. This was a shared code, known only to the German officials and commanders.\nWith this two-part system—locking the vault with the Commander’s Key and encrypting the message with the Symmetric Key—Germany’s vital communications were more secure than ever.\nThe Seal of Trust: Ensuring Authenticity There was still one final safeguard to prevent sabotage. In wartime, it wasn’t just about intercepting messages; the Allies could attempt to forge fake orders. To prevent this, every vault was sealed with a certificate, a unique symbol that proved the authenticity of the sender.\n“If the seal on the vault does not match the certificate we have on file,” Colonel Richter warned, “we will know it’s a forgery.”\nThis certificate acted like a digital signature, guaranteeing that the message came from the true sender. Much like modern digital certificates, it was impossible to fake, and it provided the final layer of trust needed for secure communications.\nThe Test: A Dangerous Mission The system was put to the test almost immediately. Major Weber had been given a critical task—his orders contained the exact coordinates for a surprise attack that could turn the tide of the war. If those coordinates were compromised, his entire unit would be walking into a trap.\nFollowing the new protocol, Weber encrypted his message using the Symmetric Key, locked it in the small vault with his Commander’s Key, and sealed it with his unique certificate. The vault was handed to Lieutenant Heiner, a trusted courier, who set off through enemy territory, carrying Germany’s future in his satchel.\nThe journey was fraught with danger. Allied patrols were everywhere, and if Heiner were caught, the vault could fall into enemy hands. But even if it did, the Allies wouldn’t be able to open it. The vault was locked, the message encrypted, and the certificate verified.\nVictory Sealed After days of travel, Heiner reached German High Command. The vault, still sealed, was inspected carefully. Colonel Richter checked the certificate against the records and, satisfied with its authenticity, used the General’s Cipher to unlock the vault. Inside, the encrypted message was revealed, and with the Symmetric Key, the orders were finally decrypted.\nThe operation proceeded flawlessly. Major Weber’s unit executed the attack with precision, and the Allies were left in the dark, unaware of how their enemies had communicated so securely.\nThe Fiction Behind the Facts Although this story of Operation Valkyrie is a fictional account, the methods it describes are very real. The Commander’s Key represents the public key used to lock (encrypt) information, while the General’s Cipher is the private key used to unlock (decrypt) it. The Symmetric Key is the shared key used to encrypt and decrypt the actual message, ensuring that both sender and receiver understand the contents.\nFinally, the certificate acts as a digital signature, ensuring the message hasn’t been tampered with and confirming its authenticity. These are the foundational concepts of Public Key Infrastructure (PKI), the system that secures everything from your bank transactions to your emails in today’s digital world.\nThe vault may be fictional, but the principles behind it now protect millions of sensitive communications every day.\nNote: If you catch all the plot holes, you’re officially a genius! 😆 So don’t miss out on the detailed article about Understanding Public Key Infrastructure.\n","permalink":"https://bookofdaniel.in/posts/how-does-tls-works-under-the-hood/","summary":"\u003cp\u003eIn the cold winter of 1944, Germany’s once-unbreakable \u003ca href=\"https://en.wikipedia.org/wiki/Enigma_machine\"\u003eEnigma\u003c/a\u003e code had been compromised by the Allies. With every intercepted message, their plans were at risk of exposure, and entire operations could be jeopardized. In response, German High Command devised a new, unbreakable system of communication. They called it \u003cstrong\u003eOperation Valkyrie\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThis wasn’t just another machine like the \u003ca href=\"https://en.wikipedia.org/wiki/Enigma_machine\"\u003eEnigma\u003c/a\u003e; this was a new system that couldn’t be cracked, even if the messages were intercepted. Colonel Franz Richter, a seasoned officer in charge of communications, knew that their most sensitive military orders would need to be protected more securely than ever. He gathered his top commanders, including Major Klaus Weber, to unveil the system that could change the course of the war.\u003c/p\u003e","title":"How Does TLS Works Under the Hood"},{"content":"","permalink":"https://bookofdaniel.in/posts/a-daily-plan-for-success/","summary":"","title":"A Daily Plan for Success"},{"content":"Vim is a highly configurable text editor used in programming. It\u0026rsquo;s known for its efficiency, enabling users to navigate and edit documents with minimal use of the mouse. This blog post will introduce you to the most important Vim shortcuts and commands, making your coding journey smoother and more efficient.\nUnderstanding Vim Modes Before we dive into the commands, it\u0026rsquo;s crucial to understand Vim\u0026rsquo;s modes. Vim operates in several modes, each with a specific purpose:\nNormal Mode: The default mode when you open Vim. It\u0026rsquo;s used to execute commands. Insert Mode: Allows you to insert text into your document. Visual Mode: Used for selecting lines, blocks, and text in your document. Command-Line Mode: Allows you to enter Vim commands and/or search for text. You can switch between these modes using various commands, which we\u0026rsquo;ll cover in the following sections[1][2][4][9].\nCursor Movement In normal mode, you can move the cursor around with the following commands:\nh: move cursor left j: move cursor down k: move cursor up l: move cursor right w: jump forwards to the start of a word b: jump backwards to the start of a word 0: jump to the start of the line $: jump to the end of the line gg: go to the first line of the document G: go to the last line of the document Insert Mode To enter insert mode from normal mode, use the following commands:\ni: enter insert mode before the cursor a: enter insert mode after the cursor A: enter insert mode at the end of the line o: open a new line below the current line and enter insert mode O: open a new line above the current line and enter insert mode To return to normal mode from insert mode, press Esc.\nVisual Mode To select text, you can switch to visual mode using these commands:\nv: enter visual mode V: enter linewise visual mode Ctrl + v: enter blockwise visual mode Cut, Copy, and Paste Vim uses different terminology for these operations:\nx: delete character under the cursor dd: delete line D: delete from cursor to end of line yy: yank (copy) line p: paste after the cursor P: paste before the cursor Undo and Redo To undo or redo changes, use the following commands:\nu: undo Ctrl + r: redo Search and Replace Vim provides powerful search and replace functionality:\n/pattern: search for a pattern n: move to the next match N: move to the previous match :%s/old/new/g: replace all occurrences of \u0026lsquo;old\u0026rsquo; with \u0026rsquo;new\u0026rsquo; in the entire file File Operations You can perform various file operations using these commands:\n:e {file}: edit another file :w: write (save) file :wq: write file and exit :q!: exit without saving Miscellaneous Here are some additional useful commands:\n:help {keyword}: open help for a keyword :set number: show line numbers :set nonumber: hide line numbers :split {file}: split the window horizontally and open a file :vsplit {file}: split the window vertically and open a file Ctrl + w + arrow keys: navigate between split windows This guide covers the most commonly used Vim commands. As you become more proficient with Vim, you can explore additional commands and functionalities. Happy coding!\nReferences: [1] https://www.linuxfoundation.org/blog/blog/classic-sysadmin-vim-101-a-beginners-guide-to-vim [2] https://opensource.com/article/19/3/getting-started-vim [3] https://www.reddit.com/r/vim/comments/166q64q/an_effective_beginner_vim_tutorial_focusing_on/ [4] https://coderwall.com/p/adv71w/basic-vim-commands-for-getting-started [5] https://youtube.com/watch?v=ggSyF1SVFr4 [6] https://danielmiessler.com/p/vim/ [7] https://www.unomaha.edu/college-of-information-science-and-technology/computer-science-learning-center/_files/resources/CSLC-Helpdocs-Vim.pdf [8] https://www.reddit.com/r/vim/comments/lbjw3u/good_guides_on_vim/ [9] https://www.freecodecamp.org/news/vim-beginners-guide/ [10] https://www.reddit.com/r/vim/comments/k60da0/best_vim_tutorial_for_beginners/ [11] https://vim.rtorr.com [12] https://www.howtoforge.com/vim-basics [13] https://thevaluable.dev/vim-commands-beginner/ [14] https://youtube.com/watch?v=RZ4p-saaQkc [15] https://linuxhandbook.com/basic-vim-commands/ ","permalink":"https://bookofdaniel.in/posts/a-beginners-guide-to-vim-shortcuts/","summary":"\u003cp\u003eVim is a highly configurable text editor used in programming. It\u0026rsquo;s known for its efficiency, enabling users to navigate and edit documents with minimal use of the mouse. This blog post will introduce you to the most important Vim shortcuts and commands, making your coding journey smoother and more efficient.\u003c/p\u003e\n\u003ch2 id=\"understanding-vim-modes\"\u003eUnderstanding Vim Modes\u003c/h2\u003e\n\u003cp\u003eBefore we dive into the commands, it\u0026rsquo;s crucial to understand Vim\u0026rsquo;s modes. Vim operates in several modes, each with a specific purpose:\u003c/p\u003e","title":"A Beginners Guide to Vim Shortcuts"},{"content":"In our current era of remote work and virtual learning, prioritizing physical and mental fitness is more crucial than ever. Luckily, there are numerous strategies you can incorporate into your daily routine without stepping outside your home. The following 15 ways, inspired by the insightful content from a YouTube video by Warikoo, will guide you towards a healthier and more balanced lifestyle.\nBasic Fitness Sit Straight: Maintain a good posture while working or studying. Use a memory foam pillow or a similar support to keep your back straight and avoid slouching.\nMaintain a Consistent Sleep Schedule: Your body should know when to sleep and when to wake up. Aim for at least 6-7 hours of sleep every day to ensure you are mentally and physically rested.\nEat Properly: Avoid eating out of boredom or for entertainment. Limit sugar and fried food, and try to eat three balanced meals a day. Stop eating three hours before sleeping and include as much protein as you can in your diet.\nStay Hydrated: Drink at least five, ideally eight glasses of water a day to keep your body hydrated and to help it absorb nutrients.\nCreate a Morning Routine: Start your day by dedicating time to yourself. This could include meditation, reading, exercising, or any other activity that you enjoy.\nPhysical Fitness Body Weight Exercises: These are exercises that you can do using your own body weight, such as push-ups, squats, tricep dips, and planks.\nResistance Band Exercises: Resistance bands can be used to add difficulty to your workouts. They come in different weights and can be used for exercises like bicep curls and shoulder exercises.\nWeights: If you\u0026rsquo;re ready for a more challenging workout, consider buying weights. You can use them for exercises like shoulder lifts and bicep curls.\nAnkle Weights: Ankle weights can be used to add resistance to your normal activities, like walking up and down the stairs.\nPomodoro Technique: This technique involves working for 25 minutes, then taking a 5-minute break. During your break, you could climb stairs or do other physical activities.\nMental Fitness Meditation: Meditation can help you become more aware of your thoughts. It involves focusing on your breath and observing your thoughts without trying to control them.\nJournaling: Writing about your emotions can help you understand them better. Try to write about any significant emotions you felt during the day and why you felt that way.\nGratitude Jar: Keep a jar where you write down things you\u0026rsquo;re grateful for. On difficult days, you can read these notes to remind yourself of the positive aspects of your life.\nBreathing Exercises: Whenever you feel stressed or need a break, take a moment to focus on your breath. This can help slow down your heart rate and provide your body with oxygen.\nCreate a Personal Space: Designate a space in your house where you can connect with yourself and your thoughts. This could be a chair, a corner, or a table.\nRemember, staying fit at home doesn\u0026rsquo;t require expensive equipment or a gym membership. All you need is a little creativity and the determination to maintain a healthy lifestyle.\n","permalink":"https://bookofdaniel.in/posts/15-ways-to-stay-fit-while-sitting-at-home/","summary":"\u003cp\u003eIn our current era of remote work and virtual learning, prioritizing physical and mental fitness is more crucial than ever. Luckily, there are numerous strategies you can incorporate into your daily routine without stepping outside your home. The following 15 ways, inspired by the insightful content from a YouTube video by \u003ca href=\"https://youtu.be/FKNRNY5ji_M?si=ueCpV5g2vEtG5bjS\"\u003eWarikoo\u003c/a\u003e, will guide you towards a healthier and more balanced lifestyle.\u003c/p\u003e\n\u003ch2 id=\"basic-fitness\"\u003eBasic Fitness\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSit Straight\u003c/strong\u003e: Maintain a good posture while working or studying. Use a memory foam pillow or a similar support to keep your back straight and avoid slouching.\u003c/p\u003e","title":"15 Ways to Stay Fit While Sitting at Home"}]